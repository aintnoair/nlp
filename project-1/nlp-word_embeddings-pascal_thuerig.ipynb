{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af98235081195f5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# NLP - Word Embeddings - Pascal Thürig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b9a787988edcb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction\n",
    "Starting point for this project is the following key requirements:\n",
    "1. Use the BoolQ Dataset from Hugging Face\n",
    "2. Use pre-trained model for word embeddings (word2vec, GloVe or fastText)\n",
    "3. Train a 2-layer classifier with ReLU non-linearity\n",
    "\n",
    "In this project I will be using pre-trained embeddings from word2vec and a simple 2-layer neural network to do the reading comprehension task on the BoolQ dataset.\n",
    "I will document every decision made, from preprocessing to model training and evaluation. The goal is to classify each BoolQ question-answer pair as either 'Yes' or 'No'.\n",
    "\n",
    "\n",
    "\n",
    "## TLDR; Here are the key decisions and justifications:\n",
    "- BoolQ Dataset: Provided by Project berief\n",
    "- Task: Classify BoolQ questions as either \"yes\" or \"no\" using pre-trained embeddings and a simple neural network\n",
    "- Pre-trained embeddings: word2vec - Google News 300; for simplicity and already have a bit of experience with it\n",
    "- Model: 2-layer NN with ReLU activation: Provided by Project brief\n",
    "- Tokenizing: Yes, using a subword tokenizer.\n",
    "- Lowercasing: Yes, all text will be lowercased.\n",
    "- Stemming: No, stemming will not be applied.\n",
    "- Lemmatizing: No, lemmatizing is not used initially but could be tried later.\n",
    "- Stopword removal: No, stopwords are not removed to retain key information.\n",
    "- Removal of other words: No, no other word removal is planned.\n",
    "- Format cleaning: No further cleaning required, the dataset is already clean.\n",
    "- Truncation: Yes, input text is truncated to a maximum of 512 tokens.\n",
    "- Feature selection: None, relying on word2vec embeddings directly.\n",
    "- Input format: Tokenized and padded sequences of word2vec embeddings.\n",
    "- Label format: Binary (1 for \"yes\", 0 for \"no\").\n",
    "- train/valid/test splits: 66% train, 8% validation, 26% test.\n",
    "- Padding: Yes, sequences are padded for uniform input length.\n",
    "- Embedding: Pre-trained word2vec embeddings are used for simplicity.\n",
    "- Planned correctness tests: Shape consistency checks, binary label correctness, and validation of truncation and padding.\n",
    "- Hyperparameters:\n",
    "    - Learning Rate: 1e-2 – 1e-5\n",
    "    - Batch Size: 16 - 64 (choosing maximum possible that my GPU can handle)\n",
    "    - Epochs 10 - 50 in 5-/10-step increments\n",
    "    - Hidden size: 64 - 512 \n",
    "    - Early Stopping: Patience of 3 - 10 Epochs of non-improvement (depending on total epoch number)\n",
    "- Evaluation: Accuracy and F1-Score: Accuracy for general performance and F1 to handle class imbalances\n",
    "- Error Analysis: Investigating False Positives and False negatives to understand where the model fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923605f52ff9399",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Setup\n",
    "Importing necessary libraries:\n",
    "- datasets\n",
    "- gensim\n",
    "- nltk\n",
    "- transformers\n",
    "- numpy\n",
    "- torch\n",
    "- wandb\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f9167e7b46f14a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:25.434244Z",
     "start_time": "2024-10-01T08:52:24.614062Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.11/site-packages (0.18.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (2.14.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb) (71.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets gensim nltk transformers numpy torch wandb scikit-learn\n",
    "\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7004488f0ec3eb3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First up the BoolQ dataset is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "290a1930d5aa00db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:30.192143Z",
     "start_time": "2024-10-01T08:52:25.437081Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'do iran and afghanistan speak the same language', 'answer': True, 'passage': 'Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.'}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')\n",
    "\n",
    "\n",
    "test_question = train_data[0]['question']\n",
    "print(train_data[0])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e81ed52db754a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For easy access during experiments I like to define the hyperparameters at the top of my notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27891755da01cbdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:30.208194Z",
     "start_time": "2024-10-01T08:52:30.190532Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "w2v_model_name = \"word2vec-google-news-300\"\n",
    "w2v_model_path = \"word2vec-google-news-300.model\"\n",
    "EMBEDDING_DIM = 300\n",
    "max_seq_length = 100\n",
    "\n",
    "batch_size = 10\n",
    "n_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "hidden_dim = 64\n",
    "\n",
    "run_number = 1\n",
    "wandb_project_name = \"nlp-word_embeddings-pascal_thuerig\"\n",
    "wandb_run_name = f\"run_{run_number}-batch_size_{batch_size}-n_epochs_{n_epochs}-lr_{learning_rate}-hidden_dim_{hidden_dim}\"\n",
    "\n",
    "sequence_length = max_seq_length * 2  # Contatenate question and passage\n",
    "input_dim = sequence_length * EMBEDDING_DIM\n",
    "output_dim = 2  # Binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac159201e4f8349",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now the pre-trained embeddings from word2vec - word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7cd134842c8958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:32.771968Z",
     "start_time": "2024-10-01T08:52:30.198855Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from local storage.\n"
     ]
    }
   ],
   "source": [
    "# Check if the model file exists\n",
    "try:\n",
    "    # Load the model if it exists locally\n",
    "    word2vec_model = gensim.models.KeyedVectors.load(w2v_model_path)\n",
    "    print(\"Model loaded from local storage.\")\n",
    "except FileNotFoundError:\n",
    "    # Download and save the model if it doesn't exist\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    word2vec_model = api.load(w2v_model_name)\n",
    "    word2vec_model.save(w2v_model_path)  # Save the model locally\n",
    "    print(\"Model downloaded and saved to local storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ae072eeb522f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "The BoolQ data will be processed in the following way:\n",
    "1.  Tokenizing: the input questions and passages using a subword tokenizer\n",
    "2.  Lowercasing: the text for simplicity and to reduce the total vocabulary size\n",
    "3.  Stemming: No, will not stem the words as to not lose information\n",
    "4.  Lemmatizing: No, will try if it improves performance\n",
    "5.  Stopword removal: No, will not be removed to not lose potentially critical information [research](https://datascience.stackexchange.com/questions/31048/pros-cons-of-stop-word-removal)\n",
    "6.  Removal of other words: No, will not be removing any other words\n",
    "7.  Format cleaning: The dataset is already sufficiently clean, it shouldn't impact performance\n",
    "8.  Truncation: the input text is truncated to a maximum of 512 tokens\n",
    "9.  Feature selection: Not applicable as we focus on raw text as input and leveraging the pre-trained word embeddings no further feature extraction is needed.\n",
    "10. Input format: Will take the form of the tokenized and padded sequences of word embeddings\n",
    "11. Label format: Binary labels \"yes\" or \"no\"\n",
    "12. train/valid/test splits: Prerequisite to project (66/8/26)\n",
    "13. Padding: the sequences is padded to ensure all inputs have the same length in each batch\n",
    "14. Embedding: Using word2vec, solely for simplicity as I already know it.\n",
    "15. Planned correctness tests: Check for shape mismatches between tokenized text and word embeddings. - Ensure that input sequences are properly truncated and padded. - Verify that binary labels are correctly assigned and match the expected outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5c0360f3733c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. Preprocess text (lowercasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33f5bbffdb41ff92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:32.772975Z",
     "start_time": "2024-10-01T08:52:32.769538Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do iran and afghanistan speak the same language\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "print(lowercase_text(test_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb45d935e577dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2. Tokenize with AutoTokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8718b756b58eb0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:32.944141Z",
     "start_time": "2024-10-01T08:52:32.772832Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'do', 'iran', 'and', 'afghanistan', 'speak', 'the', 'same', 'language', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "tensor([[ 101, 2079, 4238, 1998, 7041, 3713, 1996, 2168, 2653,  102,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])\n",
      "Padding/Truncation Check Passed!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text,\n",
    "                    padding='max_length',    \n",
    "                    truncation=True,         \n",
    "                    max_length=max_seq_length,\n",
    "                    return_tensors='pt')\n",
    "\n",
    "tokenized_output = tokenize_text(test_question)\n",
    "\n",
    "print(tokenized_output.tokens())\n",
    "print(tokenized_output['input_ids'])\n",
    "print(tokenized_output['attention_mask'])\n",
    "\n",
    "# Check if padding/truncation worked by asserting the length\n",
    "assert tokenized_output['input_ids'].shape[1] == max_seq_length, \\\n",
    "    f\"Tokenized length is not equal to max_seq_length: {tokenized_output['input_ids'].shape[1]}\"\n",
    "\n",
    "print(\"Padding/Truncation Check Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668490c2f22c73e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3. Truncate or add padding -> found out I can already do this in the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27021cbde658ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:32.954664Z",
     "start_time": "2024-10-01T08:52:32.944280Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2024c0694cc02f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4. Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7019b2ed6d1b5df3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:32.954935Z",
     "start_time": "2024-10-01T08:52:32.947067Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pipeline(text, word2vec_model):\n",
    "    text = lowercase_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    tokens = pad_or_truncate(tokens, max_seq_length)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480916ef022ad32",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5. Embed tokens using word2vec (word2vec-google-news-300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a08858dc075f9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:32.954997Z",
     "start_time": "2024-10-01T08:52:32.949693Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokens_to_embeddings(tokens, word2vec_model, embedding_dim=EMBEDDING_DIM):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec_model:\n",
    "            embeddings.append(word2vec_model[token])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406a75fa663ed2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6. Create a custom BoolQ dataset class to:\n",
    "    - get the data into a compatible format for the pyTorch dataloader.\n",
    "    - organize question-answer pairs and apply the preprocessing pipeline.\n",
    "    - easily batch, shuffle, and load the data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68b80b754455711f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:33.000157Z",
     "start_time": "2024-10-01T08:52:32.983008Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data, word2vec_model, max_seq_length=max_seq_length):\n",
    "        self.data = data\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        question = self.data[id]['question']\n",
    "        passage = self.data[id]['passage']\n",
    "        \n",
    "        label = 1 if self.data[id]['answer'] else 0\n",
    "        \n",
    "        question_tokens = preprocess_pipeline(question, self.word2vec_model)\n",
    "        passage_tokens = preprocess_pipeline(passage, self.word2vec_model)\n",
    "        \n",
    "        question_embeddings = tokens_to_embeddings(question_tokens, self.word2vec_model)\n",
    "        passage_embeddings = tokens_to_embeddings(passage_tokens, self.word2vec_model)\n",
    "        \n",
    "        embeddings = np.concatenate((question_embeddings, passage_embeddings), axis=0)\n",
    "        \n",
    "        return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a911c23aa942989",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "7. Dataloaders as required by pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdc7fd0fa951faa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:33.006806Z",
     "start_time": "2024-10-01T08:52:32.986095Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = BoolQDataset(train_data, word2vec_model)\n",
    "validation_dataset = BoolQDataset(validation_data, word2vec_model)\n",
    "test_dataset = BoolQDataset(test_data, word2vec_model)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c1b1d4d889aac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "8. Initialize weights and biases for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4d8b0f2e45c1aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.606718Z",
     "start_time": "2024-10-01T08:52:32.989594Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:n4pojchs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_1-batch_size_10-n_epochs_10-lr_0.0001-hidden_dim_64</strong> at: <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/n4pojchs' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/n4pojchs</a><br/> View project at: <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241001_092754-n4pojchs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:n4pojchs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/nlp/project-1/wandb/run-20241001_094335-mql5jl9o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/mql5jl9o' target=\"_blank\">run_1-batch_size_10-n_epochs_10-lr_0.0001-hidden_dim_64</a></strong> to <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/mql5jl9o' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/mql5jl9o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=wandb_project_name, name=wandb_run_name)\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.epochs = n_epochs\n",
    "wandb.config.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41588a415c4a9457",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "The model architecture for this project is already fixed in the project brief as follows:\n",
    "- **Network Architecture:** 2-Layer with ReLu non-linearity.\n",
    "- **Loss / Optimizer:** Loss: CrossEntropyLoss / Optimizer: Adam (potentially trying SGD with or without momentum in experiments)\n",
    "- **Experiments to run**: Mentioned in Training section below\n",
    "- **Number of training runs**: Will depend on number of experiments\n",
    "- **Checkpointing / Early stopping:** 3 - 10 epochs of non-improvement of the validation loss\n",
    "- **Planned correctness tests:** Shape and Dimension consistency tests, Gradient Check, Sanity Check & Prediction Testd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a17c9339ed26b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1. Creating the neural network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e88677b1ed7c692d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.617651Z",
     "start_time": "2024-10-01T08:52:34.606885Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa8093a7ab3628",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2. Create instance of model and move it to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7faae51ad4eb6f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:54.826145Z",
     "start_time": "2024-10-01T08:52:54.795490Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = TwoLayerNN(input_dim, hidden_dim, output_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdaaae9cb17991",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3. Loss (nn.CrossEntropyLoss) and optimizer (optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a9517bd58eaa7cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.679377Z",
     "start_time": "2024-10-01T08:52:34.630790Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a130a8db2718d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e70ec39db30393ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.679631Z",
     "start_time": "2024-10-01T08:52:34.633373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.view(inputs.size(0), -1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        wandb.log({\"epoch\": epoch + 1, \"average_loss\": running_loss / len(train_loader)})\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395aa4d914c29cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5. Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68905d506c269a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.679795Z",
     "start_time": "2024-10-01T08:52:34.635528Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "    wandb.log({\"validation_accuracy\": accuracy})  # Log validation accuracy to WandB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e49144e1bbf412",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training\n",
    "Train the model with the following different hyperparameters:\n",
    "- Learning rate: 1e-2 – 1e-5\n",
    "- Batch size: 16 - 64\n",
    "- Epochs: 10 - 50\n",
    "- Hidden size: 64 - 512\n",
    "- Early Stopping: Patience of 3 - 10 Epochs of non-improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c59674184ff3416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.679901Z",
     "start_time": "2024-10-01T08:52:34.638039Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'BatchEncoding' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m, in \u001b[0;36mBoolQDataset.__getitem__\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     12\u001b[0m passage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mid\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mid\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m question_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword2vec_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m passage_tokens \u001b[38;5;241m=\u001b[39m preprocess_pipeline(passage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec_model)\n\u001b[1;32m     19\u001b[0m question_embeddings \u001b[38;5;241m=\u001b[39m tokens_to_embeddings(question_tokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec_model)\n",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m, in \u001b[0;36mpreprocess_pipeline\u001b[0;34m(text, word2vec_model)\u001b[0m\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m lowercase_text(text)\n\u001b[1;32m      3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenize_text(text)\n\u001b[0;32m----> 5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpad_or_truncate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m, in \u001b[0;36mpad_or_truncate\u001b[0;34m(tokens, max_length, pad_token)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens[:max_length]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'BatchEncoding' and 'list'"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, validation_loader, criterion, optimizer, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ef8c4a2461190",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Evaluation\n",
    "The model will be evaluated for the key metrics of:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "The results will be averaged using micro averaging because I care about the total number of correct prediction regardless of the class (\"yes\" or \"no\"). \n",
    "\n",
    "Errors will be evaluated by making a confusion matrix and giving me the distribution of ture positives, false positives, true negatives and false negatives. Helping me figure out where the model is making most of it's mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99adb580a3aa62c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.680192Z",
     "start_time": "2024-10-01T08:52:34.640358Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f45c239d144cd8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Finish the WandB run\n",
    "Closing the WandB run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc6238cff90999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.680282Z",
     "start_time": "2024-10-01T08:52:34.643088Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dffd5636dc69ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Interpretation\n",
    "\n",
    "To set concrete expectations for my model I take into account a couple of key benchmarks:\n",
    "- **Accuracy:** Given the task of binary classification an accuracy of ~50% can be achieved with random guesses.\n",
    "    - Expecting my model to hit an accuracy of ~60-75%.\n",
    "- **F1 Score:** For this dataset I expect the F1 score to be similar to the accuracy of ~60-75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5c07a77e6af43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T08:52:34.680329Z",
     "start_time": "2024-10-01T08:52:34.645525Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
