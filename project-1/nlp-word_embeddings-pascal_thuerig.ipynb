{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af98235081195f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP - Word Embeddings - Pascal Thürig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b9a787988edcb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "Starting point for this project is the following key requirements:\n",
    "1. Use the BoolQ Dataset from Hugging Face\n",
    "2. Use pre-trained model for word embeddings (word2vec, GloVe or fastText)\n",
    "3. Train a 2-layer classifier with ReLU non-linearity\n",
    "\n",
    "In this project I will be using pre-trained embeddings from word2vec and a simple 2-layer neural network to do the reading comprehension task on the BoolQ dataset.\n",
    "I will document every decision made, from preprocessing to model training and evaluation. The goal is to classify each BoolQ question-answer pair as either 'Yes' or 'No'.\n",
    "\n",
    "\n",
    "\n",
    "## TLDR; Here are the key decisions and justifications:\n",
    "- BoolQ Dataset: Provided by Project berief\n",
    "- Task: Classify BoolQ questions as either \"yes\" or \"no\" using pre-trained embeddings and a simple neural network\n",
    "- Pre-trained embeddings: word2vec - Google News 300; for simplicity and already have a bit of experience with it\n",
    "- Model: 2-layer NN with ReLU activation: Provided by Project brief\n",
    "- Tokenizing: Yes, using a subword tokenizer.\n",
    "- Lowercasing: Yes, all text will be lowercased.\n",
    "- Stemming: No, stemming will not be applied.\n",
    "- Lemmatizing: No, lemmatizing is not used initially but could be tried later.\n",
    "- Stopword removal: No, stopwords are not removed to retain key information.\n",
    "- Removal of other words: No, no other word removal is planned.\n",
    "- Format cleaning: No further cleaning required, the dataset is already clean.\n",
    "- Truncation: Yes, input text is truncated to a maximum of 512 tokens.\n",
    "- Feature selection: None, relying on word2vec embeddings directly.\n",
    "- Input format: Tokenized and padded sequences of word2vec embeddings.\n",
    "- Label format: Binary (1 for \"yes\", 0 for \"no\").\n",
    "- train/valid/test splits: 66% train, 8% validation, 26% test.\n",
    "- Padding: Yes, sequences are padded for uniform input length.\n",
    "- Embedding: Pre-trained word2vec embeddings are used for simplicity.\n",
    "- Planned correctness tests: Shape consistency checks, binary label correctness, and validation of truncation and padding.\n",
    "- Hyperparameters:\n",
    "    - Learning Rate: 1e-2 – 1e-5\n",
    "    - Batch Size: 16 - 64 (choosing maximum possible that my GPU can handle)\n",
    "    - Epochs 10 - 50 in 5-/10-step increments\n",
    "    - Hidden size: 64 - 512 \n",
    "    - Early Stopping: Patience of 3 - 10 Epochs of non-improvement (depending on total epoch number)\n",
    "- Evaluation: Accuracy and F1-Score: Accuracy for general performance and F1 to handle class imbalances\n",
    "- Error Analysis: Investigating False Positives and False negatives to understand where the model fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923605f52ff9399",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup\n",
    "Importing necessary libraries:\n",
    "- datasets\n",
    "- gensim\n",
    "- nltk\n",
    "- transformers\n",
    "- numpy\n",
    "- torch\n",
    "- wandb\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "id": "62f9167e7b46f14a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:27.620127Z",
     "start_time": "2024-10-05T23:47:25.840500Z"
    }
   },
   "source": [
    "%pip install datasets gensim transformers numpy torch wandb scikit-learn\n",
    "\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "import sklearn"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (4.45.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: wandb in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (0.18.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from gensim) (7.0.5)\n",
      "\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\beyonest\\.virtualenvs\\nlp-dnu4kijs\\lib\\site-packages (from torch) (4.12.2)"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "f7004488f0ec3eb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First up the BoolQ dataset is loaded"
   ]
  },
  {
   "cell_type": "code",
   "id": "290a1930d5aa00db",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:34.549542Z",
     "start_time": "2024-10-05T23:47:27.657669Z"
    }
   },
   "source": [
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')\n",
    "\n",
    "\n",
    "test_question = train_data[5]['question']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you use oyster card at epsom station', 'answer': False, 'passage': \"Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\"}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "d5e81ed52db754a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For easy access during experiments I like to define the hyperparameters at the top of my notebooks"
   ]
  },
  {
   "cell_type": "code",
   "id": "27891755da01cbdd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:34.568853Z",
     "start_time": "2024-10-05T23:47:34.564350Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 300\n",
    "max_seq_length = 512\n",
    "\n",
    "run_number = 5\n",
    "batch_size = 512\n",
    "n_epochs = 50\n",
    "learning_rate = 1e-5\n",
    "hidden_dim = 64\n",
    "patience = 5\n",
    "\n",
    "wandb_project_name = \"nlp-word_embeddings-pascal_thuerig\"\n",
    "wandb_run_name = f\"run_{run_number}-batch_size_{batch_size}-n_epochs_{n_epochs}-lr_{learning_rate}-hidden_dim_{hidden_dim}\"\n",
    "\n",
    "sequence_length = max_seq_length * 2  # Concatenate question and passage\n",
    "input_dim = sequence_length * EMBEDDING_DIM\n",
    "output_dim = 2  # Binary classification\n",
    "\n",
    "print(f\"Seq Length: {sequence_length}\")\n",
    "print(f\"Input dim: {input_dim}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq Length: 1024\n",
      "Input dim: 307200\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "id": "8ac159201e4f8349",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now the pre-trained embeddings from word2vec - word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca7cd134842c8958",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.060827Z",
     "start_time": "2024-10-05T23:47:34.644029Z"
    }
   },
   "source": [
    "model_name = \"fasttext-wiki-news-subwords-300\"\n",
    "model_path = \"fasttext-wiki-news-subwords-300.model\"\n",
    "\n",
    "# Check if the model file exists\n",
    "try:\n",
    "    # Load the model if it exists locally\n",
    "    embeddings_model = gensim.models.KeyedVectors.load(model_path)\n",
    "    print(\"Model loaded from local storage.\")\n",
    "except FileNotFoundError:\n",
    "    # Download and save the model if it doesn't exist\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    embeddings_model = api.load(model_name)\n",
    "    embeddings_model.save(model_path)  # Save the model locally\n",
    "    print(\"Model downloaded and saved to local storage.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from local storage.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "f45ae072eeb522f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "The BoolQ data will be processed in the following way:\n",
    "1.  Tokenizing: the input questions and passages using a subword tokenizer\n",
    "2.  Lowercasing: the text for simplicity and to reduce the total vocabulary size\n",
    "3.  Stemming: No, will not stem the words as to not lose information\n",
    "4.  Lemmatizing: No, will try if it improves performance\n",
    "5.  Stopword removal: No, will not be removed to not lose potentially critical information [research](https://datascience.stackexchange.com/questions/31048/pros-cons-of-stop-word-removal)\n",
    "6.  Removal of other words: No, will not be removing any other words\n",
    "7.  Format cleaning: The dataset is already sufficiently clean, it shouldn't impact performance\n",
    "8.  Truncation: the input text is truncated to a maximum of 512 tokens\n",
    "9.  Feature selection: Not applicable as we focus on raw text as input and leveraging the pre-trained word embeddings no further feature extraction is needed.\n",
    "10. Input format: Will take the form of the tokenized and padded sequences of word embeddings\n",
    "11. Label format: Binary labels \"yes\" or \"no\"\n",
    "12. train/valid/test splits: Prerequisite to project (66/8/26)\n",
    "13. Padding: the sequences is padded to ensure all inputs have the same length in each batch\n",
    "14. Embedding: Using word2vec, solely for simplicity as I already know it.\n",
    "15. Planned correctness tests: Check for shape mismatches between tokenized text and word embeddings. - Ensure that input sequences are properly truncated and padded. - Verify that binary labels are correctly assigned and match the expected outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5c0360f3733c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Preprocess text (lowercasing)"
   ]
  },
  {
   "cell_type": "code",
   "id": "33f5bbffdb41ff92",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.112826Z",
     "start_time": "2024-10-05T23:47:35.109310Z"
    }
   },
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "print(lowercase_text(test_question))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "id": "4afb45d935e577dc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Tokenize with AutoTokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8718b756b58eb0f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.599824Z",
     "start_time": "2024-10-05T23:47:35.154144Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text,\n",
    "                     padding='max_length',    \n",
    "                     truncation=True,         \n",
    "                     max_length=max_seq_length,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "tokenized_output = tokenize_text(test_question)\n",
    "\n",
    "# Print tokens\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_output['input_ids'][0]))\n",
    "\n",
    "# Print input_ids and their corresponding tokens\n",
    "test_input_ids = tokenized_output['input_ids'][0]\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_input_ids)\n",
    "for token, id in zip(test_tokens, test_input_ids):\n",
    "    print(f\"Token: {token} - ID: {id.item()}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BEYONEST\\.virtualenvs\\nlp-dnu4KIjs\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'can', 'you', 'use', 'oyster', 'card', 'at', 'eps', '##om', 'station', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Token: [CLS] - ID: 101\n",
      "Token: can - ID: 2064\n",
      "Token: you - ID: 2017\n",
      "Token: use - ID: 2224\n",
      "Token: oyster - ID: 21480\n",
      "Token: card - ID: 4003\n",
      "Token: at - ID: 2012\n",
      "Token: eps - ID: 20383\n",
      "Token: ##om - ID: 5358\n",
      "Token: station - ID: 2276\n",
      "Token: [SEP] - ID: 102\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "b668490c2f22c73e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Truncate or add padding -> *found out I can already do this in the tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "id": "da27021cbde658ea",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.631193Z",
     "start_time": "2024-10-05T23:47:35.628266Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2024c0694cc02f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "7019b2ed6d1b5df3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.651918Z",
     "start_time": "2024-10-05T23:47:35.645695Z"
    }
   },
   "source": [
    "def preprocess_pipeline(text):\n",
    "    text = lowercase_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # Ensure tokenized length is correct\n",
    "    assert tokens['input_ids'].shape[1] == max_seq_length, \\\n",
    "        f\"Tokenized length is not equal to max_seq_length: {tokens['input_ids'].shape[1]}\"\n",
    "    \n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    \n",
    "    return token_strings\n",
    "\n",
    "preprocessed_text = preprocess_pipeline(test_question)\n",
    "print(\"Tokens:\", preprocessed_text)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'can', 'you', 'use', 'oyster', 'card', 'at', 'eps', '##om', 'station', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "4480916ef022ad32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Embed tokens using word2vec (word2vec-google-news-300)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.733221Z",
     "start_time": "2024-10-05T23:47:35.730048Z"
    }
   },
   "id": "23c7a25f8eb26186",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.750738Z",
     "start_time": "2024-10-05T23:47:35.748235Z"
    }
   },
   "id": "a2b2a6114f1e3914",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a08858dc075f9bf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.806663Z",
     "start_time": "2024-10-05T23:47:35.799229Z"
    }
   },
   "source": [
    "def tokens_to_embeddings(tokens, embedding_model=embeddings_model, embedding_dim=EMBEDDING_DIM):\n",
    "    embeddings = []\n",
    "    for t in tokens:\n",
    "        if t in embedding_model:\n",
    "            embeddings.append(embedding_model[t])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "embedded_text = tokens_to_embeddings(preprocessed_text)\n",
    "print(embedded_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [-0.029857 -0.034625 -0.038095 ... -0.018202  0.049474  0.021932]\n",
      " [-0.047138  0.043438  0.024106 ... -0.011364 -0.037262 -0.031316]\n",
      " ...\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]]\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "id": "f406a75fa663ed2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. Create a custom BoolQ dataset class to:\n",
    "    - get the data into a compatible format for the pyTorch dataloader.\n",
    "    - organize question-answer pairs and apply the preprocessing pipeline.\n",
    "    - easily batch, shuffle, and load the data during training."
   ]
  },
  {
   "cell_type": "code",
   "id": "68b80b754455711f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.909561Z",
     "start_time": "2024-10-05T23:47:35.905086Z"
    }
   },
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data, word2vec_model, max_seq_length=max_seq_length):\n",
    "        self.data = data\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data[idx]['question']\n",
    "        passage = self.data[idx]['passage']\n",
    "        \n",
    "        label = 1 if self.data[idx]['answer'] else 0\n",
    "        \n",
    "        question_tokens = preprocess_pipeline(question)\n",
    "        passage_tokens = preprocess_pipeline(passage)\n",
    "        \n",
    "        question_embeddings = tokens_to_embeddings(question_tokens)\n",
    "        passage_embeddings = tokens_to_embeddings(passage_tokens)\n",
    "        \n",
    "        embeddings = np.concatenate((question_embeddings, passage_embeddings), axis=0).flatten()\n",
    "        \n",
    "        return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "id": "4a911c23aa942989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. Dataloaders as required by pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "id": "fdc7fd0fa951faa3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:35.961594Z",
     "start_time": "2024-10-05T23:47:35.925099Z"
    }
   },
   "source": [
    "train_dataset = BoolQDataset(train_data, embeddings_model)\n",
    "validation_dataset = BoolQDataset(validation_data, embeddings_model)\n",
    "test_dataset = BoolQDataset(test_data, embeddings_model)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "id": "d57c1b1d4d889aac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. Initialize weights and biases for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4d8b0f2e45c1aed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:36.399805Z",
     "start_time": "2024-10-05T23:47:35.978734Z"
    }
   },
   "source": [
    "wandb.init(project=wandb_project_name, name=wandb_run_name, config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"hidden_size\": hidden_dim,\n",
    "})\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\BEYONEST\\Documents\\nlp\\project-1\\wandb\\run-20241006_014735-six016ln</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/six016ln' target=\"_blank\">run_5-batch_size_512-n_epochs_50-lr_1e-05-hidden_dim_64</a></strong> to <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/six016ln' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/six016ln</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/six016ln?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ba40891e90>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "id": "41588a415c4a9457",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model\n",
    "\n",
    "The model architecture for this project is already fixed in the project brief as follows:\n",
    "- **Network Architecture:** 2-Layer with ReLu non-linearity.\n",
    "- **Loss / Optimizer:** Loss: CrossEntropyLoss / Optimizer: Adam (potentially trying SGD with or without momentum in experiments)\n",
    "- **Experiments to run**: Mentioned in Training section below\n",
    "- **Number of training runs**: Will depend on number of experiments\n",
    "- **Checkpointing / Early stopping:** 3 - 10 epochs of non-improvement of the validation loss\n",
    "- **Planned correctness tests:** Shape and Dimension consistency tests, Gradient Check, Sanity Check & Prediction Testd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a17c9339ed26b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Creating the neural network class:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e88677b1ed7c692d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:36.420576Z",
     "start_time": "2024-10-05T23:47:36.416638Z"
    }
   },
   "source": [
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "e9aa8093a7ab3628",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Create instance of model and move it to the GPU"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:36.440910Z",
     "start_time": "2024-10-05T23:47:36.436124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "45da8fb1b72b249a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "7faae51ad4eb6f1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:36.612699Z",
     "start_time": "2024-10-05T23:47:36.489022Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoLayerNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "id": "2ccdaaae9cb17991",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Loss (nn.CrossEntropyLoss) and optimizer (optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a9517bd58eaa7cd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:36.630980Z",
     "start_time": "2024-10-05T23:47:36.627850Z"
    }
   },
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "id": "35a130a8db2718d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "e70ec39db30393ed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:36.651369Z",
     "start_time": "2024-10-05T23:47:36.646362Z"
    }
   },
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    early_stop_counter = 0\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels).item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(validation_loader)\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training loss: {avg_train_loss:.4f}, Validation loss: {avg_val_loss:.4f}\")\n",
    "        wandb.log({\"epoch\": epoch + 1, \"average_training_loss\": avg_train_loss, \"average_validation_loss\": avg_val_loss})\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Finished Training\")"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "id": "7395aa4d914c29cc",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "id": "68905d506c269a9c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T23:47:36.667674Z",
     "start_time": "2024-10-05T23:47:36.665691Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60e49144e1bbf412",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training\n",
    "Train the model with the following different hyperparameters:\n",
    "- Learning rate: 1e-2 – 1e-5\n",
    "- Batch size: 16 - 64\n",
    "- Epochs: 10 - 50\n",
    "- Hidden size: 64 - 512\n",
    "- Early Stopping: Patience of 3 - 10 Epochs of non-improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c59674184ff3416",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T00:21:21.828382Z",
     "start_time": "2024-10-05T23:47:36.687059Z"
    }
   },
   "source": [
    "train_model(model, train_loader, validation_loader, criterion, optimizer, epochs=n_epochs)\n",
    "wandb.finish()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training loss: 0.6691, Validation loss: 0.6776\n",
      "Epoch [2/50], Training loss: 0.6599, Validation loss: 0.6793\n",
      "Epoch [3/50], Training loss: 0.6566, Validation loss: 0.6784\n",
      "Epoch [4/50], Training loss: 0.6528, Validation loss: 0.6759\n",
      "Epoch [5/50], Training loss: 0.6489, Validation loss: 0.6742\n",
      "Epoch [6/50], Training loss: 0.6432, Validation loss: 0.6731\n",
      "Epoch [7/50], Training loss: 0.6391, Validation loss: 0.6714\n",
      "Epoch [8/50], Training loss: 0.6337, Validation loss: 0.6699\n",
      "Epoch [9/50], Training loss: 0.6288, Validation loss: 0.6687\n",
      "Epoch [10/50], Training loss: 0.6224, Validation loss: 0.6672\n",
      "Epoch [11/50], Training loss: 0.6172, Validation loss: 0.6673\n",
      "Epoch [12/50], Training loss: 0.6128, Validation loss: 0.6655\n",
      "Epoch [13/50], Training loss: 0.6076, Validation loss: 0.6655\n",
      "Epoch [14/50], Training loss: 0.6032, Validation loss: 0.6644\n",
      "Epoch [15/50], Training loss: 0.5997, Validation loss: 0.6642\n",
      "Epoch [16/50], Training loss: 0.5954, Validation loss: 0.6626\n",
      "Epoch [17/50], Training loss: 0.5909, Validation loss: 0.6625\n",
      "Epoch [18/50], Training loss: 0.5871, Validation loss: 0.6614\n",
      "Epoch [19/50], Training loss: 0.5820, Validation loss: 0.6613\n",
      "Epoch [20/50], Training loss: 0.5791, Validation loss: 0.6610\n",
      "Epoch [21/50], Training loss: 0.5755, Validation loss: 0.6604\n",
      "Epoch [22/50], Training loss: 0.5718, Validation loss: 0.6607\n",
      "Epoch [23/50], Training loss: 0.5681, Validation loss: 0.6589\n",
      "Epoch [24/50], Training loss: 0.5634, Validation loss: 0.6600\n",
      "Epoch [25/50], Training loss: 0.5607, Validation loss: 0.6590\n",
      "Epoch [26/50], Training loss: 0.5570, Validation loss: 0.6586\n",
      "Epoch [27/50], Training loss: 0.5539, Validation loss: 0.6583\n",
      "Epoch [28/50], Training loss: 0.5500, Validation loss: 0.6585\n",
      "Epoch [29/50], Training loss: 0.5472, Validation loss: 0.6578\n",
      "Epoch [30/50], Training loss: 0.5439, Validation loss: 0.6583\n",
      "Epoch [31/50], Training loss: 0.5405, Validation loss: 0.6589\n",
      "Epoch [32/50], Training loss: 0.5374, Validation loss: 0.6573\n",
      "Epoch [33/50], Training loss: 0.5339, Validation loss: 0.6576\n",
      "Epoch [34/50], Training loss: 0.5309, Validation loss: 0.6573\n",
      "Epoch [35/50], Training loss: 0.5274, Validation loss: 0.6573\n",
      "Epoch [36/50], Training loss: 0.5244, Validation loss: 0.6577\n",
      "Epoch [37/50], Training loss: 0.5209, Validation loss: 0.6574\n",
      "Early stopping at epoch 37\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.080 MB of 0.080 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7530fac739e54cd18da88744be8e1d9d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_training_loss</td><td>██▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>average_validation_loss</td><td>▇██▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_training_loss</td><td>0.52095</td></tr><tr><td>average_validation_loss</td><td>0.65739</td></tr><tr><td>epoch</td><td>37</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_5-batch_size_512-n_epochs_50-lr_1e-05-hidden_dim_64</strong> at: <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/six016ln' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/six016ln</a><br/> View project at: <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241006_014735-six016ln\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "id": "5b5ef8c4a2461190",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation\n",
    "The model will be evaluated for the key metrics of:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "The results will be averaged using micro averaging because I care about the total number of correct prediction regardless of the class (\"yes\" or \"no\"). \n",
    "\n",
    "Errors will be evaluated by making a confusion matrix and giving me the distribution of ture positives, false positives, true negatives and false negatives. Helping me figure out where the model is making most of it's mistakes."
   ]
  },
  {
   "cell_type": "code",
   "id": "99adb580a3aa62c8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T00:21:21.845378Z",
     "start_time": "2024-10-06T00:21:21.843671Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84f45c239d144cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Finish the WandB run\n",
    "Closing the WandB run"
   ]
  },
  {
   "cell_type": "code",
   "id": "bffc6238cff90999",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T00:21:21.874953Z",
     "start_time": "2024-10-06T00:21:21.871936Z"
    }
   },
   "source": "# wandb.finish()",
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "60dffd5636dc69ec",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Interpretation\n",
    "\n",
    "To set concrete expectations for my model I take into account a couple of key benchmarks:\n",
    "- **Accuracy:** Given the task of binary classification an accuracy of ~50% can be achieved with random guesses.\n",
    "    - Expecting my model to hit an accuracy of ~60-75%.\n",
    "- **F1 Score:** For this dataset I expect the F1 score to be similar to the accuracy of ~60-75%"
   ]
  },
  {
   "cell_type": "code",
   "id": "eea5c07a77e6af43",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T00:21:21.894492Z",
     "start_time": "2024-10-06T00:21:21.892981Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
