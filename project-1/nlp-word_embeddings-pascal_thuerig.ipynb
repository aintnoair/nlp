{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af98235081195f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP - Word Embeddings - Pascal Thürig"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "Starting point for this project is the following key requirements:\n",
    "1. Use the BoolQ Dataset from Hugging Face\n",
    "2. Use pre-trained model for word embeddings (word2vec, GloVe or fastText)\n",
    "3. Train a 2-layer classifier with ReLU non-linearity\n",
    "\n",
    "In this project I will be using pre-trained embeddings from word2vec and a simple 2-layer neural network to do the reading comprehension task on the BoolQ dataset.\n",
    "I will document every decision made, from preprocessing to model training and evaluation. The goal is to classify each BoolQ question-answer pair as either 'Yes' or 'No'.\n",
    "\n",
    "The data consists of the questions, a passage and the answer. In total there are 12'697 entries in the dataset. Splitting them into train (8427), validation (1000) and test (3270).\n",
    "\n",
    "The following experiment have been performed:\n",
    "- Learning rates: 1e-2 - 1e-6\n",
    "- Batch Sizes: 64 - 512\n",
    "- Epochs: 20 - 50\n",
    "- Hidden Dimension: 64, 96, 128, 256\n",
    "- Early stopping: 3 - 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## TLDR; Here are the key decisions and justifications:\n",
    "- BoolQ Dataset: Provided by Project berief\n",
    "- Task: Classify BoolQ questions as either \"yes\" or \"no\" using pre-trained embeddings and a simple neural network\n",
    "- Pre-trained embeddings: word2vec - Google News 300; for simplicity and already have a bit of experience with it\n",
    "- Model: 2-layer NN with ReLU activation: Provided by Project brief\n",
    "- Tokenizing: Yes, using a subword tokenizer.\n",
    "- Lowercasing: Yes, all text will be lowercased.\n",
    "- Stemming: No, stemming will not be applied.\n",
    "- Lemmatizing: No, lemmatizing is not used initially but could be tried later.\n",
    "- Stopword removal: No, stopwords are not removed to retain key information.\n",
    "- Removal of other words: No, no other word removal is planned.\n",
    "- Format cleaning: No further cleaning required, the dataset is already clean.\n",
    "- Truncation: Yes, input text is truncated to a maximum of 512 tokens.\n",
    "- Feature selection: None, relying on word2vec embeddings directly.\n",
    "- Input format: Tokenized and padded sequences of word2vec embeddings.\n",
    "- Label format: Binary (1 for \"yes\", 0 for \"no\").\n",
    "- train/valid/test splits: 66% train, 8% validation, 26% test.\n",
    "- Padding: Yes, sequences are padded for uniform input length.\n",
    "- Embedding: Pre-trained word2vec embeddings are used for simplicity.\n",
    "- Planned correctness tests: Shape consistency checks, binary label correctness, and validation of truncation and padding.\n",
    "- Hyperparameters:\n",
    "    - Learning Rate: 1e-2 – 1e-5\n",
    "    - Batch Size: 16 - 64 (choosing maximum possible that my GPU can handle)\n",
    "    - Epochs 10 - 50 in 5-/10-step increments\n",
    "    - Hidden size: 64 - 512 \n",
    "    - Early Stopping: Patience of 3 - 10 Epochs of non-improvement (depending on total epoch number)\n",
    "- Evaluation: Accuracy and F1-Score: Accuracy for general performance and F1 to handle class imbalances\n",
    "- Error Analysis: Investigating False Positives and False negatives to understand where the model fails"
   ],
   "id": "435001ad93aa5edc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Importing necessary libraries:\n",
    "- datasets\n",
    "- gensim\n",
    "- transformers\n",
    "- numpy\n",
    "- torch\n",
    "- wandb\n",
    "- sklearn"
   ],
   "id": "41b47c95d26e625d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Importing and installing all necessary libraries\n",
    "\n",
    "%pip install datasets gensim transformers numpy torch wandb scikit-learn\n",
    "\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ],
   "id": "cb8228d03fda3c51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First up the BoolQ dataset is loaded",
   "id": "776daebf1983e2dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Downloading the required BoolQ dataset and splitting it like required from the project presentation\n",
    "\n",
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')\n",
    "\n",
    "\n",
    "test_question = train_data[5]['question']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")"
   ],
   "id": "bfaac9a57eb60a59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For easy access during experiments I like to define the hyperparameters at the top of my notebooks\n",
    "\n",
    "In retrospect defining my hyperparameters up here was more of a hassle than anything else I probably will not be doing this anymore, but you live and you learn...\n",
    "As I did the experiments manually and not with a hyperparameter tuner like optuna I found myself constantly scrolling through the notebook."
   ],
   "id": "d1d312b840870f78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 300\n",
    "max_seq_length = 512\n",
    "\n",
    "# Hyperparamters of my best performing run (val_loss)\n",
    "run_number = 15\n",
    "batch_size = 512\n",
    "n_epochs = 50\n",
    "learning_rate = 1e-5\n",
    "hidden_dim = 256\n",
    "patience = 5\n",
    "\n",
    "# defining the WandB project and run name\n",
    "wandb_project_name = \"nlp-word_embeddings-pascal_thuerig\"\n",
    "wandb_run_name = f\"run_{run_number}-batch_size_{batch_size}-n_epochs_{n_epochs}-lr_{learning_rate}-hidden_dim_{hidden_dim}\"\n",
    "\n",
    "# calculating the input and output dimension as I play around with the hidden dimension as a hyperparameter\n",
    "sequence_length = max_seq_length * 2  # Concatenate question and passage\n",
    "input_dim = sequence_length * EMBEDDING_DIM\n",
    "output_dim = 2  # Binary classification\n",
    "\n",
    "print(f\"Seq Length: {sequence_length}\")\n",
    "print(f\"Input dim: {input_dim}\")"
   ],
   "id": "cdb4e3610fb8f723"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the pre-trained embeddings from word2vec - word2vec-google-news-300\n",
    "\n",
    "This also changed: I am using fastText as I ran into lots of problems with subword embeddings and word2vec. Found out they seemingly don't match very well. I still wanted to keep the subword embeddings however, hoping for better performance. "
   ],
   "id": "cf040af81aa3f4ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = \"fasttext-wiki-news-subwords-300\"\n",
    "model_path = \"fasttext-wiki-news-subwords-300.model\"\n",
    "\n",
    "# Check if the model file exists\n",
    "try:\n",
    "    # Load the model if it exists locally\n",
    "    embeddings_model = gensim.models.KeyedVectors.load(model_path)\n",
    "    print(\"Model loaded from local storage.\")\n",
    "except FileNotFoundError:\n",
    "    # Download and save the model if it doesn't exist\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    embeddings_model = api.load(model_name)\n",
    "    embeddings_model.save(model_path)  # Save the model locally\n",
    "    print(\"Model downloaded and saved to local storage.\")"
   ],
   "id": "371710b7221a0b7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "The BoolQ data will be processed in the following way:\n",
    "1.  Tokenizing: the input questions and passages using a subword tokenizer\n",
    "2.  Lowercasing: the text for simplicity and to reduce the total vocabulary size\n",
    "3.  Stemming: No, will not stem the words as to not lose information\n",
    "4.  Lemmatizing: No, will try if it improves performance\n",
    "5.  Stopword removal: No, will not be removed to not lose potentially critical information [research](https://datascience.stackexchange.com/questions/31048/pros-cons-of-stop-word-removal)\n",
    "6.  Removal of other words: No, will not be removing any other words\n",
    "7.  Format cleaning: The dataset is already sufficiently clean, it shouldn't impact performance\n",
    "8.  Truncation: the input text is truncated to a maximum of 512 tokens\n",
    "9.  Feature selection: Not applicable as we focus on raw text as input and leveraging the pre-trained word embeddings no further feature extraction is needed.\n",
    "10. Input format: Will take the form of the tokenized and padded sequences of word embeddings\n",
    "11. Label format: Binary labels \"yes\" or \"no\"\n",
    "12. train/valid/test splits: Prerequisite to project (66/8/26)\n",
    "13. Padding: the sequences is padded to ensure all inputs have the same length in each batch\n",
    "14. Embedding: Using word2vec, solely for simplicity as I already know it.\n",
    "15. Planned correctness tests: Check for shape mismatches between tokenized text and word embeddings. - Ensure that input sequences are properly truncated and padded. - Verify that binary labels are correctly assigned and match the expected outputs.\n"
   ],
   "id": "cd6c20aba906a77a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Preprocess text (lowercasing)",
   "id": "bc99d75839877e99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Lowercasing all text\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "print(lowercase_text(test_question))"
   ],
   "id": "d1de26da4036eb1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. Tokenize with AutoTokenizer from Hugging Face\n",
    "    I wanted to use the AutoTokenizer to get subword embeddings, hoping for a performance improvement. Also, I could easily pad and truncate the sequences within the tokenizer."
   ],
   "id": "fb096d41f358f377"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Found out I could pad and truncate straight in the tokenizer when reading the documentation\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text,\n",
    "                     padding='max_length',    \n",
    "                     truncation=True,         \n",
    "                     max_length=max_seq_length,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "tokenized_output = tokenize_text(test_question)\n",
    "\n",
    "# Print tokens to check visually and manually if the look alright\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_output['input_ids'][0]))\n",
    "test_input_ids = tokenized_output['input_ids'][0]\n",
    "print(test_input_ids)\n"
   ],
   "id": "a6612d4f2ef8d82b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Truncate or add padding -> *found out I can already do this in the tokenizer*",
   "id": "da8d16d81da08a7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5d1548230b5579d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. Preprocess Pipeline\n",
    "\n",
    "Applying my little preprocessing that I am doing to the text"
   ],
   "id": "470bfc20ed52ee31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_pipeline(text):\n",
    "    text = lowercase_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # Ensure tokenized length is correct\n",
    "    assert tokens['input_ids'].shape[1] == max_seq_length, \\\n",
    "        f\"Tokenized length is not equal to max_seq_length: {tokens['input_ids'].shape[1]}\"\n",
    "    \n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    \n",
    "    return token_strings\n",
    "\n",
    "preprocessed_text = preprocess_pipeline(test_question)\n",
    "print(\"Tokens:\", preprocessed_text)\n"
   ],
   "id": "77e7b8fc40bfec2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. Embed tokens using word2vec (word2vec-google-news-300)\n",
    "\n",
    "You might have guessed; Here I implement the fastText model and not the word2vec model anymore. The fastText model can handle oov words as well as the subword embeddings, this is why I switched and used it instead of word2vec."
   ],
   "id": "308c963876f2b242"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# I struggled with implementing this and getting an output that wasn't all zeros before switching to the fastText model. after changing out the model it started working reliably.\n",
    "# This function is a mix of my own implementation according to documentation and some AI help until it worked.\n",
    "\n",
    "def tokens_to_embeddings(tokens, embedding_model=embeddings_model, embedding_dim=EMBEDDING_DIM):\n",
    "    embeddings = []\n",
    "    for t in tokens:\n",
    "        if t in embedding_model:\n",
    "            embeddings.append(embedding_model[t])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "embedded_text = tokens_to_embeddings(preprocessed_text)\n",
    "print(embedded_text)"
   ],
   "id": "74a8a11a0bb432b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6. Create a custom BoolQ dataset class to:\n",
    "    - get the data into a compatible format for the pyTorch dataloader.\n",
    "    - organize question-answer pairs and apply the preprocessing pipeline.\n",
    "    - easily batch, shuffle, and load the data during training.\n",
    "\n",
    "\n",
    "Implemented this custom dataset class by following the documentation on [PyTorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)"
   ],
   "id": "94de258dd58ca75a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_length=max_seq_length):\n",
    "        self.data = data\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data[idx]['question']\n",
    "        passage = self.data[idx]['passage']\n",
    "        \n",
    "        label = 1 if self.data[idx]['answer'] else 0\n",
    "        \n",
    "        question_tokens = preprocess_pipeline(question)\n",
    "        passage_tokens = preprocess_pipeline(passage)\n",
    "        \n",
    "        question_embeddings = tokens_to_embeddings(question_tokens)\n",
    "        passage_embeddings = tokens_to_embeddings(passage_tokens)\n",
    "        \n",
    "        embeddings = np.concatenate((question_embeddings, passage_embeddings), axis=0).flatten()\n",
    "        \n",
    "        return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ],
   "id": "ba415af3c954cae7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "7. Dataloaders as required by pyTorch\n",
    "\n",
    "Implemented by following the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders)"
   ],
   "id": "2b132fbbeb25d1a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = BoolQDataset(train_data, embeddings_model)\n",
    "validation_dataset = BoolQDataset(validation_data, embeddings_model)\n",
    "test_dataset = BoolQDataset(test_data, embeddings_model)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "a6bf279f25298977"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8. Initialize weights and biases for experiment tracking",
   "id": "765416f13a85bf40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "wandb.init(project=wandb_project_name, name=wandb_run_name, config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"hidden_size\": hidden_dim,\n",
    "    \"patience\": patience,\n",
    "})\n"
   ],
   "id": "931f18c0a65deeea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "The model architecture for this project is already fixed in the project brief as follows:\n",
    "- **Network Architecture:** 2-Layer with ReLu non-linearity.\n",
    "- **Loss / Optimizer:** Loss: CrossEntropyLoss / Optimizer: Adam (potentially trying SGD with or without momentum in experiments)\n",
    "- **Experiments to run**: Mentioned in Training section below\n",
    "- **Number of training runs**: Will depend on number of experiments\n",
    "- **Checkpointing / Early stopping:** 3 - 10 epochs of non-improvement of the validation loss\n",
    "- **Planned correctness tests:** Shape and Dimension consistency tests, Gradient Check, Sanity Check & Prediction Testd"
   ],
   "id": "4f2d7d34c7d11d09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Creating the neural network class:",
   "id": "d6c6adf6bb475aa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setting up the NN as required by the Project description as a 2-layer with ReLU non-linearity inbetween\n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "798b82384ffd9b6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Create instance of model and move it to the GPU",
   "id": "b5455d278a9c8f69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "2eb4e3dcab58916f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoLayerNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "print(device)"
   ],
   "id": "2e2deb953cb94be7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. Loss (nn.CrossEntropyLoss) and optimizer (optim.Adam)\n",
    "\n",
    "CrossEntropy loss is used as it is the most commonly used loss for classification tasks\n",
    "The Adam optimizer was chosen because it adjusts learning rates for faster, more efficient training, is particularly suited for complex, noisy NLP tasks."
   ],
   "id": "65578563e4fd5b1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "id": "dfd7306c34142b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. Training loop",
   "id": "8dd66e315baa85c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, save_path=\"best_model.pth\"):\n",
    "    early_stop_counter = 0\n",
    "    best_val_loss = np.inf\n",
    "    best_model_wts = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                \n",
    "                # add to total validation loss\n",
    "                val_loss += criterion(val_outputs, val_labels).item()\n",
    "                \n",
    "                # Convert outputs to predictions\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                \n",
    "                # Store predictions and true labels for metric calculation\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(val_labels.cpu().numpy())\n",
    "        \n",
    "        # Average validation loss\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Calculate metrics with sklearn library\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions)\n",
    "        recall = recall_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions)\n",
    "        conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "        \n",
    "        # print metrics and confusion matrix\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training loss: {avg_train_loss:.4f}, Validation loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n {conf_matrix}\")\n",
    "        \n",
    "        # logging to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"average_training_loss\": avg_train_loss,\n",
    "            \"average_validation_loss\": avg_val_loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"confusion_matrix\": conf_matrix,\n",
    "        })\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return best_model_wts"
   ],
   "id": "6453b8f7c882c689"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c2481b7d7c566944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5e2e26a495a24b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Train the model with the following different hyperparameters:\n",
    "- Learning rate: 1e-2 – 1e-5\n",
    "- Batch size: 16 - 64\n",
    "- Epochs: 10 - 50\n",
    "- Hidden size: 64 - 512\n",
    "- Early Stopping: Patience of 3 - 10 Epochs of non-improvement\n"
   ],
   "id": "6fbecb71c1c8edb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_model(model, train_loader, validation_loader, criterion, optimizer, epochs=n_epochs)\n",
    "wandb.finish()"
   ],
   "id": "94123c85b55178a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The model will be evaluated for the key metrics of:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "The results will be averaged using micro averaging because I care about the total number of correct prediction regardless of the class (\"yes\" or \"no\"). \n",
    "\n",
    "Errors will be evaluated by making a confusion matrix and giving me the distribution of ture positives, false positives, true negatives and false negatives. Helping me figure out where the model is making most of it's mistakes.\n",
    "\n",
    "Doing the evaluation in the training loop and logging everything to my [WandB](https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/overview)\n",
    "\n",
    "The model is mostly predicting \"yes\" as the dataset is slightly imbalanced. "
   ],
   "id": "d80fd80bdfd56e78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8622672e9389838"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "### Batch size and learning rate\n",
    "\n",
    "When testing different hyperparameter combinations it was clear quite quickly that a large batch size of 512 was performing consistently better than smaller batch sizes. So I stuck with that for further experiments and hyperparameter combinations. Changing the learning rate had inconsistent effects on performance. Lowering it all the way to 1e-2 would result in a drastic drop in the validation loss but would oscillate and very quickly get caught by the early stop criteria. Increasing the learning rate to 1e-6 however would result in extremely long and slowly improving runs with worse performance than a lower lr of 1e-5. I found 1e-5 to be the best performing learning rate.\n",
    "\n",
    "### Hidden Size\n",
    "\n",
    "Increasing the hidden dimension would drastically increase the runtime. Other than that it seemingly had next to no impact on the performance of the model. When increasing the hidden size the model would more consistently only predict a single class all the time as seen by the confusion matrix in [WandB](https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/overview)\n",
    "\n",
    "### Epochs and early stopping\n",
    "\n",
    "I found that early stopping after 5 epochs of non improvement was a good point to stop as it gave the model chance to recover. I have not experimented with the number of epochs as much as I had planned. I kept it at a maximum of 50 for most runs and let the early stopping criteria handle runs that were not improving anymore. This gave some runs the chance to take their time so to speak and gradually improve.\n",
    "\n",
    "# Results\n",
    "\n",
    "The runs delivered mixed and confusing results. For example the run with the best accuracy had one of the worst validation losses of all runs. The run with the best validation loss had one of the worst accuracies and would simply predict a single class over 80% of the time. The increase in batch size would result in the model predicting just one class most of the time when runs with smaller batch sizes were more even in their choice of class."
   ],
   "id": "c92b8b6a69697de3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "To set concrete expectations for my model I take into account a couple of key benchmarks:\n",
    "- **Accuracy:** Given the task of binary classification an accuracy of ~50% can be achieved with random guesses.\n",
    "    - Expecting my model to hit an accuracy of ~60-75%.\n",
    "- **F1 Score:** For this dataset I expect the F1 score to be similar to the accuracy of ~60-75%"
   ],
   "id": "1298651ef4c3260e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Looking at the results in Weights and Biases it is difficult to say this model performs at all. My implementation could undoubtedly be improved upon however I believe a 2-layer model is just not able to depict the complexity that is required for a reading comprehension. I suspect this might even be the purpose of this first project to introduce us to the complexity that is nlp. \n",
   "id": "ed41c84100f38120"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stuff to improve:\n",
    "- Experiment tracking\n",
    "- Model evaluation\n",
    "- Find out how an example project of this would look like\n",
    "\n",
    "A big unknown for me in this project was the experiment tracking and evaluation of the model. I tried a lot that did not work at first and ran out of time in the end. This has been my first project I completed without a group, and it showed me how much I still don't know. Lacking experience and the knowledge of how to professionally complete such a project are areas I want to improve in during this semester. I want to learn how to not only get the model working but how to tune and optimize the hyperparameters, track everything and make it reproducible. These are all areas where I have little to no experience so far. I am looking forward to see how other students have tackled this project and try and learn from their implementation and get a better understanding of how to do such a project in general."
   ],
   "id": "6b10cb2669ab4d39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e0351466721a2863"
  },
  {
   "cell_type": "code",
   "id": "eea5c07a77e6af43",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T00:21:21.894492Z",
     "start_time": "2024-10-06T00:21:21.892981Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
