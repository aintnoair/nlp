{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af98235081195f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP - Word Embeddings - Pascal Thürig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b9a787988edcb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "Starting point for this project is the following key requirements:\n",
    "1. Use the BoolQ Dataset from Hugging Face\n",
    "2. Use pre-trained model for word embeddings (word2vec, GloVe or fastText)\n",
    "3. Train a 2-layer classifier with ReLU non-linearity\n",
    "\n",
    "In this project I will be using pre-trained embeddings from word2vec and a simple 2-layer neural network to do the reading comprehension task on the BoolQ dataset.\n",
    "I will document every decision made, from preprocessing to model training and evaluation. The goal is to classify each BoolQ question-answer pair as either 'Yes' or 'No'.\n",
    "\n",
    "\n",
    "\n",
    "## TLDR; Here are the key decisions and justifications:\n",
    "- BoolQ Dataset: Provided by Project berief\n",
    "- Task: Classify BoolQ questions as either \"yes\" or \"no\" using pre-trained embeddings and a simple neural network\n",
    "- Pre-trained embeddings: word2vec - Google News 300; for simplicity and already have a bit of experience with it\n",
    "- Model: 2-layer NN with ReLU activation: Provided by Project brief\n",
    "- Tokenizing: Yes, using a subword tokenizer.\n",
    "- Lowercasing: Yes, all text will be lowercased.\n",
    "- Stemming: No, stemming will not be applied.\n",
    "- Lemmatizing: No, lemmatizing is not used initially but could be tried later.\n",
    "- Stopword removal: No, stopwords are not removed to retain key information.\n",
    "- Removal of other words: No, no other word removal is planned.\n",
    "- Format cleaning: No further cleaning required, the dataset is already clean.\n",
    "- Truncation: Yes, input text is truncated to a maximum of 512 tokens.\n",
    "- Feature selection: None, relying on word2vec embeddings directly.\n",
    "- Input format: Tokenized and padded sequences of word2vec embeddings.\n",
    "- Label format: Binary (1 for \"yes\", 0 for \"no\").\n",
    "- train/valid/test splits: 66% train, 8% validation, 26% test.\n",
    "- Padding: Yes, sequences are padded for uniform input length.\n",
    "- Embedding: Pre-trained word2vec embeddings are used for simplicity.\n",
    "- Planned correctness tests: Shape consistency checks, binary label correctness, and validation of truncation and padding.\n",
    "- Hyperparameters:\n",
    "    - Learning Rate: 1e-2 – 1e-5\n",
    "    - Batch Size: 16 - 64 (choosing maximum possible that my GPU can handle)\n",
    "    - Epochs 10 - 50 in 5-/10-step increments\n",
    "    - Hidden size: 64 - 512 \n",
    "    - Early Stopping: Patience of 3 - 10 Epochs of non-improvement (depending on total epoch number)\n",
    "- Evaluation: Accuracy and F1-Score: Accuracy for general performance and F1 to handle class imbalances\n",
    "- Error Analysis: Investigating False Positives and False negatives to understand where the model fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923605f52ff9399",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup\n",
    "Importing necessary libraries:\n",
    "- datasets\n",
    "- gensim\n",
    "- nltk\n",
    "- transformers\n",
    "- numpy\n",
    "- torch\n",
    "- wandb\n",
    "- sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62f9167e7b46f14a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:50.221496Z",
     "start_time": "2024-10-05T14:02:49.320439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.3.3)\r\n",
      "Requirement already satisfied: optuna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.35.2)\r\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.3)\r\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\r\n",
      "Requirement already satisfied: wandb in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.18.2)\r\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.13.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.4.4)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.66.5)\r\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.10.7)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.25.1)\r\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0)\r\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.11.4)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (7.0.4)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (1.13.3)\r\n",
      "Requirement already satisfied: colorlog in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (6.8.2)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (2.0.35)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2023.10.3)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.15.0)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2.8.8)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (4.2.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (5.26.1)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (5.9.5)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (2.14.0)\r\n",
      "Requirement already satisfied: setproctitle in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (69.2.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Requirement already satisfied: Mako in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.13.1)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\r\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets gensim optuna transformers numpy torch wandb scikit-learn\n",
    "\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import itertools\n",
    "import optuna\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7004488f0ec3eb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First up the BoolQ dataset is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "290a1930d5aa00db",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:56.280443Z",
     "start_time": "2024-10-05T14:02:50.213408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you use oyster card at epsom station', 'answer': False, 'passage': \"Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\"}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')\n",
    "\n",
    "\n",
    "test_question = train_data[5]['question']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e81ed52db754a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For easy access during experiments I like to define the hyperparameters at the top of my notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27891755da01cbdd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:56.294872Z",
     "start_time": "2024-10-05T14:02:56.260445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq Length: 1024\n",
      "Input dim: 307200\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 300\n",
    "max_seq_length = 512\n",
    "\n",
    "learning_rates = np.logspace(-2, -5, num=4)\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs = range(10, 51, 10)\n",
    "hidden_sizes = range(32, 513, 32)\n",
    "patiences = range(3, 4)\n",
    "\n",
    "wandb_project_name = \"nlp-word_embeddings-pascal_thuerig\"\n",
    "\n",
    "sequence_length = max_seq_length * 2  # Concatenate question and passage\n",
    "input_dim = sequence_length * EMBEDDING_DIM\n",
    "output_dim = 2  # Binary classification\n",
    "\n",
    "print(f\"Seq Length: {sequence_length}\")\n",
    "print(f\"Input dim: {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac159201e4f8349",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "New: Loading the fastText model to handle subword tokens from AutoTokenizer - ran into problems with word2vec\n",
    "Deprecated: *Now the pre-trained embeddings from word2vec - word2vec-google-news-300*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca7cd134842c8958",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.043650Z",
     "start_time": "2024-10-05T14:02:56.265703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from local storage.\n"
     ]
    }
   ],
   "source": [
    "# Check if the model file exists\n",
    "model_name = \"fasttext-wiki-news-subwords-300\"\n",
    "model_path = \"fasttext-wiki-news-subwords-300.model\"\n",
    "\n",
    "try:\n",
    "    # Load the model if it exists locally\n",
    "    embeddings_model = gensim.models.KeyedVectors.load(model_path)\n",
    "    print(\"Model loaded from local storage.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    embeddings_model = api.load(model_name)\n",
    "    embeddings_model.save(model_path)\n",
    "    print(\"Model downloaded and saved to local storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ae072eeb522f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "The BoolQ data will be processed in the following way:\n",
    "1.  Tokenizing: the input questions and passages using a subword tokenizer\n",
    "2.  Lowercasing: the text for simplicity and to reduce the total vocabulary size\n",
    "3.  Stemming: No, will not stem the words as to not lose information\n",
    "4.  Lemmatizing: No, will try if it improves performance\n",
    "5.  Stopword removal: No, will not be removed to not lose potentially critical information [research](https://datascience.stackexchange.com/questions/31048/pros-cons-of-stop-word-removal)\n",
    "6.  Removal of other words: No, will not be removing any other words\n",
    "7.  Format cleaning: The dataset is already sufficiently clean, it shouldn't impact performance\n",
    "8.  Truncation: the input text is truncated to a maximum of 512 tokens\n",
    "9.  Feature selection: Not applicable as we focus on raw text as input and leveraging the pre-trained word embeddings no further feature extraction is needed.\n",
    "10. Input format: Will take the form of the tokenized and padded sequences of word embeddings\n",
    "11. Label format: Binary labels \"yes\" or \"no\"\n",
    "12. train/valid/test splits: Prerequisite to project (66/8/26)\n",
    "13. Padding: the sequences is padded to ensure all inputs have the same length in each batch\n",
    "14. Embedding: Using word2vec, solely for simplicity as I already know it.\n",
    "15. Planned correctness tests: Check for shape mismatches between tokenized text and word embeddings. - Ensure that input sequences are properly truncated and padded. - Verify that binary labels are correctly assigned and match the expected outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5c0360f3733c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Preprocess text (lowercasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33f5bbffdb41ff92",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.047148Z",
     "start_time": "2024-10-05T14:02:57.043142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "print(lowercase_text(test_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb45d935e577dc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Tokenize with AutoTokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8718b756b58eb0f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.215303Z",
     "start_time": "2024-10-05T14:02:57.047532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'can', 'you', 'use', 'oyster', 'card', 'at', 'eps', '##om', 'station', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Token: [CLS] - ID: 101\n",
      "Token: can - ID: 2064\n",
      "Token: you - ID: 2017\n",
      "Token: use - ID: 2224\n",
      "Token: oyster - ID: 21480\n",
      "Token: card - ID: 4003\n",
      "Token: at - ID: 2012\n",
      "Token: eps - ID: 20383\n",
      "Token: ##om - ID: 5358\n",
      "Token: station - ID: 2276\n",
      "Token: [SEP] - ID: 102\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n",
      "Token: [PAD] - ID: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text,\n",
    "                     padding='max_length',    \n",
    "                     truncation=True,         \n",
    "                     max_length=max_seq_length,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "tokenized_output = tokenize_text(test_question)\n",
    "\n",
    "# Print tokens\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_output['input_ids'][0]))\n",
    "\n",
    "# Print input_ids and their corresponding tokens\n",
    "test_input_ids = tokenized_output['input_ids'][0]\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_input_ids)\n",
    "for test_token, test_id in zip(test_tokens, test_input_ids):\n",
    "    print(f\"Token: {test_token} - ID: {test_id.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668490c2f22c73e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Truncate or add padding -> *found out I can already do this in the tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da27021cbde658ea",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.216698Z",
     "start_time": "2024-10-05T14:02:57.214340Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2024c0694cc02f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7019b2ed6d1b5df3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.231611Z",
     "start_time": "2024-10-05T14:02:57.217891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'can', 'you', 'use', 'oyster', 'card', 'at', 'eps', '##om', 'station', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pipeline(text):\n",
    "    text = lowercase_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # Ensure tokenized length is correct\n",
    "    assert tokens['input_ids'].shape[1] == max_seq_length, \\\n",
    "        f\"Tokenized length is not equal to max_seq_length: {tokens['input_ids'].shape[1]}\"\n",
    "    \n",
    "    # Convert token IDs back to token strings for FastText\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    \n",
    "    return token_strings\n",
    "\n",
    "# Now when you preprocess the text, you get the token strings (subword tokens) compatible with FastText\n",
    "preprocessed_text = preprocess_pipeline(test_question)\n",
    "print(\"Tokens:\", preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480916ef022ad32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Embed tokens using word2vec (word2vec-google-news-300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a08858dc075f9bf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.231760Z",
     "start_time": "2024-10-05T14:02:57.225212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [-0.029857 -0.034625 -0.038095 ... -0.018202  0.049474  0.021932]\n",
      " [-0.047138  0.043438  0.024106 ... -0.011364 -0.037262 -0.031316]\n",
      " ...\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_embeddings(tokens, embedding_model=embeddings_model, embedding_dim=EMBEDDING_DIM):\n",
    "    embeddings = []\n",
    "    for t in tokens:\n",
    "        if t in embedding_model:\n",
    "            embeddings.append(embedding_model[t])  # Get the FastText embedding for the token\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))  # OOV tokens get zero vector\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Now get embeddings for the preprocessed tokens\n",
    "embedded_text = tokens_to_embeddings(preprocessed_text)\n",
    "print(embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406a75fa663ed2f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. Create a custom BoolQ dataset class to:\n",
    "    - get the data into a compatible format for the pyTorch dataloader.\n",
    "    - organize question-answer pairs and apply the preprocessing pipeline.\n",
    "    - easily batch, shuffle, and load the data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "68b80b754455711f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.263210Z",
     "start_time": "2024-10-05T14:02:57.232347Z"
    }
   },
   "outputs": [],
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data, embedding_model, max_seq_length=max_seq_length):\n",
    "        self.data = data\n",
    "        self.embedding_model = embedding_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data[idx]['question']\n",
    "        passage = self.data[idx]['passage']\n",
    "        \n",
    "        label = 1 if self.data[idx]['answer'] else 0\n",
    "        \n",
    "        question_tokens = preprocess_pipeline(question)\n",
    "        passage_tokens = preprocess_pipeline(passage)\n",
    "        \n",
    "        question_embeddings = tokens_to_embeddings(question_tokens)\n",
    "        passage_embeddings = tokens_to_embeddings(passage_tokens)\n",
    "        \n",
    "        embeddings = np.concatenate((question_embeddings, passage_embeddings), axis=0).flatten()\n",
    "        \n",
    "        return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a911c23aa942989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. Datasets for DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fdc7fd0fa951faa3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.263553Z",
     "start_time": "2024-10-05T14:02:57.235972Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = BoolQDataset(train_data, embeddings_model)\n",
    "validation_dataset = BoolQDataset(validation_data, embeddings_model)\n",
    "test_dataset = BoolQDataset(test_data, embeddings_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c1b1d4d889aac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. Initialize weights and biases for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4d8b0f2e45c1aed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.263659Z",
     "start_time": "2024-10-05T14:02:57.238191Z"
    }
   },
   "outputs": [],
   "source": [
    "# moved section to train section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41588a415c4a9457",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model\n",
    "\n",
    "The model architecture for this project is already fixed in the project brief as follows:\n",
    "- **Network Architecture:** 2-Layer with ReLu non-linearity.\n",
    "- **Loss / Optimizer:** Loss: CrossEntropyLoss / Optimizer: Adam (potentially trying SGD with or without momentum in experiments)\n",
    "- **Experiments to run**: Mentioned in Training section below\n",
    "- **Number of training runs**: Will depend on number of experiments\n",
    "- **Checkpointing / Early stopping:** 3 - 10 epochs of non-improvement of the validation loss\n",
    "- **Planned correctness tests:** Shape and Dimension consistency tests, Gradient Check, Sanity Check & Prediction Testd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a17c9339ed26b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Creating the neural network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e88677b1ed7c692d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.264136Z",
     "start_time": "2024-10-05T14:02:57.241532Z"
    }
   },
   "outputs": [],
   "source": [
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa8093a7ab3628",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Create instance of model and move it to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7faae51ad4eb6f1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.264260Z",
     "start_time": "2024-10-05T14:02:57.245600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# moved model definition down to train section\n",
    "# model = TwoLayerNN(input_dim, hidden_dim, output_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdaaae9cb17991",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Loss (nn.CrossEntropyLoss) and optimizer (optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a9517bd58eaa7cd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.264450Z",
     "start_time": "2024-10-05T14:02:57.250407Z"
    }
   },
   "outputs": [],
   "source": [
    "# moved down to optuna objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a130a8db2718d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.264528Z",
     "start_time": "2024-10-05T14:02:57.252822Z"
    }
   },
   "id": "5848553aa0d75f07"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e70ec39db30393ed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.264655Z",
     "start_time": "2024-10-05T14:02:57.256584Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, criterion, optimizer, epochs, patience):\n",
    "    early_stop_counter = 0\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in validation_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels).item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(validation_loader)\n",
    "        avg_running_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_running_loss}, Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\"train_loss\": avg_running_loss, \"val_loss\": avg_val_loss, \"epoch\": epoch+1})\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return best_val_loss  # Return the best validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395aa4d914c29cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68905d506c269a9c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:02:57.264699Z",
     "start_time": "2024-10-05T14:02:57.260778Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60e49144e1bbf412",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training\n",
    "Train the model with the following different hyperparameters:\n",
    "- Learning rate: 1e-2 – 1e-5\n",
    "- Batch size: 16 - 64\n",
    "- Epochs: 10 - 50\n",
    "- Hidden size: 64 - 512\n",
    "- Early Stopping: Patience of 3 - 10 Epochs of non-improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3c59674184ff3416",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T14:12:14.566371Z",
     "start_time": "2024-10-05T14:02:57.265282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-05 16:02:57,264] A new study created in memory with name: no-name-ee401eba-0e86-4dcf-addd-546d1af771d3\n",
      "/var/folders/kn/1r08h7jx2hlcz7rndw5qc1d80000gn/T/ipykernel_20441/3543273679.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:2qy558gn) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.053 MB of 0.053 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7179e19d6cf64ba0a09e0f104ac7dea2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss</td><td>0.64816</td></tr><tr><td>val_loss</td><td>0.65957</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">run_1-lr_0.00011573088407327561-bs_32-ep_10-hs_416-patience_4</strong> at: <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/2qy558gn' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/2qy558gn</a><br/> View project at: <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20241005_160003-2qy558gn/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:2qy558gn). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/blackbook/Desktop/dev-5-sem/nlp/project-1/wandb/run-20241005_160257-a74r1isn</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/a74r1isn' target=\"_blank\">run_1-lr_1.203016756246563e-05-bs_16-ep_30-hs_416-patience_3</a></strong> to <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/a74r1isn' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word_embeddings-pascal_thuerig/runs/a74r1isn</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 0.6570972321834239, Validation Loss: 0.6675769469094655\n",
      "Epoch 2/30, Training Loss: 0.6065706814262853, Validation Loss: 0.666551685522473\n",
      "Epoch 3/30, Training Loss: 0.5435310319999137, Validation Loss: 0.6581010520458221\n",
      "Epoch 4/30, Training Loss: 0.48122225902564614, Validation Loss: 0.6570852039352296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[W 2024-10-05 16:12:14,460] Trial 0 failed with parameters: {'learning_rate': 1.203016756246563e-05, 'batch_size': 16, 'epochs': 30, 'hidden_size': 416, 'patience': 3} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/kn/1r08h7jx2hlcz7rndw5qc1d80000gn/T/ipykernel_20441/3543273679.py\", line 24, in objective\n",
      "    best_val_loss = train_model(model, train_loader, validation_loader, criterion, optimizer, epochs, patience)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/kn/1r08h7jx2hlcz7rndw5qc1d80000gn/T/ipykernel_20441/3855687657.py\", line 16, in train_model\n",
      "    optimizer.step()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py\", line 163, in step\n",
      "    adam(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py\", line 311, in adam\n",
      "    func(params,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py\", line 432, in _single_tensor_adam\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-05 16:12:14,461] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[83], line 33\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Create a study object and optimize\u001B[39;00m\n\u001B[1;32m     32\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mminimize\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Log the best parameters\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest hyperparameters: \u001B[39m\u001B[38;5;124m\"\u001B[39m, study\u001B[38;5;241m.\u001B[39mbest_params)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[0;32mIn[83], line 24\u001B[0m, in \u001B[0;36mobjective\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m     21\u001B[0m validation_loader \u001B[38;5;241m=\u001B[39m DataLoader(validation_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Train the model and get the best validation loss\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m best_val_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m wandb\u001B[38;5;241m.\u001B[39mfinish()\n\u001B[1;32m     27\u001B[0m run_number \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[82], line 16\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, validation_loader, criterion, optimizer, epochs, patience)\u001B[0m\n\u001B[1;32m     14\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     15\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 16\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     19\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    371\u001B[0m             )\n\u001B[0;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:163\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    152\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    155\u001B[0m         group,\n\u001B[1;32m    156\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    160\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    161\u001B[0m         state_steps)\n\u001B[0;32m--> 163\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:311\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    309\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 311\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:432\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    430\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 432\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    434\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    436\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    run_number = 1\n",
    "    \n",
    "    # Suggest hyperparameters using Optuna\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 50, step=10)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 512, step=32)\n",
    "    patience = trial.suggest_int(\"patience\", 3, 4)\n",
    "\n",
    "    wandb_run = f\"run_{run_number}-lr_{learning_rate}-bs_{batch_size}-ep_{epochs}-hs_{hidden_size}-patience_{patience}\"\n",
    "    wandb.init(project=wandb_project_name, name=wandb_run, config=trial.params)\n",
    "    \n",
    "    \n",
    "    # Create your model\n",
    "    model = TwoLayerNN(input_dim, hidden_size, output_dim).to(device)  # Make sure to define YourModel\n",
    "    criterion = nn.CrossEntropyLoss()  # Change as necessary\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model and get the best validation loss\n",
    "    best_val_loss = train_model(model, train_loader, validation_loader, criterion, optimizer, epochs, patience)\n",
    "\n",
    "    wandb.finish()\n",
    "    run_number += 1\n",
    "    # Return the best validation loss to Optuna\n",
    "    return best_val_loss\n",
    "\n",
    "# Create a study object and optimize\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Log the best parameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ef8c4a2461190",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation\n",
    "The model will be evaluated for the key metrics of:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "The results will be averaged using micro averaging because I care about the total number of correct prediction regardless of the class (\"yes\" or \"no\"). \n",
    "\n",
    "Errors will be evaluated by making a confusion matrix and giving me the distribution of ture positives, false positives, true negatives and false negatives. Helping me figure out where the model is making most of it's mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99adb580a3aa62c8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-05T14:12:14.553869Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f45c239d144cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Finish the WandB run\n",
    "Closing the WandB run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc6238cff90999",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-05T14:12:14.554873Z"
    }
   },
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dffd5636dc69ec",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Interpretation\n",
    "\n",
    "To set concrete expectations for my model I take into account a couple of key benchmarks:\n",
    "- **Accuracy:** Given the task of binary classification an accuracy of ~50% can be achieved with random guesses.\n",
    "    - Expecting my model to hit an accuracy of ~60-75%.\n",
    "- **F1 Score:** For this dataset I expect the F1 score to be similar to the accuracy of ~60-75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5c07a77e6af43",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-05T14:12:14.555827Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
