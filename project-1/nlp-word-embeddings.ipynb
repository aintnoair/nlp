{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af98235081195f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP Project - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d15ded-f6ec-4ccb-8a7c-7718d161c8bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T21:21:01.842484Z",
     "start_time": "2024-09-30T21:20:58.436790Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506b645f-848a-4316-961b-f0e3aa4724e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T21:24:35.573339Z",
     "start_time": "2024-09-30T21:24:29.986609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'do iran and afghanistan speak the same language', 'answer': True, 'passage': 'Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.'}\n",
      "Number of training samples: 8427 | 4213.5%\n",
      "Number of validation samples: 1000 | 500.0%\n",
      "Number of validation samples: 3270 | 1635.0%\n"
     ]
    }
   ],
   "source": [
    "# Load the BoolQ dataset\n",
    "dataset = load_dataset('google/boolq')\n",
    "train_data = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "validation_data = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "test_data = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "print(train_data[0])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1f664e-5649-4da6-97e3-b1cd00359ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and training parameters\n",
    "model_name = \"word2vec-google-news-300\"\n",
    "model_path = \"word2vec_google_news_300.model\"\n",
    "batch_size = 10\n",
    "n_epochs = 1\n",
    "learning_rate = 0.0001\n",
    "MAX_SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c09ebde-73e4-4981-b0d3-b69989dae360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from local storage.\n"
     ]
    }
   ],
   "source": [
    "# Check if the model file exists\n",
    "try:\n",
    "    # Load the model if it exists locally\n",
    "    word2vec_model = gensim.models.KeyedVectors.load(model_path)\n",
    "    print(\"Model loaded from local storage.\")\n",
    "except FileNotFoundError:\n",
    "    # Download and save the model if it doesn't exist\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    word2vec_model = api.load(model_name)\n",
    "    word2vec_model.save(model_path)  # Save the model locally\n",
    "    print(\"Model downloaded and saved to local storage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T13:03:06.728135Z",
     "start_time": "2024-09-29T13:03:06.595313Z"
    }
   },
   "id": "632d7f6869333236"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c9366e-f4b8-4543-8a2e-246ad9411678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Functions\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def handle_oov_tokens(tokens, word2vec_model, oov_token=\"<UNK>\"):\n",
    "    \"\"\"Handle out-of-vocabulary tokens by replacing them with a specified token.\"\"\"\n",
    "    return [token if token in word2vec_model else oov_token for token in tokens]\n",
    "\n",
    "def pad_or_truncate(tokens, max_length, pad_token=\"<PAD>\"):\n",
    "    \"\"\"Pad or truncate the list of tokens to the specified maximum length.\"\"\"\n",
    "    if len(tokens) > max_length:\n",
    "        return tokens[:max_length]\n",
    "    else:\n",
    "        return tokens + [pad_token] * (max_length - len(tokens))\n",
    "\n",
    "def preprocess_pipeline(text, word2vec_model):\n",
    "    \"\"\"Pipeline for preprocessing text: cleaning, tokenizing, handling OOV tokens, and padding.\"\"\"\n",
    "    text = preprocess_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = handle_oov_tokens(tokens, word2vec_model)\n",
    "    tokens = pad_or_truncate(tokens, MAX_SEQ_LENGTH)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711a9b6e-b821-406b-a77e-65817a56b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to embeddings\n",
    "def tokens_to_embeddings(tokens, word2vec_model, embedding_dim=300):\n",
    "    \"\"\"Convert a list of tokens to their corresponding embeddings.\"\"\"\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec_model:\n",
    "            embeddings.append(word2vec_model[token])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))  # Use zero vector for OOV\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d536696d-09ff-45d9-8f6c-9518c4bc36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data, word2vec_model, max_seq_length=MAX_SEQ_LENGTH):\n",
    "        self.data = data\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.data[idx]['question']\n",
    "        passage = self.data[idx]['passage']\n",
    "        \n",
    "        # Use 'answer' instead of 'label'\n",
    "        label = 1 if self.data[idx]['answer'] else 0  # Convert boolean to binary\n",
    "\n",
    "        # Preprocess the question and passage\n",
    "        question_tokens = preprocess_pipeline(question, self.word2vec_model)\n",
    "        passage_tokens = preprocess_pipeline(passage, self.word2vec_model)\n",
    "\n",
    "        # Convert tokens to embeddings\n",
    "        question_embeddings = tokens_to_embeddings(question_tokens, self.word2vec_model)\n",
    "        passage_embeddings = tokens_to_embeddings(passage_tokens, self.word2vec_model)\n",
    "\n",
    "        # Concatenate embeddings\n",
    "        embeddings = np.concatenate((question_embeddings, passage_embeddings), axis=0)\n",
    "\n",
    "        # Ensure correct shape for input (1, 60000) if MAX_SEQ_LENGTH is 10\n",
    "        return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57d7f61-cf3c-464b-afe5-c13e664499f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataset = BoolQDataset(train_data, word2vec_model)\n",
    "val_dataset = BoolQDataset(validation_data, word2vec_model)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80a9dc6-8eae-4416-8964-69d4779b22eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33maintnoair\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/nlp/project-1/wandb/run-20240928_174434-f1f6c4na</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aintnoair/nlp-word-embeddings/runs/f1f6c4na' target=\"_blank\">test-run-1</a></strong> to <a href='https://wandb.ai/aintnoair/nlp-word-embeddings' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aintnoair/nlp-word-embeddings' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word-embeddings</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aintnoair/nlp-word-embeddings/runs/f1f6c4na' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word-embeddings/runs/f1f6c4na</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize WandB\n",
    "wandb.init(project='nlp-word-embeddings', name='test-run-1')\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.epochs = n_epochs\n",
    "wandb.config.batch_size = batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636a6da4-484b-4b26-aafc-575436b2da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212ae18f-d872-4892-9600-acb7deff9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embedding_dim = 300\n",
    "sequence_length = MAX_SEQ_LENGTH * 2  # Concatenate question and passage\n",
    "input_dim = sequence_length * embedding_dim  # Adjusted input dimension\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd6ddbe-e0b1-4fe9-9afb-b34128033fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and move it to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = TwoLayerNN(input_dim, hidden_dim, output_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4134b0d-40ce-4426-9ede-2a4d7cc389a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Lower learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28508f16-8023-4957-a1c5-7c080796e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Flatten inputs\n",
    "            inputs = inputs.view(inputs.size(0), -1).to(device)  # Reshape to (batch_size, 30000)\n",
    "            labels = labels.to(device)  # Move labels to GPU\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # Now inputs should be of shape (batch_size, 30000)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Log the loss to WandB\n",
    "            if i % 10 == 0:  # Log every 10 steps\n",
    "                wandb.log({\"loss\": loss.item()})\n",
    "                print(f\"Step [{i}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Log average loss for the epoch\n",
    "        wandb.log({\"epoch\": epoch + 1, \"average_loss\": running_loss / len(train_loader)})\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b0cbf-15c6-409c-9d0a-dd1165cf02ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28e93ca-fdaf-4314-862f-fe58f6a7bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "    wandb.log({\"validation_accuracy\": accuracy})  # Log validation accuracy to WandB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c3af4d-a9b0-42d4-abd6-49f648fb2aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0], Loss: 0.6934\n",
      "Step [10], Loss: 0.6859\n",
      "Step [20], Loss: 0.6344\n",
      "Step [30], Loss: 0.6474\n",
      "Step [40], Loss: 0.5504\n",
      "Step [50], Loss: 0.6313\n",
      "Step [60], Loss: 0.7589\n",
      "Step [70], Loss: 0.7376\n",
      "Step [80], Loss: 0.6591\n",
      "Step [90], Loss: 0.6491\n",
      "Step [100], Loss: 0.5686\n",
      "Step [110], Loss: 0.8231\n",
      "Step [120], Loss: 0.7227\n",
      "Step [130], Loss: 0.5911\n",
      "Step [140], Loss: 0.7782\n",
      "Step [150], Loss: 0.5761\n",
      "Step [160], Loss: 0.7498\n",
      "Step [170], Loss: 0.5494\n",
      "Step [180], Loss: 0.7995\n",
      "Step [190], Loss: 0.5931\n",
      "Step [200], Loss: 0.4515\n",
      "Step [210], Loss: 0.5080\n",
      "Step [220], Loss: 0.7135\n",
      "Step [230], Loss: 0.8707\n",
      "Step [240], Loss: 0.6733\n",
      "Step [250], Loss: 0.5771\n",
      "Step [260], Loss: 0.6548\n",
      "Step [270], Loss: 0.6609\n",
      "Step [280], Loss: 0.5912\n",
      "Step [290], Loss: 0.6627\n",
      "Step [300], Loss: 0.7234\n",
      "Step [310], Loss: 0.8221\n",
      "Step [320], Loss: 0.6096\n",
      "Step [330], Loss: 0.8084\n",
      "Step [340], Loss: 0.5453\n",
      "Step [350], Loss: 0.5535\n",
      "Step [360], Loss: 0.6402\n",
      "Step [370], Loss: 0.5939\n",
      "Step [380], Loss: 0.4990\n",
      "Step [390], Loss: 0.4730\n",
      "Step [400], Loss: 0.6046\n",
      "Step [410], Loss: 0.6416\n",
      "Step [420], Loss: 0.7197\n",
      "Step [430], Loss: 0.4432\n",
      "Step [440], Loss: 0.4292\n",
      "Step [450], Loss: 0.9083\n",
      "Step [460], Loss: 0.5937\n",
      "Step [470], Loss: 0.5215\n",
      "Step [480], Loss: 0.6462\n",
      "Step [490], Loss: 0.6188\n",
      "Step [500], Loss: 0.3722\n",
      "Step [510], Loss: 0.7881\n",
      "Step [520], Loss: 0.7791\n",
      "Step [530], Loss: 0.6604\n",
      "Step [540], Loss: 0.5082\n",
      "Step [550], Loss: 0.7154\n",
      "Step [560], Loss: 0.5805\n",
      "Step [570], Loss: 0.5239\n",
      "Step [580], Loss: 0.7095\n",
      "Step [590], Loss: 0.6522\n",
      "Step [600], Loss: 0.6542\n",
      "Step [610], Loss: 0.4653\n",
      "Step [620], Loss: 0.7010\n",
      "Step [630], Loss: 0.6025\n",
      "Step [640], Loss: 0.6425\n",
      "Step [650], Loss: 0.5611\n",
      "Step [660], Loss: 0.5434\n",
      "Step [670], Loss: 0.5636\n",
      "Step [680], Loss: 0.6880\n",
      "Step [690], Loss: 0.7066\n",
      "Step [700], Loss: 0.7241\n",
      "Step [710], Loss: 0.6484\n",
      "Step [720], Loss: 0.5192\n",
      "Step [730], Loss: 0.6118\n",
      "Step [740], Loss: 0.5753\n",
      "Step [750], Loss: 0.5811\n",
      "Step [760], Loss: 0.5128\n",
      "Step [770], Loss: 0.6183\n",
      "Step [780], Loss: 0.6928\n",
      "Step [790], Loss: 0.6575\n",
      "Step [800], Loss: 0.7131\n",
      "Step [810], Loss: 0.6024\n",
      "Step [820], Loss: 0.8483\n",
      "Step [830], Loss: 0.6543\n",
      "Step [840], Loss: 0.4110\n",
      "Step [850], Loss: 0.3684\n",
      "Step [860], Loss: 0.9941\n",
      "Step [870], Loss: 0.5305\n",
      "Step [880], Loss: 0.4921\n",
      "Step [890], Loss: 0.5923\n",
      "Step [900], Loss: 0.6354\n",
      "Step [910], Loss: 0.4737\n",
      "Step [920], Loss: 0.7660\n",
      "Step [930], Loss: 0.5116\n",
      "Step [940], Loss: 0.4683\n",
      "Epoch [1/1], Loss: 0.6405\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "424f5cae-e90f-4d07-8196-71f1a1282df0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2000x300 and 60000x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[15], line 10\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model, val_loader)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m val_loader:\n\u001B[1;32m      9\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Move to device\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     12\u001B[0m     total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[10], line 11\u001B[0m, in \u001B[0;36mTwoLayerNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 11\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     13\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (2000x300 and 60000x128)"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93980bf1-6bba-4de2-9f79-47875622ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▅▄▃▆▃▆▃▂▇▃▃▅▆▃▃▃▂▂▃▄▇▄▃▆▅▅▄▃▄▅▃▃▃▂▃▁█▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>0.64045</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.46833</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-run-1</strong> at: <a href='https://wandb.ai/aintnoair/nlp-word-embeddings/runs/f1f6c4na' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word-embeddings/runs/f1f6c4na</a><br/> View project at: <a href='https://wandb.ai/aintnoair/nlp-word-embeddings' target=\"_blank\">https://wandb.ai/aintnoair/nlp-word-embeddings</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240928_174434-f1f6c4na/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the WandB run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eef39f-8434-45e9-a3cf-ddd8abb0436f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
