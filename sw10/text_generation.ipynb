{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47f1ba5",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2\n",
    "\n",
    "In this exercise, we will use a distilled version of GPT-2 to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f007da6",
   "metadata": {},
   "source": [
    "Check out `distilgpt2`'s [model description](https://huggingface.co/distilgpt2) on the Hugging Face model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef04b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model.eval()\n",
    "sentence = 'Yesterday, I dreamed about being an apple on a cruise through Antarctica.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f05a2",
   "metadata": {},
   "source": [
    "First, we encode the `sentence` with the GPT-2 `tokenizer` and then run a forward pass through the GPT-2 `model` to get familiar with its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcde8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b78f30d4",
   "metadata": {},
   "source": [
    "Compute the perplexity for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659a4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09093a5e",
   "metadata": {},
   "source": [
    "Now we use the transformer library's `.generate` function by passing `input_ids` and otherwise using the default parameters to generate a continuation to our prompt: \"Yesterday, I dreamed about\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1298b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "032cdf2c",
   "metadata": {},
   "source": [
    "Not bad. Increase the `max_length` argument to `generate` from 20 (default) to 50 and see how the story continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662227c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6464e1bb",
   "metadata": {},
   "source": [
    "Uh oh. The model gets stuck in a repetitive loop. Let's prevent that by setting `no_repeat_ngram_size` to 3 (trigram blocking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f02b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c90ab3b8",
   "metadata": {},
   "source": [
    "What is the default behavior of `.generate`? Print the model's config to see what generation parameters it uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4a88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df6d775e",
   "metadata": {},
   "source": [
    "Look at the [documentation of GenerationMixin](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin) to see what decoding method is used with these parameters. Scroll down to the parameters of the `generate` function to see what the default values for e.g. `num_beams` is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791e4b1",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e0d32",
   "metadata": {},
   "source": [
    "Let's use beam search with 5 beams instead. Check out the documentation again to see what arguments you have to use for beam search decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84927a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c9ae95",
   "metadata": {},
   "source": [
    "Greedy decoding and beam search are deterministic decoding methods. If you want, you can run the previous generations again and see that the output doesn't change.\n",
    "\n",
    "Let's now change to probabilistic decoding to get more diverse texts. Set `do_sample` to True and `num_beams` to 1. Execute your generation multiple times and see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a31d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1411c057",
   "metadata": {},
   "source": [
    "If you run this generation multiple times, you will sometimes see weird outputs. This happens when a low-probability token gets sampled. To avoid this, we limit the options to the top-*k* tokens of the next-token distribution. Set `top_k` to 5 and 50, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f093e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4fbf39a",
   "metadata": {},
   "source": [
    "Try the same with top-*p* sampling and vary *p*, e.g. use 0.1, 0.8 and 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da2b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
