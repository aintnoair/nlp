{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47f1ba5",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2\n",
    "\n",
    "In this exercise, we will use a distilled version of GPT-2 to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78aee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f007da6",
   "metadata": {},
   "source": [
    "Check out `distilgpt2`'s [model description](https://huggingface.co/distilgpt2) on the Hugging Face model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef04b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model.eval()\n",
    "sentence = 'Yesterday, I dreamed about being an apple on a cruise through Antarctica.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f05a2",
   "metadata": {},
   "source": [
    "First, we encode the `sentence` with the GPT-2 `tokenizer` and then run a forward pass through the GPT-2 `model` to get familiar with its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bcde8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "tensor(4.6747)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        labels=inputs['input_ids'],\n",
    "    )\n",
    "print(outputs.keys())\n",
    "print(outputs.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f30d4",
   "metadata": {},
   "source": [
    "Compute the perplexity for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0659a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity in bits: 25.54\n"
     ]
    }
   ],
   "source": [
    "# perplexity is 2^H(p, q), and the cross-entropy H(p, q) is already our loss\n",
    "perplexity = 2 ** outputs.loss.item()\n",
    "print(f\"Perplexity in bits: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09093a5e",
   "metadata": {},
   "source": [
    "Now we use the transformer library's `.generate` function by passing `input_ids` and otherwise using the default parameters to generate a continuation to our prompt: \"Yesterday, I dreamed about\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1298b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/miniconda3/envs/Python 3 (ipykernel/lib/python3.12/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yesterday, I dreamed about it. I was a little bit scared of the idea of being a kid'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Yesterday, I dreamed about\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(input_ids=inputs['input_ids'])  # uses torch.no_grad() inside\n",
    "# outputs = model.generate(**inputs)  # shorter, passes attention_mask as well\n",
    "tokenizer.batch_decode(outputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032cdf2c",
   "metadata": {},
   "source": [
    "Not bad. Increase the `max_length` argument to `generate` from 20 (default) to 50 and see how the story continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662227c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yesterday, I dreamed about it. I was a little bit scared of the idea of being a kid. I was a little scared of the idea of being a kid. I was a little scared of the idea of being a kid. I was a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(outputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464e1bb",
   "metadata": {},
   "source": [
    "Uh oh. The model gets stuck in a repetitive loop. Let's prevent that by setting `no_repeat_ngram_size` to 3 (trigram blocking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382f02b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yesterday, I dreamed about it. I was a little bit scared of the idea of being a kid. I had no idea what it was like to be a kid, and I was so scared of it.\\n\\n\\nI was so excited about'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_length=50, no_repeat_ngram_size=3, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(outputs[0])  # can use decode on first element if you know batch_size is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ab3b8",
   "metadata": {},
   "source": [
    "What is the default behavior of `.generate`? Print the model's config to see what generation parameters it uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f4a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text-generation': {'do_sample': True, 'max_length': 50}}\n"
     ]
    }
   ],
   "source": [
    "# print(model.config)  # we see that under \"task_specific_params\", the config has a value \"text-generation\"\n",
    "print(model.config.task_specific_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d775e",
   "metadata": {},
   "source": [
    "Look at the [documentation of GenerationMixin](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin) to see what decoding method is used with these parameters. Scroll down to the parameters of the `generate` function to see what the default values for e.g. `num_beams` is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791e4b1",
   "metadata": {},
   "source": [
    "**Answer:** With `do_sample` set to True and `num_beams` being 1 by default, our model would use multinomial (= pure) sampling with a maximum length of 50. But since the `do_sample` parameter is stored in `model.config.task_specific_params.text_generation`, the `generate` method doesn't find it, and uses the default value for `do_sample` instead, which is False. That means our generations above where using greedy decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e0d32",
   "metadata": {},
   "source": [
    "Let's use beam search with 5 beams instead. Check out the documentation again to see what arguments you have to use for beam search decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84927a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yesterday, I dreamed about it for a long time, and now I’m finally able to do it again.\\n\\nI’ve been working on it for quite a while now, and it’s finally ready to go.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for beam search decoding, set do_sample to False and num_beams > 1\n",
    "outputs = model.generate(**inputs, do_sample=False, num_beams=5, max_length=50, no_repeat_ngram_size=3, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(outputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9ae95",
   "metadata": {},
   "source": [
    "Greedy decoding and beam search are deterministic decoding methods. If you want, you can run the previous generations again and see that the output doesn't change.\n",
    "\n",
    "Let's now change to probabilistic decoding to get more diverse texts. Set `do_sample` to True and `num_beams` to 1. Execute your generation multiple times and see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1a31d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yesterday, I dreamed about joining the ranks of my husband and wife. That’s what I wanted to express in the first place. We have become the epitome of what happens when you‡re with kids, family, friends. Your'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(outputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411c057",
   "metadata": {},
   "source": [
    "If you run this generation multiple times, you will sometimes see weird outputs. This happens when a low-probability token gets sampled. To avoid this, we limit the options to the top-*k* tokens of the next-token distribution. Set `top_k` to 5 and 50, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421f093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5: Yesterday, I dreamed about a little more than a month ago and I’ve always dreamed of a few days ago and I was excited about it. I think I’ve always dreamed of something that I’ve always dreamed of.\n",
      "k = 50: Yesterday, I dreamed about one day writing a game called Mockingjay with a big goal: ‏#GamerGate,‏#GamerGate is a #GamerGate news channel.‏\n",
      "\n",
      "I was thinking about the next thing I\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_length=50, top_k=5, pad_token_id=tokenizer.eos_token_id)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(f'k = 5: {text}')\n",
    "outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_length=50, top_k=50, pad_token_id=tokenizer.eos_token_id)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(f'k = 50: {text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbf39a",
   "metadata": {},
   "source": [
    "Try the same with top-*p* sampling and vary *p*, e.g. use 0.1, 0.8 and 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9da2b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.1: Yesterday, I dreamed about doing a blog. I dreamed about setting up a forum and then playing against it. This blog, the \"Beware of the \"Digg\" site, could make it a place to write more. If things didn't\n",
      "p = 0.8: Yesterday, I dreamed about making a computer that was a huge computer by design. And of course, I'm happy to report that we successfully completed our project with an impressive number of hands-on testing.\n",
      "\n",
      "\n",
      "I'm really pleased that I\n",
      "p = 0.95: Yesterday, I dreamed about it with my father. I was just being around my kid and watching a movie, but I thought I'd love to do it for him. I would make his movie and I would like to give them to me to really\n"
     ]
    }
   ],
   "source": [
    "def top_p_generate(p):\n",
    "    outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_length=50, top_p=5, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(f'p = 0.1: {top_p_generate(0.1)}')\n",
    "print(f'p = 0.8: {top_p_generate(0.8)}')\n",
    "print(f'p = 0.95: {top_p_generate(0.95)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
