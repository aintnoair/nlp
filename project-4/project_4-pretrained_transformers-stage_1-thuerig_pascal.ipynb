{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers for BoolQ reading comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9a71120dba852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sources\n",
    "\n",
    "My sources for this project are linked in the respecting sections of the notebook. I used AI tools such as ChatGPT to correct my writing and grammar in stage 1 of this project and plan on using it for debugging during stage 2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1f5527cebe2ac9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### **Importing Python Packages**\n",
    "Making sure the notebook is reproducible and runs without error, I will install the necessary libraries in a pip cell below.\n",
    "\n",
    "### **Data Loading and Split**\n",
    "The BoolQ dataset contains binary question-answer pairs. Each entry consists of a question, a passage, and the corresponding binary answer (yes/no). The dataset is split as required by the course materials:\n",
    "- **Train Split:** The first 8427 entries of the training data.\n",
    "- **Validation Split:** The last 1000 entries of the training data.\n",
    "- **Test Split:** The validation split provided in the BoolQ dataset (3270 entries).\n",
    "\n",
    "### **Seeding for Reproducibility**\n",
    "A seed value of 42 is used to ensure reproducibility of results across different runs.\n",
    "\n",
    "### **Batch size**\n",
    "Setting the batch size in the beginning of the notebook for use throughout the code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e6e55ab05ef1c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tokenizer\n",
    "In this project, the `AutoTokenizer` from the Hugging Face Transformers library is used to handle text preprocessing. Specifically, the `bert-base-cased` associated with the \"bert-base-cased\" model is instantiated. This tokenizer manages:\n",
    "- Whitespace and special character removal.\n",
    "- Retaining case sensitivity to preserve semantic meaning.\n",
    "- Padding and truncation to ensure sequences are within the 512-token limit.\n",
    "\n",
    "##### Tokenization Details\n",
    "- **Questions:** The length of questions is capped at 21 tokens, based on dataset analysis.\n",
    "- **Passages:** Passages are padded or truncated to ensure the overall sequence remains within the 512-token limit.\n",
    "\n",
    "\n",
    "#### Handling OOV words\n",
    "The BERT Tokenizer utilizes a WordPiece tokenization strategy. Words not present in the vocabulary are decomposed into subword units. For example, an unknown word like \"unhappiness\" might be tokenized into \"un\", \"##happiness\". This approach ensures that even unseen words are represented through known subword components.\n",
    "\n",
    "\n",
    "#### Padding and Truncation\n",
    "Sequences are padded to a maximum length of 512 tokens, aligning with BERT's architecture constraints. Padding is applied to the maximum sequence length, not just the length of the longest sequence in a batch. This approach ensures consistent input dimensions across all batches, prioritizing simplicity and uniformity over computational efficiency. While dynamic batch padding could improve efficiency and reduce memory usage, the focus of this project is not on optimizing transformer models for efficiency. Therefore, fixed padding is used to streamline implementation and maintain uniformity.\n",
    "\n",
    "\n",
    "#### Case Sensitivity\n",
    "The \"bert-base-cased\" tokenizer is case-sensitive, distinguishing between words like \"Apple\" and \"apple\". This sensitivity preserves the semantic nuances of the text.\n",
    "\n",
    "\n",
    "### Embedding Layer\n",
    "The model leverages the pre-trained embeddings form the \"bert-base-cased\" model. These embeddings are fine-tuned during training to adapt to the specific nuances of the BoolQ dataset.\n",
    "\n",
    "\n",
    "### Positional Embeddings\n",
    "BERT inherently incorporates positional embeddings, unlike the transformer encoder in the last project, to capture the order of tokens in a sequence.\n",
    "\n",
    "\n",
    "### Input / Output / Label Format\n",
    "Each data point consists of a question, passage, and binary label (True/False). The preprocessing transforms these into the following formats:\n",
    "- **Tokenization**:\n",
    "    - *input*: Concatenated question and passage with `[CLS]` at the start, and `[SEP]` is placed between and at the end of the concatenated sequence.\n",
    "    - *output*: Tokenized sequences with attention masks:\n",
    "        - `input_ids`: Tensor of shape `(batch_size, sequence_length)`, representing token IDs.\n",
    "        - `attention_mask`: Tensor of shape `(batch_size, sequence_length)`, indicating non-padded tokens.\n",
    "- **Transformer Encoder**:\n",
    "    - *input*: Tokenized `input_ids` and `attention_mask` tensors with shape `(batch_size, sequence_length)`.\n",
    "    - *output*: Contextualized embeddings from the final encoder layer of shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "- **Classifier**:\n",
    "    - *input*: The embedding corresponding to the `[CLS]` token with shape `(batch_size, hidden_dim)`.\n",
    "    - *output*: A single logit per data point with shape `(batch_size, 1)`, representing the probability of the label being True.\n",
    "- **Label format**:\n",
    "    - Binary labels are encoded as integers (`0` for False, `1` for True)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3771c0c4b074dd6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Architecture\n",
    "- **Pretrained Transformer Encoder:**\n",
    "  - Hugging Faceâ€™s `bert-base-cased` processes tokenized inputs.\n",
    "  - The output corresponding to the `[CLS]` token is extracted as a fixed-size representation.\n",
    "  - *The `bert-base-cased` model is pretrained using a masked language modeling and next sentence prediction objective and not question answering. This is why I deem it usable for this project where fine-tuned pretrained models are not allowed.*\n",
    "- **Classifier:**\n",
    "  - A two-layer fully connected network processes the `[CLS]` token embedding:\n",
    "    - **First Layer:** Projects the embedding to the hidden dimension with ReLU activation.\n",
    "    - **Dropout Layer:** Introduced after ReLU to reduce overfitting.\n",
    "    - **Second Layer:** Maps the hidden representation to a single binary output using Sigmoid activation.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "The Binary Cross-Entropy Loss (BCE) function is used to calculate the difference between predicted and true labels for binary classification.\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "*Learning rates stated here are for testing model functionality. Hyperparameters for experiments stated in `Experiments`.*\n",
    "- **AdamW Optimizer:**\n",
    "  - A learning rate of `2e-5` is used for the Transformer encoder.\n",
    "  - A higher learning rate of `2e-4` is applied to the classifier layers to allow faster convergence.\n",
    "\n",
    "\n",
    "### Checkpointing and Early Stopping\n",
    "- **Checkpointing:** Save the model with the best validation accuracy. Criteria for this will be the maximum validation accuracy.\n",
    "- **Early Stopping:** Terminates training if validation loss does not improve for 10 consecutive epochs.\n",
    "\n",
    "\n",
    "### Correctness Tests\n",
    "- **Tokenization**:\n",
    "    - Ensure the tokenized output does not exceed 512 tokens.\n",
    "    - Verify alignment between `input_ids` and `attention_mask` dimensions.\n",
    "\n",
    "- **DataLoader**:\n",
    "    - Verify batch size consistency during data loading.\n",
    "    - Check that the output tensors for `input_ids` and `attention_mask` match the expected batch size and sequence length.\n",
    "\n",
    "- **Model Input/Output**:\n",
    "    - Confirm the input to the Transformer encoder has the shape `(batch_size, sequence_length)`.\n",
    "    - Validate that the output of the Transformer encoder has the shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "\n",
    "- **Classifier Dimensions**:\n",
    "    - Check that the input to the classifier corresponds to the `[CLS]` token embedding with shape `(batch_size, hidden_dim)`.\n",
    "    - Ensure the output of the classifier has the shape `(batch_size, 1)`.\n",
    "\n",
    "- **Reproducibility**:\n",
    "    - Validate consistent results across multiple runs with the same random seed.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f27207bfc93ef5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "**Batch Size:** I will start with a batch_size of 16 and increase it to the maximum my hardware can handle then leaving it fixed as it is not a hyperparameter.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "The following hyperparameter ranges were explored during tuning:\n",
    "- **Learning Rate:** `[1e-3, 1e-6]` -> The learning rate for the classifier will be 10x the transformer learning rate, as described in the optimizer section.\n",
    "- **Classifier Hidden Dimension:** `[64, 512]`\n",
    "- **Dropout Rate:** `[0.1, 0.3]`\n",
    "- **Weight Decay:** `[1e-4, 1e-6]`\n",
    "- **Warmup Steps:** `[0.0, 0.1]` in % of total number of steps\n",
    "\n",
    "\n",
    "### Training Strategy\n",
    "For testing the model will be run with manually set hyperparameters. In a second stage the model will utilize optuna to automatically find the optimal hyperparameter combination.\n",
    "- **Epochs:** A maximum of 100 epochs is set, with early stopping enabled. *This will be adjusted based on the runtime per epoch.*\n",
    "- **Warmup Steps:** 0-10% warmup steps improved convergence in prior transformer projects during training. Will test with and without warmup.\n",
    "\n",
    "\n",
    "### Metrics\n",
    "- **Validation Accuracy:** To evaluate model performance across different hyperparameter configurations, I will use validation accuracy as the primary metric.\n",
    "- **Confusion Matrix:** This will give a comprehensive view of true positives, true negatives, false positives, and false negatives, allowing me deeper insight into the modelâ€™s performance.\n",
    "\n",
    "\n",
    "### Logging\n",
    "Weights and Biases (WandB) is used for experiment tracking, logging metrics such as train and validation loss, accuracy, and confusion matrices.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff047d494796bba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The percentage of yes answers in each data split is: Train; 62.64%, Val; 59.50%, Test;62.17%\n",
    "Seeing how difficult it was in past projects to reach a much better accuracy than the baseline majority class I am setting my goal for the pretrained BERT model at 64% accuracy on the test set.\n",
    "\n",
    "\n",
    "### Error Analysis\n",
    "To understand why the model may fail on certain predictions, I will conduct an error analysis investigating weather miss classifications are related to the confidence score the model has in its predictions. Low confidence on correct answers or high confidence on wrong answers may indicate areas where the model is uncertain or overconfident.\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "After the validation step, a confusion matrix is computed to assess true positives, false positives, true negatives, and false negatives. This provides insights into the model's prediction performance.\n",
    "\n",
    "\n",
    "## Planned Correctness Tests\n",
    "- Visually checking for decreasing loss during training.\n",
    "- Verifying predictions with a confusion matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c6e66c160f036d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "My last project went decently well, beating the majority class accuracy of 62.17% on the test set. Before writing this interpretation i toyed around with the `bert-large-cased` model, implementing and running it as quickly as possible just to see what it could do. With 333 Million parameters in the transformer model I had to use a `batch_size` of 16 to not run out of memory. Giving it a single run over the weekend, with \"looks about right\" choice for hyperparameters, it managed to reach a test accuracy of 72.63% after over 23 hours of runtime. Impressed by this result I am setting my expectations for the properly implemented and fine-tuned `bert-base-cased` model to reach a test accuracy of 69%. Nice."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663d884170047d86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:02:41.758318Z",
     "start_time": "2024-11-18T15:02:41.756431Z"
    }
   },
   "id": "f53fb18422d02e2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
