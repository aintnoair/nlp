{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers for BoolQ reading comprehension\n",
    "*All changes and additions compared to Stage 1 are marked in <span style=\"color: orange;\">orange</span>*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9a71120dba852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sources\n",
    "\n",
    "My sources for this project are linked in the respecting sections of the notebook. I used AI tools such as ChatGPT to correct my writing and grammar in stage 1 of this project and plan on using it for debugging during stage 2. Additionally, I used the `hyperparameter_tuing` notebook from the MLOPS course as a rough guide for the structure in this project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1f5527cebe2ac9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">TLDR - Executive summary</span>\n",
    "### **Topic:**\n",
    "Implement and train a pre-trained Transformer model end to end for binary question answering on the BoolQ dataset from Hugging Face.\n",
    "\n",
    "### **Data:**\n",
    "The dataset is for reading comprehension and question answering tasks. It consists of questions, passages, and the corresponding yes/no answers, with separate training and test splits. For this project, the last 1000 entries of the training split were set aside to use as a validation split. Class imbalance was observed across the splits, with approximately 62% of samples labeled \"yes\".\n",
    "\n",
    "### **Methods:**\n",
    "The project involves preprocessing the data using the `AutoTokenizer` that instantiates the correct tokenizer with the pre-trained `bert-base-cased` model. Using the `[CLS]` token from bert-base-cased model output as the input for my two-layer classifier with ReLU and a dropout layer inbetween both layers to finally receive the binary output. \n",
    "\n",
    "### **Model:**\n",
    "The model is based on a `bert-base-cased` model from Hugging Face and is structured as follows:\n",
    "- **Bert Transformer**: Used without modification, with the training mode enabled.\n",
    "- **Classifier**: A two-layer classifier with ReLU non-linearity and a dropout layer.\n",
    "- **Loss Function**: Binary Cross-Entropy Loss `BCELoss`.\n",
    "\n",
    "### **Experiments:**\n",
    "Link to WandB: [Weights and Biases](https://wandb.ai/aintnoair/nlp-p4-pretrained_transformers/reports/Pre-trained-Transformers--VmlldzoxMDMzNDA4OA) <br>\n",
    "Initial manual testing runs were conducted to verify the modelâ€™s functionality, followed by hyperparameter optimization using Optuna (5 trials due to time constraints). The ranges explored were:\n",
    "- learning_rate: 1e-6, 1e-3\n",
    "- hidden_dim: 64, 512, step=64\n",
    "- dropout_rate: 0.0, 0.5\n",
    "- weight_decay: 1e-6, 1e-2\n",
    "- warmup_ratio: 0.0, 0.1 (percentage of total training steps)\n",
    "\n",
    "### **Results:**\n",
    "Key findings:\n",
    "- The best model achieved a validation accuracy of **73.60%** and a test accuracy of **72.63%**, significantly outperforming the majority baseline test accuracy of **62.17%**.\n",
    "-  model effectively addressed overfitting using a dropout rate of **0.2077** and weight decay of **7.37e-05**.\n",
    "- A smaller `bert-base-cased` model, when fine-tuned with optimized hyperparameters, matched the larger `bert-large-cased` model, which achieved a test accuracy of **72.63%**. This highlights the importance of careful hyperparameter tuning over simply scaling up model size.\n",
    "- Despite strong overall performance, the model showed a slight bias toward the majority \"yes\" class, attributed to the class imbalance in the dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d9e5c7491517699"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### **Importing Python Packages**\n",
    "Making sure the notebook is reproducible and runs without error, I will install the necessary libraries in a pip cell below.\n",
    "\n",
    "### **Data Loading and Split**\n",
    "The BoolQ dataset contains binary question-answer pairs. Each entry consists of a question, a passage, and the corresponding binary answer (yes/no). The dataset is split as required by the course materials:\n",
    "- **Train Split:** The first 8427 entries of the training data.\n",
    "- **Validation Split:** The last 1000 entries of the training data.\n",
    "- **Test Split:** The validation split provided in the BoolQ dataset (3270 entries).\n",
    "\n",
    "### **Seeding for Reproducibility**\n",
    "A seed value of 42 is used to ensure reproducibility of results across different runs.\n",
    "\n",
    "### **Batch size**\n",
    "Setting the batch size in the beginning of the notebook for use throughout the code.\n",
    "<span style=\"color: orange;\">Also setting the maximum sequence length as well as the model name in the beginning of the notebook as these will not change during use.</span>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e6e55ab05ef1c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:29:23.794484Z",
     "start_time": "2024-12-01T20:29:22.904749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: install all necessary packages\n",
    "%pip install -q torch transformers datasets lightning seaborn matplotlib optuna wandb scikit-learn jupyter"
   ],
   "id": "72420aeaa79cdde7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:29:26.909788Z",
     "start_time": "2024-12-01T20:29:23.795916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Dict\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import wandb"
   ],
   "id": "45e4397f452f6d94",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:29:26.921250Z",
     "start_time": "2024-12-01T20:29:26.910844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: set random seed to 42\n",
    "pl.seed_everything(42, workers=True)"
   ],
   "id": "a77a551be255670e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T20:29:26.921452Z",
     "start_time": "2024-12-01T20:29:26.918183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: set constants for projects\n",
    "BATCH_SIZE: int = 32\n",
    "MAX_SEQ_LENGTH: int = 512\n",
    "MODEL_NAME: str = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\"\n",
    "      f\"\\nMax seq length: {MAX_SEQ_LENGTH}\"\n",
    "      f\"\\nModel Name: {MODEL_NAME}\")"
   ],
   "id": "25ab177913c402df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Max seq length: 512\n",
      "Model Name: meta-llama/Llama-3.2-1B\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Experimental data exploration: not really necessary for the project but done anyway.\n",
    "\n",
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')\n",
    "\n",
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")\n",
    "\n",
    "max_question_len = max(len(question) for question in train_data['question'])\n",
    "max_passage_len = max(len(question) for question in train_data['passage'])\n",
    "\n",
    "train_yes_count = sum(1 for label in train_data['answer'] if label == 1)\n",
    "train_no_count = sum(1 for label in train_data['answer'] if label == 0)\n",
    "train_total = train_yes_count + train_no_count\n",
    "\n",
    "validation_yes_count = sum(1 for label in validation_data['answer'] if label == 1)\n",
    "validation_no_count = sum(1 for label in validation_data['answer'] if label == 0)\n",
    "validation_total = validation_yes_count + validation_no_count\n",
    "\n",
    "test_yes_count = sum(1 for label in test_data['answer'] if label == 1)\n",
    "test_no_count = sum(1 for label in test_data['answer'] if label == 0)\n",
    "test_total = test_yes_count + test_no_count\n",
    "\n",
    "print(f\"Train set (yes/no) Ratio: {round(train_yes_count / train_no_count, 2)}, Percent Yes: {round(train_yes_count / train_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Validation set (yes/no) Ratio: {round(validation_yes_count / validation_no_count, 2)}, Percent Yes: {round(validation_yes_count / validation_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Test set (yes/no) Ratio: {round(test_yes_count / test_no_count, 2)}, Percent Yes: {round(test_yes_count / test_total * 100, 2)}%\")\n",
    "print(f\"max question length: {max_question_len}\")\n",
    "print(f\"max passage length: {max_passage_len}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-01T20:34:38.806260Z",
     "start_time": "2024-12-01T20:34:34.653128Z"
    }
   },
   "id": "d95626a3afea16a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you use oyster card at epsom station', 'answer': False, 'passage': \"Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\"}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n",
      "Train set (yes/no) Ratio: 1.68, Percent Yes: 62.64%\n",
      "Validation set (yes/no) Ratio: 1.47, Percent Yes: 59.5%\n",
      "Test set (yes/no) Ratio: 1.64, Percent Yes: 62.17%\n",
      "max question length: 100\n",
      "max passage length: 4720\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tokenizer\n",
    "In this project, the `AutoTokenizer` from the Hugging Face Transformers library is used to handle text preprocessing. Specifically, the `bert-base-cased` associated with the \"bert-base-cased\" model is instantiated. This tokenizer manages:\n",
    "- Whitespace and special character removal.\n",
    "- Retaining case sensitivity to preserve semantic meaning.\n",
    "- Padding and truncation to ensure sequences are within the 512-token limit.\n",
    "\n",
    "##### Tokenization Details\n",
    "- **Questions:** The length of questions is capped at 21 tokens, based on dataset analysis.\n",
    "- **Passages:** Passages are padded or truncated to ensure the overall sequence remains within the 512-token limit.\n",
    "\n",
    "\n",
    "#### Handling OOV words\n",
    "The BERT Tokenizer utilizes a WordPiece tokenization strategy. Words not present in the vocabulary are decomposed into subword units. For example, an unknown word like \"unhappiness\" might be tokenized into \"un\", \"##happiness\". This approach ensures that even unseen words are represented through known subword components.\n",
    "\n",
    "\n",
    "#### Padding and Truncation\n",
    "Sequences are padded to a maximum length of 512 tokens, aligning with BERT's architecture constraints. ~~Padding is applied to the maximum sequence length, not just the length of the longest sequence in a batch. This approach ensures consistent input dimensions across all batches, prioritizing simplicity and uniformity over computational efficiency. While dynamic batch padding could improve efficiency and reduce memory usage, the focus of this project is not on optimizing transformer models for efficiency. Therefore, fixed padding is used to streamline implementation and maintain uniformity.~~\n",
    "<span style=\"color: orange;\">Correction: I got dynamic padding working, so I implemented that to save on hardware resources and speed up training.</span>\n",
    "\n",
    "\n",
    "#### Case Sensitivity\n",
    "The \"bert-base-cased\" tokenizer is case-sensitive, distinguishing between words like \"Apple\" and \"apple\". This sensitivity preserves the semantic nuances of the text.\n",
    "\n",
    "\n",
    "### Embedding Layer\n",
    "The model leverages the pre-trained embeddings form the \"bert-base-cased\" model. These embeddings are fine-tuned during training to adapt to the specific nuances of the BoolQ dataset.\n",
    "\n",
    "\n",
    "### Positional Embeddings\n",
    "BERT inherently incorporates positional embeddings, unlike the transformer encoder in the last project, to capture the order of tokens in a sequence.\n",
    "\n",
    "\n",
    "### Input / Output / Label Format\n",
    "Each data point consists of a question, passage, and binary label (True/False). The preprocessing transforms these into the following formats:\n",
    "- **Tokenization**:\n",
    "    - *input*: Concatenated question and passage with `[CLS]` at the start, and `[SEP]` is placed between and at the end of the concatenated sequence.\n",
    "    - *output*: Tokenized sequences with attention masks:\n",
    "        - `input_ids`: Tensor of shape `(batch_size, sequence_length)`, representing token IDs.\n",
    "        - `attention_mask`: Tensor of shape `(batch_size, sequence_length)`, indicating non-padded tokens.\n",
    "- **Transformer Encoder**:\n",
    "    - *input*: Tokenized `input_ids` and `attention_mask` tensors with shape `(batch_size, sequence_length)`.\n",
    "    - *output*: Contextualized embeddings from the final encoder layer of shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "- **Classifier**:\n",
    "    - *input*: The embedding corresponding to the `[CLS]` token with shape `(batch_size, hidden_dim)`.\n",
    "    - *output*: A single logit per data point with shape `(batch_size, 1)`, representing the probability of the label being True.\n",
    "- **Label format**:\n",
    "    - Binary labels are encoded as integers (`0` for False, `1` for True)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3771c0c4b074dd6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, Any], tokenizer: AutoTokenizer, max_length: int = MAX_SEQ_LENGTH):\n",
    "        \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data[\"question\"])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Get question and passage\n",
    "        question = self.data[\"question\"][idx]\n",
    "        passage = self.data[\"passage\"][idx]\n",
    "        label = self.data[\"answer\"][idx]\n",
    "\n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            question,\n",
    "            passage,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Correctness tests for tokenization\n",
    "        assert encoded[\"input_ids\"].shape[-1] <= self.max_length, \"Token length exceeds max_length!\"\n",
    "        assert encoded[\"input_ids\"].shape == encoded[\"attention_mask\"].shape, \"Mismatch in token shapes!\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),  # Remove batch dimension\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),  # Float for binary classification\n",
    "        }"
   ],
   "id": "85e50f2eba87ed5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer_name: str, batch_size: int = BATCH_SIZE, max_length: int = MAX_SEQ_LENGTH):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Loading the dataset based on lecture slides\n",
    "        self.train_data = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "        self.validation_data = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "        self.test_data = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "        # Create datasets\n",
    "        self.train_dataset = BoolQDataset(self.train_data, self.tokenizer, self.max_length)\n",
    "        self.val_dataset = BoolQDataset(self.validation_data, self.tokenizer, self.max_length)\n",
    "        self.test_dataset = BoolQDataset(self.test_data, self.tokenizer, self.max_length)\n",
    "\n",
    "        # Test dataset length\n",
    "        assert len(self.train_dataset) == 8427, \"Train dataset length is incorrect!\"\n",
    "        assert len(self.val_dataset) == 1000, \"Validation dataset length is incorrect!\"\n",
    "        assert len(self.test_dataset) == 3270, \"Test dataset length is incorrect!\"\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "# Initialize DataModule\n",
    "data_module = BoolQDataModule(tokenizer_name=MODEL_NAME, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Prepare and test data loading\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Correctness test for DataLoader\n",
    "for batch in data_module.train_dataloader():\n",
    "    assert batch[\"input_ids\"].shape[0] == BATCH_SIZE, \"Batch size mismatch!\"\n",
    "    print(f\"Batch loaded successfully with shape: {batch['input_ids'].shape}\")\n",
    "    break\n",
    "\n"
   ],
   "id": "e2390e42c97a6a5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Architecture\n",
    "- **Pretrained Transformer Encoder:**\n",
    "  - Hugging Faceâ€™s `bert-base-cased` processes tokenized inputs.\n",
    "  - The output corresponding to the `[CLS]` token is extracted as a fixed-size representation.\n",
    "  - *The `bert-base-cased` model is pretrained using a masked language modeling and next sentence prediction objective and not question answering. This is why I deem it usable for this project where fine-tuned pretrained models are not allowed.*\n",
    "- **Classifier:**\n",
    "  - A two-layer fully connected network processes the `[CLS]` token embedding:\n",
    "    - **First Layer:** Projects the embedding to the hidden dimension with ReLU activation.\n",
    "    - **Dropout Layer:** Introduced after ReLU to reduce overfitting.\n",
    "    - **Second Layer:** Maps the hidden representation to a single binary output using Sigmoid activation.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "The Binary Cross-Entropy Loss (BCE) function is used to calculate the difference between predicted and true labels for binary classification.\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "*Learning rates stated here are for testing model functionality. Hyperparameters for experiments stated in `Experiments`.*\n",
    "- **AdamW Optimizer:**\n",
    "  - A learning rate of `2e-5` is used for the Transformer encoder.\n",
    "  - A higher learning rate of `2e-4` is applied to the classifier layers to allow faster convergence.\n",
    "  - <span style=\"color: orange;\">These learning rates were only en example to demonstrate the classifier lr is 10x of the transformer lr.</span>\n",
    "\n",
    "\n",
    "### Checkpointing and Early Stopping\n",
    "- **Checkpointing:** Save the model with the best validation accuracy. Criteria for this will be the maximum validation accuracy.\n",
    "- **Early Stopping:** Terminates training if validation loss does not improve for <span style=\"color: orange;\">5</span> consecutive epochs.\n",
    "\n",
    "\n",
    "### Correctness Tests\n",
    "- **Tokenization**:\n",
    "    - Ensure the tokenized output does not exceed 512 tokens.\n",
    "    - Verify alignment between `input_ids` and `attention_mask` dimensions.\n",
    "\n",
    "- **DataLoader**:\n",
    "    - Verify batch size consistency during data loading.\n",
    "    - Check that the output tensors for `input_ids` and `attention_mask` match the expected batch size and sequence length.\n",
    "\n",
    "- **Model Input/Output**:\n",
    "    - Confirm the input to the Transformer encoder has the shape `(batch_size, sequence_length)`.\n",
    "    - Validate that the output of the Transformer encoder has the shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "\n",
    "- **Classifier Dimensions**:\n",
    "    - Check that the input to the classifier corresponds to the `[CLS]` token embedding with shape `(batch_size, hidden_dim)`.\n",
    "    - Ensure the output of the classifier has the shape `(batch_size, 1)`.\n",
    "\n",
    "- **Reproducibility**:\n",
    "    - Validate consistent results across multiple runs with the same random seed.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f27207bfc93ef5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            learning_rate: float = 1e-5,\n",
    "            hidden_dim: int = 256,\n",
    "            dropout_rate: float = 0.0,\n",
    "            weight_decay: float = 1e-4,\n",
    "            warmup_ratio: float = 0.1,  # Percentage of total steps\n",
    "    ):\n",
    "        super(BoolQClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.bert.train()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Storage for confusion metrics\n",
    "        self.validation_preds = []\n",
    "        self.validation_labels = []\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits.squeeze(-1)\n",
    "    \n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        self.bert.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "        \n",
    "        self.validation_preds.extend(preds.cpu().numpy())\n",
    "        self.validation_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        cm = confusion_matrix(self.validation_labels, self.validation_preds)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "        plt.title(\"Validation Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        self.logger.experiment.log({\"Validation Confusion Matrix\": wandb.Image(fig)})\n",
    "        plt.show()\n",
    "\n",
    "        self.validation_preds.clear()\n",
    "        self.validation_labels.clear()\n",
    "\n",
    "    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        self.bert.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "\n",
    "        self.test_preds.extend(preds.cpu().numpy())\n",
    "        self.test_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return {}\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        cm = confusion_matrix(self.test_labels, self.test_preds)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "        plt.title(\"Test Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        self.logger.experiment.log({\"Test Confusion Matrix\": wandb.Image(fig)})\n",
    "        plt.show()\n",
    "\n",
    "        self.test_preds.clear()\n",
    "        self.test_labels.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        transformer_params = list(self.bert.parameters())\n",
    "        classifier_params = list(self.classifier.parameters())\n",
    "    \n",
    "        transformer_lr = self.hparams.learning_rate  # Base learning rate\n",
    "        classifier_lr = self.hparams.learning_rate * 10  # Higher learning rate for classifier\n",
    "    \n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': transformer_params, 'lr': transformer_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ])\n",
    "        \n",
    "        # Total steps calculation (approximation for one epoch)\n",
    "        total_steps = (\n",
    "            len(self.trainer.datamodule.train_dataloader())\n",
    "            * self.trainer.max_epochs\n",
    "        )\n",
    "        warmup_steps = int(total_steps * self.hparams.warmup_ratio)\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler]\n"
   ],
   "id": "7fe0f3b0318f539d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "**Batch Size:** I will start with a batch_size of 16 and increase it to the maximum my hardware can handle then leaving it fixed as it is not a hyperparameter.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "The following hyperparameter ranges were explored during tuning:\n",
    "- **Learning Rate:** `[1e-3, 1e-6]` -> The learning rate for the classifier will be 10x the transformer learning rate, as described in the optimizer section.\n",
    "- **Classifier Hidden Dimension:** `[64, 512]`\n",
    "- **Dropout Rate:** `[0.1, 0.3]`\n",
    "- **Weight Decay:** `[1e-4, 1e-6]`\n",
    "- **Warmup Steps:** `[0.0, 0.1]` in % of total number of steps\n",
    "\n",
    "\n",
    "### Training Strategy\n",
    "For testing the model will be run with manually set hyperparameters. In a second stage the model will utilize optuna to automatically find the optimal hyperparameter combination.\n",
    "- **Epochs:** A maximum of ~~100~~ <span style=\"color: orange;\">10</span> epochs is set, with early stopping enabled. <span style=\"color: orange;\">This has been adjusted due to training times per epoch.</span>\n",
    "- **Warmup Steps:** 0-10% warmup steps improved convergence in prior transformer projects during training. Will test with and without warmup.\n",
    "\n",
    "\n",
    "### Metrics\n",
    "- **Validation Accuracy:** To evaluate model performance across different hyperparameter configurations, I will use validation accuracy as the primary metric.\n",
    "- **Confusion Matrix:** This will give a comprehensive view of true positives, true negatives, false positives, and false negatives, allowing me deeper insight into the modelâ€™s performance.\n",
    "\n",
    "\n",
    "### Logging\n",
    "Weights and Biases (WandB) is used for experiment tracking, logging metrics such as train and validation loss, accuracy, and confusion matrices.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff047d494796bba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Helper Function to Format Run Name\n",
    "def format_run_name(hyperparams: dict) -> str:\n",
    "    return \"_\".join([f\"{key[:2]}_{val}\" for key, val in hyperparams.items()])\n",
    "\n",
    "# Manual Training\n",
    "def train_manual():\n",
    "    # Hyperparameters - best optuna hyperparameters filled out\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 3.233413438380264e-05,\n",
    "        \"hidden_dim\": 448,\n",
    "        \"dropout_rate\": 0.20766842736826918,\n",
    "        \"weight_decay\": 7.370027643649387e-05,\n",
    "        \"warmup_ratio\": 0.08159326264046585,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "\n",
    "    run_name = format_run_name(hyperparameters)\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"best_optuna_run\",\n",
    "        log_model=True,\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME, \n",
    "        batch_size=hyperparameters[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"],\n",
    "        weight_decay=hyperparameters[\"weight_decay\"],\n",
    "        warmup_ratio=hyperparameters[\"warmup_ratio\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=hyperparameters[\"patience\"], mode='min')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename=\"best_optuna_run\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=hyperparameters[\"epochs\"],\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    wandb.finish()\n"
   ],
   "id": "53c808634858c7f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_manual()"
   ],
   "id": "14b8806061122c6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3),\n",
    "        \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 512, step=64),\n",
    "        \"dropout_rate\": trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5),\n",
    "        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2),\n",
    "        \"warmup_ratio\": trial.suggest_uniform(\"warmup_ratio\", 0.0, 0.1),\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "    \n",
    "    run_name = format_run_name(hyperparameters)\n",
    "    \n",
    "    logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"optuna\",\n",
    "        log_model=True,\n",
    "        reinit=True,\n",
    "    )\n",
    "    \n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME,\n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "    )\n",
    "    \n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"],\n",
    "        weight_decay=hyperparameters[\"weight_decay\"],\n",
    "        warmup_ratio=hyperparameters[\"warmup_ratio\"]\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=hyperparameters[\"patience\"], mode=\"min\")\n",
    "    checkpoint = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1, filename=run_name)\n",
    "    pruning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        max_epochs=hyperparameters[\"epochs\"],\n",
    "        callbacks=[early_stopping, checkpoint, pruning_callback],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "        val_acc = trainer.callback_metrics.get(\"val_acc\")\n",
    "        if val_acc is not None:\n",
    "            return val_acc.item()\n",
    "        else:\n",
    "            return 0.0\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "    \n",
    "    \n",
    "    \n",
    "def run_optuna():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best value:\", study.best_value)\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=\"optuna_best\",\n",
    "        group=\"optuna\",\n",
    "        reinit=True,\n",
    "    )\n",
    "    wandb_logger.experiment.log({\"best_params\": study.best_params, \"best_value\": study.best_value})\n",
    "    wandb.finish()\n"
   ],
   "id": "f77973d3bf7f1722",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# run_optuna()"
   ],
   "id": "d425fd69582c16ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The percentage of yes answers in each data split is: Train; 62.64%, Val; 59.50%, Test;62.17%\n",
    "Seeing how difficult it was in past projects to reach a much better accuracy than the baseline majority class I am setting my goal for the pretrained BERT model at 64% accuracy on the test set.\n",
    "\n",
    "\n",
    "### Error Analysis\n",
    "To understand why the model may fail on certain predictions, I will conduct an error analysis investigating weather miss classifications are related to the confidence score the model has in its predictions. Low confidence on correct answers or high confidence on wrong answers may indicate areas where the model is uncertain or overconfident.\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "After the validation step, a confusion matrix is computed to assess true positives, false positives, true negatives, and false negatives. This provides insights into the model's prediction performance.\n",
    "\n",
    "\n",
    "## Planned Correctness Tests\n",
    "- Visually checking for decreasing loss during training.\n",
    "- Verifying predictions with a confusion matrix.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c6e66c160f036d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">Evaluation</span>\n",
    "Opting to forego manual preprocessing and instead utilizing the built-in functionality of the AutoTokenizer module to manage padding, truncation, extra whitespace removal, special character handling, and case sensitivity proved successful. This approach simplified the implementation and minimized the risk of errors.\n",
    "\n",
    "### Data Imbalance Analysis\n",
    "An exploration of the dataset revealed a significant class imbalance across the training, validation, and test sets. The proportion of \"yes\" labels is consistently higher across all splits:\n",
    "\n",
    "Training set: 62.64% \"yes\" (1.68 ratio)\n",
    "Validation set: 59.5% \"yes\" (1.47 ratio)\n",
    "Test set: 62.17% \"yes\" (1.64 ratio)\n",
    "This imbalance likely contributed to the model's tendency to predominantly predict the majority \"yes\" class, especially in the early stages of training. Without adjustments to address this imbalance, the model achieved a validation accuracy of approximately XXXXXXX% but failed to learn nuanced patterns in the minority \"no\" class.\n",
    "\n",
    "To address this, I incorporated dropout, weight decay, and adjusted the learning rate during training. Additionally, I experimented with hyperparameter tuning using Optuna to optimize the model's performance further.\n",
    "\n",
    "### Error and Confusion Matrix Analysis\n",
    "The best-performing model from the Optuna trials achieved a validation accuracy of **73.60%** and a test accuracy of **72.63%**, significantly outperforming the baseline test accuracy of **62.17%**. The final model's run parameters were as follows:\n",
    "\n",
    "- Run name: `le_3.233413438380264e-05_hi_448_dr_0.20766842736826918_we_7.370027643649387e-05_wa_0.08159326264046585_ba_32_pa_5_ep_10`\n",
    "    - Learning rate: 3.23e-05\n",
    "    - Hidden dimension: 448\n",
    "    - Dropout rate: 0.2077\n",
    "    - Weight decay: 7.37e-05\n",
    "    - Warmup steps: 8.16%\n",
    "    - Batch size: 32\n",
    "    - Patience: 5 epochs\n",
    "    - Max epochs: 10\n",
    "\n",
    "Analysis of the confusion matrix revealed that while the model improved in predicting the minority \"no\" class compared to earlier runs, a slight bias toward the majority \"yes\" class persisted. This suggests that further techniques, such as weighted loss functions or oversampling the minority class, could enhance performance further. It was also interesting to see how the model improved over time when plotting the validation confusion matrix every epoch.\n",
    "\n",
    "### Training Accuracy and Overfitting\n",
    "The model reached a training accuracy of approximately 98.9% within 5â€“6 epochs, highlighting its capacity to fit the training data effectively. However, the gap between training and validation accuracy persisted, indicating some degree of overfitting despite applying dropout and weight decay.\n",
    "\n",
    "### Comparison of BERT Models\n",
    "The `bert-base-cased` model tuned with Optuna matched the \"Hail Mary\" `bert-large-cased` model, which achieved a validation accuracy of 72.60% and a test accuracy of 72.63%. Surprisingly, the smaller model not only matched but slightly surpassed the larger model's test performance, all while being significantly faster and more resource-efficient to train. This result emphasizes the importance of fine-tuning and hyperparameter optimization over simply scaling up model size.\n",
    "\n",
    "### Evaluation on the test set\n",
    "The final model achieved a test accuracy of **72.63%**, marking a substantial improvement over the baseline of **62.17%**. While the model's performance is promising, the persistent imbalance in predictions suggests room for further optimization.\n",
    "\n",
    "### Recommendations for Future Projects\n",
    "To further address the class imbalance, future iterations could explore:\n",
    "\n",
    "- **Weighted Loss Functions**: Assigning higher weights to the minority \"no\" class to encourage the model to learn its patterns more effectively.\n",
    "- **Focal Loss**: Adjusting the loss function to focus more on harder-to-classify examples.\n",
    "Additionally, incorporating adversarial training or experimenting with ensemble methods could help the model better generalize to unseen data. Despite its limitations, the fine-tuned bert-base-cased model demonstrates promising performance and efficiency for the BoolQ dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6718fc66386e3c6d"
  },
  {
   "cell_type": "code",
   "source": [
    "run_name = \"best_optuna_run\"\n",
    "file_path = f\"nlp-p4-pretrained_transformers/0ng67wld/checkpoints/{run_name}.ckpt\"\n",
    "\n",
    "model = BoolQClassifier.load_from_checkpoint(file_path)\n",
    "logger = WandbLogger(\n",
    "    project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"optuna_test\",\n",
    "        reinit=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(logger=logger)\n",
    "\n",
    "trainer.test(model, datamodule=data_module)\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844b07f90d871c4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "My last project went decently well, beating the majority class accuracy of 62.17% on the test set. Before writing this interpretation I toyed around with the `bert-large-cased` model, implementing and running it as quickly as possible just to see what it could do. With 333 Million parameters in the transformer model I had to use a `batch_size` of 16 to not run out of memory. Giving it a single run over the weekend, with \"looks about right\" choice for hyperparameters, it managed to reach a test accuracy of 72.63% after over 23 hours of runtime. Impressed by this result I am setting my expectations for the properly implemented and fine-tuned `bert-base-cased` model to reach a test accuracy of 69%. Nice."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663d884170047d86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">Interpretation</span>\n",
    "Before beginning this project, I set my expectations for the `bert-base-cased` model to reach a test accuracy of around **69%**, based on my preliminary \"Hail Mary\" run with the `bert-large-cased` model, which achieved a test accuracy of **72.63%**. To my surprise, the fine-tuned bert-base-cased model not only met but exceeded these expectations, achieving a validation accuracy of **73.60%** and a test accuracy of **72.63%**.\n",
    "\n",
    "This result demonstrates that a smaller, well-tuned model can match or outperform a larger model on tasks like BoolQ, where dataset size and complexity may not fully leverage the additional capacity of a large model. The ability to fine-tune the base model with limited computational resources and achieve comparable results to the bert-large-cased model, which has 333 million parameters, highlights the efficiency of proper hyperparameter tuning.\n",
    "\n",
    "One key takeaway from this project is the importance of addressing dataset imbalances. While the model performed admirably overall, the persistent bias toward the majority \"yes\" class suggests that the imbalanced label distribution limited its ability to fully capture patterns in the minority \"no\" class. Techniques such as class weighting, focal loss, or oversampling could be valuable avenues for future improvement.\n",
    "\n",
    "Ultimately, this project showcased the power of fine-tuning smaller models and the significant impact of systematic hyperparameter optimization. While there is still room for improvement in mitigating overfitting and handling class imbalance, the results are a strong step forward in tackling the BoolQ dataset with pretrained Transformers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c9d0aa074af865a"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f53fb18422d02e2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
