{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers for BoolQ reading comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9a71120dba852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sources\n",
    "\n",
    "My sources for this project are linked in the respecting sections of the notebook. I used AI tools such as ChatGPT to correct my writing and grammar in stage 1 of this project and plan on using it for debugging during stage 2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1f5527cebe2ac9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### **Importing Python Packages**\n",
    "Making sure the notebook is reproducible and runs without error, I will install the necessary libraries in a pip cell below.\n",
    "\n",
    "### **Data Loading and Split**\n",
    "The BoolQ dataset contains binary question-answer pairs. Each entry consists of a question, a passage, and the corresponding binary answer (yes/no). The dataset is split as required by the course materials:\n",
    "- **Train Split:** The first 8427 entries of the training data.\n",
    "- **Validation Split:** The last 1000 entries of the training data.\n",
    "- **Test Split:** The validation split provided in the BoolQ dataset (3270 entries).\n",
    "\n",
    "### **Seeding for Reproducibility**\n",
    "A seed value of 42 is used to ensure reproducibility of results across different runs.\n",
    "\n",
    "### **Batch size**\n",
    "Setting the batch size in the beginning of the notebook for use throughout the code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e6e55ab05ef1c9"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# TODO: pip install necessary packages\n",
    "# %pip install "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:19.836857Z",
     "start_time": "2024-11-24T21:27:19.831175Z"
    }
   },
   "id": "b05593eb9c4e5948"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pathlib import Path\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:20.544214Z",
     "start_time": "2024-11-24T21:27:20.526578Z"
    }
   },
   "id": "e7b4769e0af19efa"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:21.021325Z",
     "start_time": "2024-11-24T21:27:21.017588Z"
    }
   },
   "id": "df24e6e9c2173db9"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:21.463259Z",
     "start_time": "2024-11-24T21:27:21.451179Z"
    }
   },
   "id": "5a1f16009b8c0997"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size: 32\n",
      "Max_length: 512\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE: int = 32\n",
    "MAX_LENGTH: int = 512\n",
    "\n",
    "print(f\"Batch_size: {BATCH_SIZE}\"\n",
    "      f\"\\nMax_length: {MAX_LENGTH}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:21.869092Z",
     "start_time": "2024-11-24T21:27:21.862733Z"
    }
   },
   "id": "9db5b03016b5fe8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tokenizer\n",
    "In this project, the `AutoTokenizer` from the Hugging Face Transformers library is used to handle text preprocessing. Specifically, the `bert-base-cased` associated with the \"bert-base-cased\" model is instantiated. This tokenizer manages:\n",
    "- Whitespace and special character removal.\n",
    "- Retaining case sensitivity to preserve semantic meaning.\n",
    "- Padding and truncation to ensure sequences are within the 512-token limit.\n",
    "\n",
    "##### Tokenization Details\n",
    "- **Questions:** The length of questions is capped at 21 tokens, based on dataset analysis.\n",
    "- **Passages:** Passages are padded or truncated to ensure the overall sequence remains within the 512-token limit.\n",
    "\n",
    "\n",
    "#### Handling OOV words\n",
    "The BERT Tokenizer utilizes a WordPiece tokenization strategy. Words not present in the vocabulary are decomposed into subword units. For example, an unknown word like \"unhappiness\" might be tokenized into \"un\", \"##happiness\". This approach ensures that even unseen words are represented through known subword components.\n",
    "\n",
    "\n",
    "#### Padding and Truncation\n",
    "Sequences are padded to a maximum length of 512 tokens, aligning with BERT's architecture constraints. Padding is applied to the maximum sequence length, not just the length of the longest sequence in a batch. This approach ensures consistent input dimensions across all batches, prioritizing simplicity and uniformity over computational efficiency. While dynamic batch padding could improve efficiency and reduce memory usage, the focus of this project is not on optimizing transformer models for efficiency. Therefore, fixed padding is used to streamline implementation and maintain uniformity.\n",
    "\n",
    "\n",
    "#### Case Sensitivity\n",
    "The \"bert-base-cased\" tokenizer is case-sensitive, distinguishing between words like \"Apple\" and \"apple\". This sensitivity preserves the semantic nuances of the text.\n",
    "\n",
    "\n",
    "### Embedding Layer\n",
    "The model leverages the pre-trained embeddings form the \"bert-base-cased\" model. These embeddings are fine-tuned during training to adapt to the specific nuances of the BoolQ dataset.\n",
    "\n",
    "\n",
    "### Positional Embeddings\n",
    "BERT inherently incorporates positional embeddings, unlike the transformer encoder in the last project, to capture the order of tokens in a sequence.\n",
    "\n",
    "\n",
    "### Input / Output / Label Format\n",
    "Each data point consists of a question, passage, and binary label (True/False). The preprocessing transforms these into the following formats:\n",
    "- **Tokenization**:\n",
    "    - *input*: Concatenated question and passage with `[CLS]` at the start, and `[SEP]` is placed between and at the end of the concatenated sequence.\n",
    "    - *output*: Tokenized sequences with attention masks:\n",
    "        - `input_ids`: Tensor of shape `(batch_size, sequence_length)`, representing token IDs.\n",
    "        - `attention_mask`: Tensor of shape `(batch_size, sequence_length)`, indicating non-padded tokens.\n",
    "- **Transformer Encoder**:\n",
    "    - *input*: Tokenized `input_ids` and `attention_mask` tensors with shape `(batch_size, sequence_length)`.\n",
    "    - *output*: Contextualized embeddings from the final encoder layer of shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "- **Classifier**:\n",
    "    - *input*: The embedding corresponding to the `[CLS]` token with shape `(batch_size, hidden_dim)`.\n",
    "    - *output*: A single logit per data point with shape `(batch_size, 1)`, representing the probability of the label being True.\n",
    "- **Label format**:\n",
    "    - Binary labels are encoded as integers (`0` for False, `1` for True)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3771c0c4b074dd6c"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, Any], tokenizer: AutoTokenizer, max_length: int = MAX_LENGTH):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data[\"question\"])\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        question = self.data[\"question\"][idx]\n",
    "        passage = self.data[\"passage\"][idx]\n",
    "        label = self.data[\"answer\"][idx]\n",
    "        \n",
    "        # Tokenize sequences\n",
    "        encoded = self.tokenizer(\n",
    "            question,\n",
    "            passage,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Tokenizer test\n",
    "        assert encoded[\"input_ids\"].shape[-1] <= self.max_length, \"Token length exceeds max_length!\"\n",
    "        assert encoded[\"input_ids\"].shape == encoded[\"attention_mask\"].shape, \"Mismatched token shapes!\"\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),  # Remove batch dimension\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),  # Float for binary classification\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:23.002451Z",
     "start_time": "2024-11-24T21:27:22.993491Z"
    }
   },
   "id": "4a3a52a99df0149d"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class BoolQDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer_name: str, batch_size: int = BATCH_SIZE, max_length: int = MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def prepare_data(self) -> None:\n",
    "        # Loading the dataset based on lecture slides\n",
    "        self.train_dataset = load_dataset('google/boolq', split='train[:-1000]')\n",
    "        self.val_dataset = load_dataset('google/boolq', split='train[-1000:]')\n",
    "        self.test_dataset = load_dataset('google/boolq', split='validation')\n",
    "    \n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        # Initialize Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        \n",
    "        # Create Datasets\n",
    "        self.train_dataset = BoolQDataset(self.train_dataset, self.tokenizer, self.max_length)\n",
    "        self.val_dataset = BoolQDataset(self.val_dataset, self.tokenizer, self.max_length)\n",
    "        self.test_dataset = BoolQDataset(self.test_dataset, self.tokenizer, self.max_length)\n",
    "        \n",
    "        # Test dataset length\n",
    "        assert len(self.train_dataset) == 8427, \"Train dataset length is incorrect!\"\n",
    "        assert len(self.val_dataset) == 1000, \"Validation dataset length is incorrect!\"\n",
    "        assert len(self.test_dataset) == 3270, \"Test dataset length is incorrect!\"\n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:23.463668Z",
     "start_time": "2024-11-24T21:27:23.448500Z"
    }
   },
   "id": "91ef584647e94275"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loaded successfully with shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "data_module = BoolQDataModule(tokenizer_name=\"bert-base-cased\", batch_size=BATCH_SIZE)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "for batch in data_module.train_dataloader():\n",
    "    assert batch[\"input_ids\"].shape[0] == BATCH_SIZE, \"Batch size mismatch!\"\n",
    "    print(f\"Batch loaded successfully with shape: {batch['input_ids'].shape}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:31.421567Z",
     "start_time": "2024-11-24T21:27:24.089471Z"
    }
   },
   "id": "a1723439b98594a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Architecture\n",
    "- **Pretrained Transformer Encoder:**\n",
    "  - Hugging Face’s `bert-base-cased` processes tokenized inputs.\n",
    "  - The output corresponding to the `[CLS]` token is extracted as a fixed-size representation.\n",
    "  - *The `bert-base-cased` model is pretrained using a masked language modeling and next sentence prediction objective and not question answering. This is why I deem it usable for this project where fine-tuned pretrained models are not allowed.*\n",
    "- **Classifier:**\n",
    "  - A two-layer fully connected network processes the `[CLS]` token embedding:\n",
    "    - **First Layer:** Projects the embedding to the hidden dimension with ReLU activation.\n",
    "    - **Dropout Layer:** Introduced after ReLU to reduce overfitting.\n",
    "    - **Second Layer:** Maps the hidden representation to a single binary output using Sigmoid activation.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "The Binary Cross-Entropy Loss (BCE) function is used to calculate the difference between predicted and true labels for binary classification.\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "*Learning rates stated here are for testing model functionality. Hyperparameters for experiments stated in `Experiments`.*\n",
    "- **AdamW Optimizer:**\n",
    "  - A learning rate of `2e-5` is used for the Transformer encoder.\n",
    "  - A higher learning rate of `2e-4` is applied to the classifier layers to allow faster convergence.\n",
    "\n",
    "\n",
    "### Checkpointing and Early Stopping\n",
    "- **Checkpointing:** Save the model with the best validation accuracy. Criteria for this will be the maximum validation accuracy.\n",
    "- **Early Stopping:** Terminates training if validation loss does not improve for 10 consecutive epochs.\n",
    "\n",
    "\n",
    "### Correctness Tests\n",
    "- **Tokenization**:\n",
    "    - Ensure the tokenized output does not exceed 512 tokens.\n",
    "    - Verify alignment between `input_ids` and `attention_mask` dimensions.\n",
    "\n",
    "- **DataLoader**:\n",
    "    - Verify batch size consistency during data loading.\n",
    "    - Check that the output tensors for `input_ids` and `attention_mask` match the expected batch size and sequence length.\n",
    "\n",
    "- **Model Input/Output**:\n",
    "    - Confirm the input to the Transformer encoder has the shape `(batch_size, sequence_length)`.\n",
    "    - Validate that the output of the Transformer encoder has the shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "\n",
    "- **Classifier Dimensions**:\n",
    "    - Check that the input to the classifier corresponds to the `[CLS]` token embedding with shape `(batch_size, hidden_dim)`.\n",
    "    - Ensure the output of the classifier has the shape `(batch_size, 1)`.\n",
    "\n",
    "- **Reproducibility**:\n",
    "    - Validate consistent results across multiple runs with the same random seed.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f27207bfc93ef5d"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "class TransformerClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            learning_rate: float,\n",
    "            hidden_dim: int,\n",
    "            dropout_rate: float,\n",
    "            warmup_steps: float = 0.1,  # in percent of total steps\n",
    "            weight_decay: float = 0.0\n",
    "    ):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.bert.train()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_accuracy = BinaryAccuracy()\n",
    "        self.val_accuracy = BinaryAccuracy()\n",
    "        self.test_accuracy = BinaryAccuracy()\n",
    "        self.val_conf_matrix = BinaryConfusionMatrix()\n",
    "        self.test_conf_matrix = BinaryConfusionMatrix()\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # CLS Token\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits.squeeze(-1)\n",
    "    \n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        logits = self(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        loss = self.loss_fn(logits, batch[\"label\"])\n",
    "        preds = (logits > 0.5).float()\n",
    "        self.train_accuracy.update(preds, batch[\"label\"])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", self.train_accuracy.compute(), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def val_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        logits = self(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        loss = self.loss_fn(logits, batch[\"label\"])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = self.val_accuracy.update(preds, batch[\"label\"])\n",
    "        self.val_conf_matrix.update(preds, batch[\"label\"])\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        val_acc = self.val_accuracy.compute()\n",
    "        cm = self.val_conf_matrix.compute().cpu().numpy()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "        for (i, j), val in np.ndenumerate(cm):\n",
    "            ax.text(j, i, f\"{val}\", ha=\"center\", va=\"center\")\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Validation Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        self.log('val_acc', val_acc, prog_bar=True)\n",
    "        wandb.log({\"val_confusion_matrix\": wandb.Image(fig)})\n",
    "\n",
    "        # Reset metrics\n",
    "        self.val_accuracy.reset()\n",
    "        self.val_conf_matrix.reset()\n",
    "    \n",
    "    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> None:\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = self.test_accuracy.update(preds, batch['label'])\n",
    "        self.test_conf_matrix.update(preds, batch['label'])\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "        \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        test_acc = self.test_accuracy.compute()\n",
    "        cm = self.test_conf_matrix.compute().cpu().numpy()\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "        for (i, j), val in np.ndenumerate(cm):\n",
    "            ax.text(j, i, f\"{val}\", ha=\"center\", va=\"center\")\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.title(\"Test Confusion Matrix\")\n",
    "\n",
    "        # Show the plot in the notebook\n",
    "        plt.show()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_acc', test_acc, prog_bar=True)\n",
    "        wandb.log({\"test_confusion_matrix\": wandb.Image(fig)})\n",
    "\n",
    "        # Reset metrics\n",
    "        self.test_accuracy.reset()\n",
    "        self.test_conf_matrix.reset()\n",
    "        \n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        transformer_params = list(self.bert.parameters())\n",
    "        classifier_params = list(self.classifier.parameters())\n",
    "        \n",
    "        # classifier lr is higher for faster convergence\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": transformer_params, \"lr\": self.hparams.learning_rate, \"weight_decay\": self.hparams.weight_decay},\n",
    "                {\"params\": classifier_params, \"lr\": self.hparams.learning_rate * 10, \"weight_decay\": self.hparams.weight_decay},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        total_steps = math.ceil(self.trainer.estimated_stepping_batches)\n",
    "        warmup_steps = int(total_steps * self.hparams.warmup_steps)\n",
    "        \n",
    "        scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:31.435641Z",
     "start_time": "2024-11-24T21:27:31.421431Z"
    }
   },
   "id": "bbff4b271d99d42b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "**Batch Size:** I will start with a batch_size of 16 and increase it to the maximum my hardware can handle then leaving it fixed as it is not a hyperparameter.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "The following hyperparameter ranges were explored during tuning:\n",
    "- **Learning Rate:** `[1e-3, 1e-6]` -> The learning rate for the classifier will be 10x the transformer learning rate, as described in the optimizer section.\n",
    "- **Classifier Hidden Dimension:** `[64, 512]`\n",
    "- **Dropout Rate:** `[0.1, 0.3]`\n",
    "- **Weight Decay:** `[1e-4, 1e-6]`\n",
    "- **Warmup Steps:** `[0.0, 0.1]` in % of total number of steps\n",
    "\n",
    "\n",
    "### Training Strategy\n",
    "For testing the model will be run with manually set hyperparameters. In a second stage the model will utilize optuna to automatically find the optimal hyperparameter combination.\n",
    "- **Epochs:** A maximum of 100 epochs is set, with early stopping enabled. *This will be adjusted based on the runtime per epoch.*\n",
    "- **Warmup Steps:** 0-10% warmup steps improved convergence in prior transformer projects during training. Will test with and without warmup.\n",
    "\n",
    "\n",
    "### Metrics\n",
    "- **Validation Accuracy:** To evaluate model performance across different hyperparameter configurations, I will use validation accuracy as the primary metric.\n",
    "- **Confusion Matrix:** This will give a comprehensive view of true positives, true negatives, false positives, and false negatives, allowing me deeper insight into the model’s performance.\n",
    "\n",
    "\n",
    "### Logging\n",
    "Weights and Biases (WandB) is used for experiment tracking, logging metrics such as train and validation loss, accuracy, and confusion matrices.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff047d494796bba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "this I want to change:\n",
    "- warmup_steps should be defined in percent of total steps. i want to pass 0.1 for a 10% warmup step period.\n",
    "- make model use warmup_steps and weight_decay\n",
    "- "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c9fb54192f00053"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "class WandbLoggingCallback(pl.Callback):\n",
    "    def __init__(self, log_interval: int = 10):\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "            if (batch_idx + 1) % self.log_interval == 0:\n",
    "                metrics = trainer.callback_metrics\n",
    "                wandb.log({\n",
    "                    \"train_loss\": metrics.get(\"train_loss\", None),\n",
    "                    \"train_acc\": metrics.get(\"train_acc\", None),\n",
    "                })\n",
    "        \n",
    "        def on_validation_epoch_end(self, trainer, pl_module):\n",
    "            metrics = trainer.callback_metrics\n",
    "            wandb.log({\n",
    "                \"val_loss\": metrics.get(\"val_loss_epoch\", None),\n",
    "                \"val_acc\": metrics.get(\"val_acc_epoch\", None),\n",
    "            })\n",
    "        \n",
    "def format_run_name(hyperparams: dict) -> str:\n",
    "    return \"-\".join([f\"{key[:2]}_{val}\" for key, val in hyperparams.items()])\n",
    "\n",
    "def manual_training_run():\n",
    "    hyperparameters = {\n",
    "        \"model_name\": \"bert-base-cased\",\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"warmup_steps\": 0.1,  # 10% warmup\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "    }\n",
    "    \n",
    "    # run_name = format_run_name(hyperparameters)\n",
    "    group_name = \"manual_testing\"\n",
    "    run_name = \"testing_run\"\n",
    "    \n",
    "    \n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=group_name,\n",
    "    )\n",
    "    \n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=hyperparameters[\"model_name\"], \n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "    )\n",
    "    \n",
    "    model = TransformerClassifier(\n",
    "        model_name=hyperparameters[\"model_name\"],\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"],\n",
    "        warmup_steps=hyperparameters[\"warmup_steps\"],\n",
    "        weight_decay=hyperparameters[\"weight_decay\"],\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "    checkpoint = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1, filename=run_name)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        max_epochs=hyperparameters[\"epochs\"],\n",
    "        callbacks=[early_stopping, checkpoint, WandbLoggingCallback()],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T21:27:31.446225Z",
     "start_time": "2024-11-24T21:27:31.438463Z"
    }
   },
   "id": "9d3cdd7164963ee0"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | bert             | BertModel             | 108 M  | eval \n",
      "1 | classifier       | Sequential            | 197 K  | train\n",
      "2 | loss_fn          | BCELoss               | 0      | train\n",
      "3 | train_accuracy   | BinaryAccuracy        | 0      | train\n",
      "4 | val_accuracy     | BinaryAccuracy        | 0      | train\n",
      "5 | test_accuracy    | BinaryAccuracy        | 0      | train\n",
      "6 | val_conf_matrix  | BinaryConfusionMatrix | 0      | train\n",
      "7 | test_conf_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "434.030   Total estimated model params size (MB)\n",
      "12        Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "036b5d7d6a354286aec1f54cafe6b2fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    570\u001B[0m     ckpt_path,\n\u001B[1;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m )\n\u001B[0;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1025\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 205\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 140\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001B[39;00m\n\u001B[0;32m--> 250\u001B[0m     batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautomatic_optimization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001B[0m, in \u001B[0;36m_AutomaticOptimization.run\u001B[0;34m(self, optimizer, batch_idx, kwargs)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001B[0m, in \u001B[0;36m_AutomaticOptimization._optimizer_step\u001B[0;34m(self, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 268\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:167\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 167\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1306\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001B[0m\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1283\u001B[0m \u001B[38;5;124;03mthe optimizer.\u001B[39;00m\n\u001B[1;32m   1284\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1304\u001B[0m \n\u001B[1;32m   1305\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1306\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:153\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:238\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule)\n\u001B[0;32m--> 238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001B[0m, in \u001B[0;36mPrecision.optimizer_step\u001B[0;34m(self, optimizer, model, closure, **kwargs)\u001B[0m\n\u001B[1;32m    121\u001B[0m closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, closure)\n\u001B[0;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/optim/adamw.py:197\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 197\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001B[0m, in \u001B[0;36mPrecision._wrap_closure\u001B[0;34m(self, model, optimizer, closure)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mhook is called.\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    106\u001B[0m \n\u001B[1;32m    107\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39menable_grad()\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 129\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001B[0m, in \u001B[0;36m_AutomaticOptimization._training_step\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m    315\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\n\u001B[0;32m--> 317\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()  \u001B[38;5;66;03m# unused hook - call anyway for backward compatibility\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:390\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[52], line 32\u001B[0m, in \u001B[0;36mTransformerClassifier.training_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     31\u001B[0m preds \u001B[38;5;241m=\u001B[39m (logits \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_accuracy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torchmetrics/metric.py:483\u001B[0m, in \u001B[0;36mMetric._wrap_update.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 483\u001B[0m     \u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    484\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torchmetrics/classification/stat_scores.py:187\u001B[0m, in \u001B[0;36mBinaryStatScores.update\u001B[0;34m(self, preds, target)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_args:\n\u001B[0;32m--> 187\u001B[0m     \u001B[43m_binary_stat_scores_tensor_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmultidim_average\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m preds, target \u001B[38;5;241m=\u001B[39m _binary_stat_scores_format(preds, target, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthreshold, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torchmetrics/functional/classification/stat_scores.py:70\u001B[0m, in \u001B[0;36m_binary_stat_scores_tensor_validation\u001B[0;34m(preds, target, multidim_average, ignore_index)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Check that target only contains [0,1] values or value in ignore_index\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m unique_values \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ignore_index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/_jit_internal.py:624\u001B[0m, in \u001B[0;36mboolean_dispatch.<locals>.fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 624\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mif_false\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/_jit_internal.py:624\u001B[0m, in \u001B[0;36mboolean_dispatch.<locals>.fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 624\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mif_false\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/functional.py:1075\u001B[0m, in \u001B[0;36m_return_output\u001B[0;34m(input, sorted, return_inverse, return_counts, dim)\u001B[0m\n\u001B[1;32m   1073\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unique_impl(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28msorted\u001B[39m, return_inverse, return_counts, dim)\n\u001B[0;32m-> 1075\u001B[0m output, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_unique_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43msorted\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1076\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/functional.py:968\u001B[0m, in \u001B[0;36m_unique_impl\u001B[0;34m(input, sorted, return_inverse, return_counts, dim)\u001B[0m\n\u001B[1;32m    967\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 968\u001B[0m     output, inverse_indices, counts \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_unique2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43msorted\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43msorted\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output, inverse_indices, counts\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmanual_training_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[53], line 75\u001B[0m, in \u001B[0;36mmanual_training_run\u001B[0;34m()\u001B[0m\n\u001B[1;32m     65\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m ModelCheckpoint(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_acc\u001B[39m\u001B[38;5;124m\"\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax\u001B[39m\u001B[38;5;124m\"\u001B[39m, save_top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, filename\u001B[38;5;241m=\u001B[39mrun_name)\n\u001B[1;32m     67\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     68\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39mhyperparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     69\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[early_stopping, checkpoint, WandbLoggingCallback()],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     72\u001B[0m     logger\u001B[38;5;241m=\u001B[39mwandb_logger,\n\u001B[1;32m     73\u001B[0m )\n\u001B[0;32m---> 75\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[1;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[0;32m---> 64\u001B[0m     \u001B[43mexit\u001B[49m(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[1;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "manual_training_run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T19:08:35.054648Z",
     "start_time": "2024-11-24T18:30:55.733189Z"
    }
   },
   "id": "2e00539737f6c4f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: implement optuna here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-24T19:08:35.049100Z"
    }
   },
   "id": "8909d15b61378759"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The percentage of yes answers in each data split is: Train; 62.64%, Val; 59.50%, Test;62.17%\n",
    "Seeing how difficult it was in past projects to reach a much better accuracy than the baseline majority class I am setting my goal for the pretrained BERT model at 64% accuracy on the test set.\n",
    "\n",
    "\n",
    "### Error Analysis\n",
    "To understand why the model may fail on certain predictions, I will conduct an error analysis investigating weather miss classifications are related to the confidence score the model has in its predictions. Low confidence on correct answers or high confidence on wrong answers may indicate areas where the model is uncertain or overconfident.\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "After the validation step, a confusion matrix is computed to assess true positives, false positives, true negatives, and false negatives. This provides insights into the model's prediction performance.\n",
    "\n",
    "\n",
    "## Planned Correctness Tests\n",
    "- Visually checking for decreasing loss during training.\n",
    "- Verifying predictions with a confusion matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c6e66c160f036d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "My last project went decently well, beating the majority class accuracy of 62.17% on the test set. Before writing this interpretation i toyed around with the `bert-large-cased` model, implementing and running it as quickly as possible just to see what it could do. With 333 Million parameters in the transformer model I had to use a `batch_size` of 16 to not run out of memory. Giving it a single run over the weekend, with \"looks about right\" choice for hyperparameters, it managed to reach a test accuracy of 72.63% after over 23 hours of runtime. Impressed by this result I am setting my expectations for the properly implemented and fine-tuned `bert-base-cased` model to reach a test accuracy of 69%. Nice."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663d884170047d86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-24T19:08:35.063941Z",
     "start_time": "2024-11-24T19:08:35.063710Z"
    }
   },
   "id": "f53fb18422d02e2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
