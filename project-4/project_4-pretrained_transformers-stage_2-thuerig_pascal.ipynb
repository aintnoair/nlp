{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers for BoolQ reading comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9a71120dba852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sources\n",
    "\n",
    "My sources for this project are linked in the respecting sections of the notebook. I used AI tools such as ChatGPT to correct my writing and grammar in stage 1 of this project and plan on using it for debugging during stage 2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1f5527cebe2ac9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### **Importing Python Packages**\n",
    "Making sure the notebook is reproducible and runs without error, I will install the necessary libraries in a pip cell below.\n",
    "\n",
    "### **Data Loading and Split**\n",
    "The BoolQ dataset contains binary question-answer pairs. Each entry consists of a question, a passage, and the corresponding binary answer (yes/no). The dataset is split as required by the course materials:\n",
    "- **Train Split:** The first 8427 entries of the training data.\n",
    "- **Validation Split:** The last 1000 entries of the training data.\n",
    "- **Test Split:** The validation split provided in the BoolQ dataset (3270 entries).\n",
    "\n",
    "### **Seeding for Reproducibility**\n",
    "A seed value of 42 is used to ensure reproducibility of results across different runs.\n",
    "\n",
    "### **Batch size**\n",
    "Setting the batch size in the beginning of the notebook for use throughout the code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e6e55ab05ef1c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: install all necessary packages\n",
    "# %pip install -q "
   ],
   "id": "72420aeaa79cdde7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Dict\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import wandb\n"
   ],
   "id": "45e4397f452f6d94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: set random seed to 42\n",
    "pl.seed_everything(42, workers=True)"
   ],
   "id": "a77a551be255670e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: set constants for projects\n",
    "BATCH_SIZE: int = 32\n",
    "MAX_SEQ_LENGTH: int = 512\n",
    "MODEL_NAME: str = \"bert-base-cased\"\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\"\n",
    "      f\"\\nMax seq length: {MAX_SEQ_LENGTH}\"\n",
    "      f\"\\nModel Name: {MODEL_NAME}\")"
   ],
   "id": "25ab177913c402df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tokenizer\n",
    "In this project, the `AutoTokenizer` from the Hugging Face Transformers library is used to handle text preprocessing. Specifically, the `bert-base-cased` associated with the \"bert-base-cased\" model is instantiated. This tokenizer manages:\n",
    "- Whitespace and special character removal.\n",
    "- Retaining case sensitivity to preserve semantic meaning.\n",
    "- Padding and truncation to ensure sequences are within the 512-token limit.\n",
    "\n",
    "##### Tokenization Details\n",
    "- **Questions:** The length of questions is capped at 21 tokens, based on dataset analysis.\n",
    "- **Passages:** Passages are padded or truncated to ensure the overall sequence remains within the 512-token limit.\n",
    "\n",
    "\n",
    "#### Handling OOV words\n",
    "The BERT Tokenizer utilizes a WordPiece tokenization strategy. Words not present in the vocabulary are decomposed into subword units. For example, an unknown word like \"unhappiness\" might be tokenized into \"un\", \"##happiness\". This approach ensures that even unseen words are represented through known subword components.\n",
    "\n",
    "\n",
    "#### Padding and Truncation\n",
    "Sequences are padded to a maximum length of 512 tokens, aligning with BERT's architecture constraints. Padding is applied to the maximum sequence length, not just the length of the longest sequence in a batch. This approach ensures consistent input dimensions across all batches, prioritizing simplicity and uniformity over computational efficiency. While dynamic batch padding could improve efficiency and reduce memory usage, the focus of this project is not on optimizing transformer models for efficiency. Therefore, fixed padding is used to streamline implementation and maintain uniformity.\n",
    "\n",
    "\n",
    "#### Case Sensitivity\n",
    "The \"bert-base-cased\" tokenizer is case-sensitive, distinguishing between words like \"Apple\" and \"apple\". This sensitivity preserves the semantic nuances of the text.\n",
    "\n",
    "\n",
    "### Embedding Layer\n",
    "The model leverages the pre-trained embeddings form the \"bert-base-cased\" model. These embeddings are fine-tuned during training to adapt to the specific nuances of the BoolQ dataset.\n",
    "\n",
    "\n",
    "### Positional Embeddings\n",
    "BERT inherently incorporates positional embeddings, unlike the transformer encoder in the last project, to capture the order of tokens in a sequence.\n",
    "\n",
    "\n",
    "### Input / Output / Label Format\n",
    "Each data point consists of a question, passage, and binary label (True/False). The preprocessing transforms these into the following formats:\n",
    "- **Tokenization**:\n",
    "    - *input*: Concatenated question and passage with `[CLS]` at the start, and `[SEP]` is placed between and at the end of the concatenated sequence.\n",
    "    - *output*: Tokenized sequences with attention masks:\n",
    "        - `input_ids`: Tensor of shape `(batch_size, sequence_length)`, representing token IDs.\n",
    "        - `attention_mask`: Tensor of shape `(batch_size, sequence_length)`, indicating non-padded tokens.\n",
    "- **Transformer Encoder**:\n",
    "    - *input*: Tokenized `input_ids` and `attention_mask` tensors with shape `(batch_size, sequence_length)`.\n",
    "    - *output*: Contextualized embeddings from the final encoder layer of shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "- **Classifier**:\n",
    "    - *input*: The embedding corresponding to the `[CLS]` token with shape `(batch_size, hidden_dim)`.\n",
    "    - *output*: A single logit per data point with shape `(batch_size, 1)`, representing the probability of the label being True.\n",
    "- **Label format**:\n",
    "    - Binary labels are encoded as integers (`0` for False, `1` for True)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3771c0c4b074dd6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, Any], tokenizer: AutoTokenizer, max_length: int = MAX_SEQ_LENGTH):\n",
    "        \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data[\"question\"])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Get question and passage\n",
    "        question = self.data[\"question\"][idx]\n",
    "        passage = self.data[\"passage\"][idx]\n",
    "        label = self.data[\"answer\"][idx]\n",
    "\n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            question,\n",
    "            passage,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Correctness tests for tokenization\n",
    "        assert encoded[\"input_ids\"].shape[-1] <= self.max_length, \"Token length exceeds max_length!\"\n",
    "        assert encoded[\"input_ids\"].shape == encoded[\"attention_mask\"].shape, \"Mismatch in token shapes!\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),  # Remove batch dimension\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),  # Float for binary classification\n",
    "        }"
   ],
   "id": "85e50f2eba87ed5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer_name: str, batch_size: int = BATCH_SIZE, max_length: int = MAX_SEQ_LENGTH):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Loading the dataset based on lecture slides\n",
    "        self.train_data = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "        self.validation_data = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "        self.test_data = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "        # Create datasets\n",
    "        self.train_dataset = BoolQDataset(self.train_data, self.tokenizer, self.max_length)\n",
    "        self.val_dataset = BoolQDataset(self.validation_data, self.tokenizer, self.max_length)\n",
    "        self.test_dataset = BoolQDataset(self.test_data, self.tokenizer, self.max_length)\n",
    "\n",
    "        # Test dataset length\n",
    "        assert len(self.train_dataset) == 8427, \"Train dataset length is incorrect!\"\n",
    "        assert len(self.val_dataset) == 1000, \"Validation dataset length is incorrect!\"\n",
    "        assert len(self.test_dataset) == 3270, \"Test dataset length is incorrect!\"\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "# Initialize DataModule\n",
    "data_module = BoolQDataModule(tokenizer_name=MODEL_NAME, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Prepare and test data loading\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Correctness test for DataLoader\n",
    "for batch in data_module.train_dataloader():\n",
    "    assert batch[\"input_ids\"].shape[0] == BATCH_SIZE, \"Batch size mismatch!\"\n",
    "    print(f\"Batch loaded successfully with shape: {batch['input_ids'].shape}\")\n",
    "    break\n",
    "\n"
   ],
   "id": "e2390e42c97a6a5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Architecture\n",
    "- **Pretrained Transformer Encoder:**\n",
    "  - Hugging Face’s `bert-base-cased` processes tokenized inputs.\n",
    "  - The output corresponding to the `[CLS]` token is extracted as a fixed-size representation.\n",
    "  - *The `bert-base-cased` model is pretrained using a masked language modeling and next sentence prediction objective and not question answering. This is why I deem it usable for this project where fine-tuned pretrained models are not allowed.*\n",
    "- **Classifier:**\n",
    "  - A two-layer fully connected network processes the `[CLS]` token embedding:\n",
    "    - **First Layer:** Projects the embedding to the hidden dimension with ReLU activation.\n",
    "    - **Dropout Layer:** Introduced after ReLU to reduce overfitting.\n",
    "    - **Second Layer:** Maps the hidden representation to a single binary output using Sigmoid activation.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "The Binary Cross-Entropy Loss (BCE) function is used to calculate the difference between predicted and true labels for binary classification.\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "*Learning rates stated here are for testing model functionality. Hyperparameters for experiments stated in `Experiments`.*\n",
    "- **AdamW Optimizer:**\n",
    "  - A learning rate of `2e-5` is used for the Transformer encoder.\n",
    "  - A higher learning rate of `2e-4` is applied to the classifier layers to allow faster convergence.\n",
    "\n",
    "\n",
    "### Checkpointing and Early Stopping\n",
    "- **Checkpointing:** Save the model with the best validation accuracy. Criteria for this will be the maximum validation accuracy.\n",
    "- **Early Stopping:** Terminates training if validation loss does not improve for 10 consecutive epochs.\n",
    "\n",
    "\n",
    "### Correctness Tests\n",
    "- **Tokenization**:\n",
    "    - Ensure the tokenized output does not exceed 512 tokens.\n",
    "    - Verify alignment between `input_ids` and `attention_mask` dimensions.\n",
    "\n",
    "- **DataLoader**:\n",
    "    - Verify batch size consistency during data loading.\n",
    "    - Check that the output tensors for `input_ids` and `attention_mask` match the expected batch size and sequence length.\n",
    "\n",
    "- **Model Input/Output**:\n",
    "    - Confirm the input to the Transformer encoder has the shape `(batch_size, sequence_length)`.\n",
    "    - Validate that the output of the Transformer encoder has the shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "\n",
    "- **Classifier Dimensions**:\n",
    "    - Check that the input to the classifier corresponds to the `[CLS]` token embedding with shape `(batch_size, hidden_dim)`.\n",
    "    - Ensure the output of the classifier has the shape `(batch_size, 1)`.\n",
    "\n",
    "- **Reproducibility**:\n",
    "    - Validate consistent results across multiple runs with the same random seed.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f27207bfc93ef5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            learning_rate: float = 1e-5,\n",
    "            hidden_dim: int = 256,\n",
    "            dropout_rate: float = 0.0,\n",
    "            weight_decay: float = 1e-4,\n",
    "            warmup_ratio: float = 0.1,  # Percentage of total steps\n",
    "    ):\n",
    "        super(BoolQClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Storage for test metrics\n",
    "        self.validation_preds = []\n",
    "        self.validation_labels = []\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        self.bert.train()\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        self.bert.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "        \n",
    "        # Store predictions and labels for confusion matrix\n",
    "        self.validation_preds.extend(preds.cpu().numpy())\n",
    "        self.validation_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        # Compute and log confusion matrix for validation\n",
    "        cm = confusion_matrix(self.validation_labels, self.validation_preds)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "        plt.title(\"Validation Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Log confusion matrix to WandB\n",
    "        self.logger.experiment.log({\"Validation Confusion Matrix\": wandb.Image(fig)})\n",
    "        plt.show()\n",
    "\n",
    "        # Clear storage for next epoch\n",
    "        self.validation_preds.clear()\n",
    "        self.validation_labels.clear()\n",
    "\n",
    "    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        self.bert.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        self.test_preds.extend(preds.cpu().numpy())\n",
    "        self.test_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "        # Log test metrics\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "        return {}\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        # Compute and log confusion matrix for test set\n",
    "        cm = confusion_matrix(self.test_labels, self.test_preds)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "        plt.title(\"Test Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Log confusion matrix to WandB\n",
    "        self.logger.experiment.log({\"Test Confusion Matrix\": wandb.Image(fig)})\n",
    "        plt.show()\n",
    "\n",
    "        # Clear storage\n",
    "        self.test_preds.clear()\n",
    "        self.test_labels.clear()\n",
    "\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Separate parameter groups\n",
    "        transformer_params = list(self.bert.parameters())\n",
    "        classifier_params = list(self.classifier.parameters())\n",
    "    \n",
    "        # Define learning rates\n",
    "        transformer_lr = self.hparams.learning_rate  # Base learning rate\n",
    "        classifier_lr = self.hparams.learning_rate * 10  # Higher learning rate for classifier\n",
    "    \n",
    "        # Create parameter groups\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': transformer_params, 'lr': transformer_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ])\n",
    "        \n",
    "        # Total steps calculation (approximation for one epoch)\n",
    "        total_steps = (\n",
    "            len(self.trainer.datamodule.train_dataloader())\n",
    "            * self.trainer.max_epochs\n",
    "        )\n",
    "        warmup_steps = int(total_steps * self.hparams.warmup_ratio)\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",  # Call after each optimizer step\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n"
   ],
   "id": "7fe0f3b0318f539d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "**Batch Size:** I will start with a batch_size of 16 and increase it to the maximum my hardware can handle then leaving it fixed as it is not a hyperparameter.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "The following hyperparameter ranges were explored during tuning:\n",
    "- **Learning Rate:** `[1e-3, 1e-6]` -> The learning rate for the classifier will be 10x the transformer learning rate, as described in the optimizer section.\n",
    "- **Classifier Hidden Dimension:** `[64, 512]`\n",
    "- **Dropout Rate:** `[0.1, 0.3]`\n",
    "- **Weight Decay:** `[1e-4, 1e-6]`\n",
    "- **Warmup Steps:** `[0.0, 0.1]` in % of total number of steps\n",
    "\n",
    "\n",
    "### Training Strategy\n",
    "For testing the model will be run with manually set hyperparameters. In a second stage the model will utilize optuna to automatically find the optimal hyperparameter combination.\n",
    "- **Epochs:** A maximum of 100 epochs is set, with early stopping enabled. *This will be adjusted based on the runtime per epoch.*\n",
    "- **Warmup Steps:** 0-10% warmup steps improved convergence in prior transformer projects during training. Will test with and without warmup.\n",
    "\n",
    "\n",
    "### Metrics\n",
    "- **Validation Accuracy:** To evaluate model performance across different hyperparameter configurations, I will use validation accuracy as the primary metric.\n",
    "- **Confusion Matrix:** This will give a comprehensive view of true positives, true negatives, false positives, and false negatives, allowing me deeper insight into the model’s performance.\n",
    "\n",
    "\n",
    "### Logging\n",
    "Weights and Biases (WandB) is used for experiment tracking, logging metrics such as train and validation loss, accuracy, and confusion matrices.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff047d494796bba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Helper Function to Format Run Name\n",
    "def format_run_name(hyperparams: dict) -> str:\n",
    "    return \"_\".join([f\"{key[:2]}_{val}\" for key, val in hyperparams.items()])\n",
    "\n",
    "checkpoint_dir = \"/bert_checkpoints/\"\n",
    "\n",
    "# Manual Training\n",
    "def train_manual():\n",
    "    # Hyperparameters\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout_rate\": 0.0,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": 10,\n",
    "        \"epochs\": 25,\n",
    "    }\n",
    "\n",
    "    # Run Name\n",
    "    run_name = format_run_name(hyperparameters)\n",
    "\n",
    "    # WandB Logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"testing\",\n",
    "        log_model=True,\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    # Initialize DataModule\n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME, \n",
    "        batch_size=hyperparameters[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Initialize Model\n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=hyperparameters[\"patience\"], mode='min')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename=run_name, dirpath=checkpoint_dir)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=2,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n"
   ],
   "id": "53c808634858c7f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# train_manual()",
   "id": "14b8806061122c6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna for hyperparameter tuning\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3),\n",
    "        \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 512, step=64),\n",
    "        \"dropout_rate\": trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5),\n",
    "        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2),\n",
    "        \"warmup_ratio\": trial.suggest_uniform(\"warmup_ratio\", 0.0, 0.1),\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "    \n",
    "    run_name = format_run_name(hyperparameters)\n",
    "    \n",
    "    logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"optuna\",\n",
    "        log_model=True,\n",
    "        reinit=True,\n",
    "    )\n",
    "    \n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME,\n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "    )\n",
    "    \n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"],\n",
    "        weight_decay=hyperparameters[\"weight_decay\"],\n",
    "        warmup_ratio=hyperparameters[\"warmup_ratio\"]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=hyperparameters[\"patience\"], mode=\"min\")\n",
    "    checkpoint = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1, filename=run_name, dirpath=checkpoint_dir)\n",
    "    pruning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=hyperparameters[\"epochs\"],\n",
    "        callbacks=[early_stopping, checkpoint, pruning_callback],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "        # Evaluate the model\n",
    "        val_acc = trainer.callback_metrics.get(\"val_acc\")\n",
    "        if val_acc is not None:\n",
    "            return val_acc.item()\n",
    "        else:\n",
    "            return 0.0\n",
    "    finally:\n",
    "        # Ensure WandB run is closed\n",
    "        wandb.finish()\n",
    "    \n",
    "    \n",
    "    \n",
    "def run_optuna():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    # Log best hyperparameters\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best value:\", study.best_value)\n",
    "\n",
    "    # Log best parameters to WandB\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=\"optuna_best\",\n",
    "        group=\"optuna\",\n",
    "        reinit=True,\n",
    "    )\n",
    "    wandb_logger.experiment.log({\"best_params\": study.best_params, \"best_value\": study.best_value})\n",
    "    wandb.finish()\n"
   ],
   "id": "f77973d3bf7f1722",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "run_optuna()",
   "id": "d425fd69582c16ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The percentage of yes answers in each data split is: Train; 62.64%, Val; 59.50%, Test;62.17%\n",
    "Seeing how difficult it was in past projects to reach a much better accuracy than the baseline majority class I am setting my goal for the pretrained BERT model at 64% accuracy on the test set.\n",
    "\n",
    "\n",
    "### Error Analysis\n",
    "To understand why the model may fail on certain predictions, I will conduct an error analysis investigating weather miss classifications are related to the confidence score the model has in its predictions. Low confidence on correct answers or high confidence on wrong answers may indicate areas where the model is uncertain or overconfident.\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "After the validation step, a confusion matrix is computed to assess true positives, false positives, true negatives, and false negatives. This provides insights into the model's prediction performance.\n",
    "\n",
    "\n",
    "## Planned Correctness Tests\n",
    "- Visually checking for decreasing loss during training.\n",
    "- Verifying predictions with a confusion matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c6e66c160f036d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "My last project went decently well, beating the majority class accuracy of 62.17% on the test set. Before writing this interpretation i toyed around with the `bert-large-cased` model, implementing and running it as quickly as possible just to see what it could do. With 333 Million parameters in the transformer model I had to use a `batch_size` of 16 to not run out of memory. Giving it a single run over the weekend, with \"looks about right\" choice for hyperparameters, it managed to reach a test accuracy of 72.63% after over 23 hours of runtime. Impressed by this result I am setting my expectations for the properly implemented and fine-tuned `bert-base-cased` model to reach a test accuracy of 69%. Nice."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663d884170047d86"
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "id": "f53fb18422d02e2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
