{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers for BoolQ reading comprehension\n",
    "*All changes and additions compared to Stage 1 are marked in <span style=\"color: orange;\">orange</span>*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9a71120dba852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sources\n",
    "\n",
    "My sources for this project are linked in the respecting sections of the notebook. I used AI tools such as ChatGPT to correct my writing and grammar in stage 1 of this project and plan on using it for debugging during stage 2. Additionally, I used the `hyperparameter_tuing` notebook from the MLOPS course as a rough guide for the structure in this project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1f5527cebe2ac9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">TLDR - Executive summary</span>\n",
    "### **Topic:**\n",
    "Implement and train a pre-trained Transformer model end to end for binary question answering on the BoolQ dataset from Hugging Face.\n",
    "\n",
    "### **Data:**\n",
    "The dataset is for reading comprehension and question answering tasks. It consists of questions, passages and the corresponding yes/no answers, with separate training and test splits. Splitting the last 1000 entries of the training split to use as a validation split in this project.\n",
    "\n",
    "### **Methods:**\n",
    "The project involves preprocessing the data using the AutoTokenizer that instantiates the correct tokenizer with the pre-trained \"bert-base-cased\" model. Using the [CLS] token from bert-base-cased model output as the input for my two-layer classifier with ReLU and a dropout layer inbetween both layers to finally receive the binary output. \n",
    "\n",
    "### **Model:**\n",
    "The model is based on a \"bert-base-cased\" model from Hugging Face and is structured as follows:\n",
    "- Bert Transformer: Leaving the default implementation, only setting it to train().\n",
    "- Classifier: A 2-layer classifier with ReLU non-linearity and a dropout layer.\n",
    "- Loss Function: BCELoss.\n",
    "\n",
    "### **Experiments:**\n",
    "Link to WandB: [Weights and Biases](https://wandb.ai/aintnoair/nlp-p4-pretrained_transformers/overview) <br>\n",
    "Some manual testing runs were conducted, verifying the functionality of the model before using optuna to find optimal hyperparameters. <br>\n",
    "Ranges explored by optuna in 5 runs (I'm aware this is not many, but deadlines and time-constraints as well as many other projects atm, sorry): \n",
    "- learning_rate: 1e-6, 1e-3\n",
    "- hidden_dim: 64, 512, step=64\n",
    "- dropout_rate: 0.0, 0.5\n",
    "- weight_decay: 1e-6, 1e-2\n",
    "- warmup_ratio: 0.0, 0.1  -- in % of total number of steps\n",
    "\n",
    "### **Results:**\n",
    "Key findings:\n",
    "- Adding 100 warmup steps at a batch size of 64 improved performance compared to no warmup.\n",
    "- A dropout rate of 0.2 was introduced to reduce overfitting on the training set.\n",
    "- After facing some issues with optuna for automatic hyperparameter tuning, manual tuning resulted in a model barely outperforming the majority baseline test accuracy of 62.17% with a test accuracy of **63.70%**.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d9e5c7491517699"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "### **Importing Python Packages**\n",
    "Making sure the notebook is reproducible and runs without error, I will install the necessary libraries in a pip cell below.\n",
    "\n",
    "### **Data Loading and Split**\n",
    "The BoolQ dataset contains binary question-answer pairs. Each entry consists of a question, a passage, and the corresponding binary answer (yes/no). The dataset is split as required by the course materials:\n",
    "- **Train Split:** The first 8427 entries of the training data.\n",
    "- **Validation Split:** The last 1000 entries of the training data.\n",
    "- **Test Split:** The validation split provided in the BoolQ dataset (3270 entries).\n",
    "\n",
    "### **Seeding for Reproducibility**\n",
    "A seed value of 42 is used to ensure reproducibility of results across different runs.\n",
    "\n",
    "### **Batch size**\n",
    "Setting the batch size in the beginning of the notebook for use throughout the code.\n",
    "<span style=\"color: orange;\">Also setting the maximum sequence length as well as the model name in the beginning of the notebook as these will not change during use.</span>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e6e55ab05ef1c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: install all necessary packages\n",
    "# %pip install -q "
   ],
   "id": "72420aeaa79cdde7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Dict\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import wandb\n"
   ],
   "id": "45e4397f452f6d94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: set random seed to 42\n",
    "pl.seed_everything(42, workers=True)"
   ],
   "id": "a77a551be255670e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: set constants for projects\n",
    "BATCH_SIZE: int = 32\n",
    "MAX_SEQ_LENGTH: int = 512\n",
    "MODEL_NAME: str = \"bert-base-cased\"\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\"\n",
    "      f\"\\nMax seq length: {MAX_SEQ_LENGTH}\"\n",
    "      f\"\\nModel Name: {MODEL_NAME}\")"
   ],
   "id": "25ab177913c402df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Experimental data exploration: not really necessary for the project but done anyway.\n",
    "\n",
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')\n",
    "\n",
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")\n",
    "\n",
    "train_yes_count = sum(1 for label in train_data['answer'] if label == 1)\n",
    "train_no_count = sum(1 for label in train_data['answer'] if label == 0)\n",
    "train_total = train_yes_count + train_no_count\n",
    "\n",
    "validation_yes_count = sum(1 for label in validation_data['answer'] if label == 1)\n",
    "validation_no_count = sum(1 for label in validation_data['answer'] if label == 0)\n",
    "validation_total = validation_yes_count + validation_no_count\n",
    "\n",
    "test_yes_count = sum(1 for label in test_data['answer'] if label == 1)\n",
    "test_no_count = sum(1 for label in test_data['answer'] if label == 0)\n",
    "test_total = test_yes_count + test_no_count\n",
    "\n",
    "print(f\"Train set (yes/no) Ratio: {round(train_yes_count / train_no_count, 2)}, Percent Yes: {round(train_yes_count / train_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Validation set (yes/no) Ratio: {round(validation_yes_count / validation_no_count, 2)}, Percent Yes: {round(validation_yes_count / validation_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Test set (yes/no) Ratio: {round(test_yes_count / test_no_count, 2)}, Percent Yes: {round(test_yes_count / test_total * 100, 2)}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-25T20:42:57.741221Z",
     "start_time": "2024-11-25T20:42:57.733209Z"
    }
   },
   "id": "d95626a3afea16a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tokenizer\n",
    "In this project, the `AutoTokenizer` from the Hugging Face Transformers library is used to handle text preprocessing. Specifically, the `bert-base-cased` associated with the \"bert-base-cased\" model is instantiated. This tokenizer manages:\n",
    "- Whitespace and special character removal.\n",
    "- Retaining case sensitivity to preserve semantic meaning.\n",
    "- Padding and truncation to ensure sequences are within the 512-token limit.\n",
    "\n",
    "##### Tokenization Details\n",
    "- **Questions:** The length of questions is capped at 21 tokens, based on dataset analysis.\n",
    "- **Passages:** Passages are padded or truncated to ensure the overall sequence remains within the 512-token limit.\n",
    "\n",
    "\n",
    "#### Handling OOV words\n",
    "The BERT Tokenizer utilizes a WordPiece tokenization strategy. Words not present in the vocabulary are decomposed into subword units. For example, an unknown word like \"unhappiness\" might be tokenized into \"un\", \"##happiness\". This approach ensures that even unseen words are represented through known subword components.\n",
    "\n",
    "\n",
    "#### Padding and Truncation\n",
    "Sequences are padded to a maximum length of 512 tokens, aligning with BERT's architecture constraints. ~~Padding is applied to the maximum sequence length, not just the length of the longest sequence in a batch. This approach ensures consistent input dimensions across all batches, prioritizing simplicity and uniformity over computational efficiency. While dynamic batch padding could improve efficiency and reduce memory usage, the focus of this project is not on optimizing transformer models for efficiency. Therefore, fixed padding is used to streamline implementation and maintain uniformity.~~\n",
    "<span style=\"color: orange;\">Correction: I got dynamic padding working, so I implemented that to save on hardware resources and speed up training.</span>\n",
    "\n",
    "\n",
    "#### Case Sensitivity\n",
    "The \"bert-base-cased\" tokenizer is case-sensitive, distinguishing between words like \"Apple\" and \"apple\". This sensitivity preserves the semantic nuances of the text.\n",
    "\n",
    "\n",
    "### Embedding Layer\n",
    "The model leverages the pre-trained embeddings form the \"bert-base-cased\" model. These embeddings are fine-tuned during training to adapt to the specific nuances of the BoolQ dataset.\n",
    "\n",
    "\n",
    "### Positional Embeddings\n",
    "BERT inherently incorporates positional embeddings, unlike the transformer encoder in the last project, to capture the order of tokens in a sequence.\n",
    "\n",
    "\n",
    "### Input / Output / Label Format\n",
    "Each data point consists of a question, passage, and binary label (True/False). The preprocessing transforms these into the following formats:\n",
    "- **Tokenization**:\n",
    "    - *input*: Concatenated question and passage with `[CLS]` at the start, and `[SEP]` is placed between and at the end of the concatenated sequence.\n",
    "    - *output*: Tokenized sequences with attention masks:\n",
    "        - `input_ids`: Tensor of shape `(batch_size, sequence_length)`, representing token IDs.\n",
    "        - `attention_mask`: Tensor of shape `(batch_size, sequence_length)`, indicating non-padded tokens.\n",
    "- **Transformer Encoder**:\n",
    "    - *input*: Tokenized `input_ids` and `attention_mask` tensors with shape `(batch_size, sequence_length)`.\n",
    "    - *output*: Contextualized embeddings from the final encoder layer of shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "- **Classifier**:\n",
    "    - *input*: The embedding corresponding to the `[CLS]` token with shape `(batch_size, hidden_dim)`.\n",
    "    - *output*: A single logit per data point with shape `(batch_size, 1)`, representing the probability of the label being True.\n",
    "- **Label format**:\n",
    "    - Binary labels are encoded as integers (`0` for False, `1` for True)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3771c0c4b074dd6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, Any], tokenizer: AutoTokenizer, max_length: int = MAX_SEQ_LENGTH):\n",
    "        \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data[\"question\"])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Get question and passage\n",
    "        question = self.data[\"question\"][idx]\n",
    "        passage = self.data[\"passage\"][idx]\n",
    "        label = self.data[\"answer\"][idx]\n",
    "\n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            question,\n",
    "            passage,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Correctness tests for tokenization\n",
    "        assert encoded[\"input_ids\"].shape[-1] <= self.max_length, \"Token length exceeds max_length!\"\n",
    "        assert encoded[\"input_ids\"].shape == encoded[\"attention_mask\"].shape, \"Mismatch in token shapes!\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),  # Remove batch dimension\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),  # Float for binary classification\n",
    "        }"
   ],
   "id": "85e50f2eba87ed5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer_name: str, batch_size: int = BATCH_SIZE, max_length: int = MAX_SEQ_LENGTH):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Loading the dataset based on lecture slides\n",
    "        self.train_data = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "        self.validation_data = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "        self.test_data = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "        # Create datasets\n",
    "        self.train_dataset = BoolQDataset(self.train_data, self.tokenizer, self.max_length)\n",
    "        self.val_dataset = BoolQDataset(self.validation_data, self.tokenizer, self.max_length)\n",
    "        self.test_dataset = BoolQDataset(self.test_data, self.tokenizer, self.max_length)\n",
    "\n",
    "        # Test dataset length\n",
    "        assert len(self.train_dataset) == 8427, \"Train dataset length is incorrect!\"\n",
    "        assert len(self.val_dataset) == 1000, \"Validation dataset length is incorrect!\"\n",
    "        assert len(self.test_dataset) == 3270, \"Test dataset length is incorrect!\"\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "# Initialize DataModule\n",
    "data_module = BoolQDataModule(tokenizer_name=MODEL_NAME, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Prepare and test data loading\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Correctness test for DataLoader\n",
    "for batch in data_module.train_dataloader():\n",
    "    assert batch[\"input_ids\"].shape[0] == BATCH_SIZE, \"Batch size mismatch!\"\n",
    "    print(f\"Batch loaded successfully with shape: {batch['input_ids'].shape}\")\n",
    "    break\n",
    "\n"
   ],
   "id": "e2390e42c97a6a5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "### Architecture\n",
    "- **Pretrained Transformer Encoder:**\n",
    "  - Hugging Face’s `bert-base-cased` processes tokenized inputs.\n",
    "  - The output corresponding to the `[CLS]` token is extracted as a fixed-size representation.\n",
    "  - *The `bert-base-cased` model is pretrained using a masked language modeling and next sentence prediction objective and not question answering. This is why I deem it usable for this project where fine-tuned pretrained models are not allowed.*\n",
    "- **Classifier:**\n",
    "  - A two-layer fully connected network processes the `[CLS]` token embedding:\n",
    "    - **First Layer:** Projects the embedding to the hidden dimension with ReLU activation.\n",
    "    - **Dropout Layer:** Introduced after ReLU to reduce overfitting.\n",
    "    - **Second Layer:** Maps the hidden representation to a single binary output using Sigmoid activation.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "The Binary Cross-Entropy Loss (BCE) function is used to calculate the difference between predicted and true labels for binary classification.\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "*Learning rates stated here are for testing model functionality. Hyperparameters for experiments stated in `Experiments`.*\n",
    "- **AdamW Optimizer:**\n",
    "  - A learning rate of `2e-5` is used for the Transformer encoder.\n",
    "  - A higher learning rate of `2e-4` is applied to the classifier layers to allow faster convergence.\n",
    "  - <span style=\"color: orange;\">These learning rates were only en example to demonstrate the classifier lr is 10x of the transformer lr.</span>\n",
    "\n",
    "\n",
    "### Checkpointing and Early Stopping\n",
    "- **Checkpointing:** Save the model with the best validation accuracy. Criteria for this will be the maximum validation accuracy.\n",
    "- **Early Stopping:** Terminates training if validation loss does not improve for <span style=\"color: orange;\">5</span> consecutive epochs.\n",
    "\n",
    "\n",
    "### Correctness Tests\n",
    "- **Tokenization**:\n",
    "    - Ensure the tokenized output does not exceed 512 tokens.\n",
    "    - Verify alignment between `input_ids` and `attention_mask` dimensions.\n",
    "\n",
    "- **DataLoader**:\n",
    "    - Verify batch size consistency during data loading.\n",
    "    - Check that the output tensors for `input_ids` and `attention_mask` match the expected batch size and sequence length.\n",
    "\n",
    "- **Model Input/Output**:\n",
    "    - Confirm the input to the Transformer encoder has the shape `(batch_size, sequence_length)`.\n",
    "    - Validate that the output of the Transformer encoder has the shape `(batch_size, sequence_length, hidden_dim)`.\n",
    "\n",
    "- **Classifier Dimensions**:\n",
    "    - Check that the input to the classifier corresponds to the `[CLS]` token embedding with shape `(batch_size, hidden_dim)`.\n",
    "    - Ensure the output of the classifier has the shape `(batch_size, 1)`.\n",
    "\n",
    "- **Reproducibility**:\n",
    "    - Validate consistent results across multiple runs with the same random seed.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f27207bfc93ef5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BoolQClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            learning_rate: float = 1e-5,\n",
    "            hidden_dim: int = 256,\n",
    "            dropout_rate: float = 0.0,\n",
    "            weight_decay: float = 1e-4,\n",
    "            warmup_ratio: float = 0.1,  # Percentage of total steps\n",
    "    ):\n",
    "        super(BoolQClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.bert.train()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Storage for confusion metrics\n",
    "        self.validation_preds = []\n",
    "        self.validation_labels = []\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        self.bert.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "        \n",
    "        self.validation_preds.extend(preds.cpu().numpy())\n",
    "        self.validation_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        cm = confusion_matrix(self.validation_labels, self.validation_preds)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "        plt.title(\"Validation Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        self.logger.experiment.log({\"Validation Confusion Matrix\": wandb.Image(fig)})\n",
    "        plt.show()\n",
    "\n",
    "        self.validation_preds.clear()\n",
    "        self.validation_labels.clear()\n",
    "\n",
    "    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        self.bert.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        preds = (logits > 0.5).float()\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "\n",
    "        self.test_preds.extend(preds.cpu().numpy())\n",
    "        self.test_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return {}\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        cm = confusion_matrix(self.test_labels, self.test_preds)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "        plt.title(\"Test Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        self.logger.experiment.log({\"Test Confusion Matrix\": wandb.Image(fig)})\n",
    "        plt.show()\n",
    "\n",
    "        self.test_preds.clear()\n",
    "        self.test_labels.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        transformer_params = list(self.bert.parameters())\n",
    "        classifier_params = list(self.classifier.parameters())\n",
    "    \n",
    "        transformer_lr = self.hparams.learning_rate  # Base learning rate\n",
    "        classifier_lr = self.hparams.learning_rate * 10  # Higher learning rate for classifier\n",
    "    \n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': transformer_params, 'lr': transformer_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ])\n",
    "        \n",
    "        # Total steps calculation (approximation for one epoch)\n",
    "        total_steps = (\n",
    "            len(self.trainer.datamodule.train_dataloader())\n",
    "            * self.trainer.max_epochs\n",
    "        )\n",
    "        warmup_steps = int(total_steps * self.hparams.warmup_ratio)\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler]\n"
   ],
   "id": "7fe0f3b0318f539d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "**Batch Size:** I will start with a batch_size of 16 and increase it to the maximum my hardware can handle then leaving it fixed as it is not a hyperparameter.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "The following hyperparameter ranges were explored during tuning:\n",
    "- **Learning Rate:** `[1e-3, 1e-6]` -> The learning rate for the classifier will be 10x the transformer learning rate, as described in the optimizer section.\n",
    "- **Classifier Hidden Dimension:** `[64, 512]`\n",
    "- **Dropout Rate:** `[0.1, 0.3]`\n",
    "- **Weight Decay:** `[1e-4, 1e-6]`\n",
    "- **Warmup Steps:** `[0.0, 0.1]` in % of total number of steps\n",
    "\n",
    "\n",
    "### Training Strategy\n",
    "For testing the model will be run with manually set hyperparameters. In a second stage the model will utilize optuna to automatically find the optimal hyperparameter combination.\n",
    "- **Epochs:** A maximum of ~~100~~ <span style=\"color: orange;\">10</span> epochs is set, with early stopping enabled. <span style=\"color: orange;\">This has been adjusted due to training times per epoch.</span>\n",
    "- **Warmup Steps:** 0-10% warmup steps improved convergence in prior transformer projects during training. Will test with and without warmup.\n",
    "\n",
    "\n",
    "### Metrics\n",
    "- **Validation Accuracy:** To evaluate model performance across different hyperparameter configurations, I will use validation accuracy as the primary metric.\n",
    "- **Confusion Matrix:** This will give a comprehensive view of true positives, true negatives, false positives, and false negatives, allowing me deeper insight into the model’s performance.\n",
    "\n",
    "\n",
    "### Logging\n",
    "Weights and Biases (WandB) is used for experiment tracking, logging metrics such as train and validation loss, accuracy, and confusion matrices.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff047d494796bba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Helper Function to Format Run Name\n",
    "def format_run_name(hyperparams: dict) -> str:\n",
    "    return \"_\".join([f\"{key[:2]}_{val}\" for key, val in hyperparams.items()])\n",
    "\n",
    "# Manual Training\n",
    "def train_manual():\n",
    "    # Hyperparameters - best optuna hyperparameters filled out\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 3.233413438380264e-05,\n",
    "        \"hidden_dim\": 448,\n",
    "        \"dropout_rate\": 0.20766842736826918,\n",
    "        \"weight_decay\": 7.370027643649387e-05,\n",
    "        \"warmup_ratio\": 0.08159326264046585,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "\n",
    "    run_name = format_run_name(hyperparameters)\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"testing\",\n",
    "        log_model=True,\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME, \n",
    "        batch_size=hyperparameters[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=hyperparameters[\"patience\"], mode='min')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename=\"best_optuna_run\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=2,\n",
    "        callbacks=[early_stopping, checkpoint],\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    wandb.finish()\n"
   ],
   "id": "53c808634858c7f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# train_manual()",
   "id": "14b8806061122c6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3),\n",
    "        \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 512, step=64),\n",
    "        \"dropout_rate\": trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5),\n",
    "        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2),\n",
    "        \"warmup_ratio\": trial.suggest_uniform(\"warmup_ratio\", 0.0, 0.1),\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": 5,\n",
    "        \"epochs\": 10,\n",
    "    }\n",
    "    \n",
    "    run_name = format_run_name(hyperparameters)\n",
    "    \n",
    "    logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"optuna\",\n",
    "        log_model=True,\n",
    "        reinit=True,\n",
    "    )\n",
    "    \n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME,\n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "    )\n",
    "    \n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"],\n",
    "        weight_decay=hyperparameters[\"weight_decay\"],\n",
    "        warmup_ratio=hyperparameters[\"warmup_ratio\"]\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=hyperparameters[\"patience\"], mode=\"min\")\n",
    "    checkpoint = ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1, filename=run_name, dirpath=checkpoint_dir)\n",
    "    pruning_callback = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        max_epochs=hyperparameters[\"epochs\"],\n",
    "        callbacks=[early_stopping, checkpoint, pruning_callback],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "        val_acc = trainer.callback_metrics.get(\"val_acc\")\n",
    "        if val_acc is not None:\n",
    "            return val_acc.item()\n",
    "        else:\n",
    "            return 0.0\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "    \n",
    "    \n",
    "    \n",
    "def run_optuna():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best value:\", study.best_value)\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=\"optuna_best\",\n",
    "        group=\"optuna\",\n",
    "        reinit=True,\n",
    "    )\n",
    "    wandb_logger.experiment.log({\"best_params\": study.best_params, \"best_value\": study.best_value})\n",
    "    wandb.finish()\n"
   ],
   "id": "f77973d3bf7f1722",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "run_optuna()",
   "id": "d425fd69582c16ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "The percentage of yes answers in each data split is: Train; 62.64%, Val; 59.50%, Test;62.17%\n",
    "Seeing how difficult it was in past projects to reach a much better accuracy than the baseline majority class I am setting my goal for the pretrained BERT model at 64% accuracy on the test set.\n",
    "\n",
    "\n",
    "### Error Analysis\n",
    "To understand why the model may fail on certain predictions, I will conduct an error analysis investigating weather miss classifications are related to the confidence score the model has in its predictions. Low confidence on correct answers or high confidence on wrong answers may indicate areas where the model is uncertain or overconfident.\n",
    "\n",
    "\n",
    "### Confusion Matrix\n",
    "After the validation step, a confusion matrix is computed to assess true positives, false positives, true negatives, and false negatives. This provides insights into the model's prediction performance.\n",
    "\n",
    "\n",
    "## Planned Correctness Tests\n",
    "- Visually checking for decreasing loss during training.\n",
    "- Verifying predictions with a confusion matrix.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c6e66c160f036d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">Evaluation</span>\n",
    "Opting to forego manual preprocessing and instead utilizing the built-in functionality of the AutoTokenizer module to manage padding, truncation, extra whitespace removal, special character handling, and case sensitivity proved successful. This approach simplified the implementation and minimized the risk of errors.\n",
    "\n",
    "### Data Imbalance Analysis\n",
    "An exploration of the dataset revealed a significant class imbalance across the training, validation, and test sets. The proportion of \"yes\" labels is consistently higher across all splits:\n",
    "\n",
    "Training set: 62.64% \"yes\" (1.68 ratio)\n",
    "Validation set: 59.5% \"yes\" (1.47 ratio)\n",
    "Test set: 62.17% \"yes\" (1.64 ratio)\n",
    "This imbalance likely contributed to the model's tendency to predominantly predict the majority \"yes\" class, especially in the early stages of training. Without adjustments to address this imbalance, the model achieved a validation accuracy of approximately XXXXXXX% but failed to learn nuanced patterns in the minority \"no\" class.\n",
    "\n",
    "To address this, I incorporated dropout, weight decay, and adjusted the learning rate during training. Additionally, I experimented with hyperparameter tuning using Optuna to optimize the model's performance further.\n",
    "\n",
    "### Error and Confusion Matrix Analysis\n",
    "In most runs, the Transformer model achieved a validation accuracy of approximately XXXXXXX%. However, after analyzing the label distribution in the validation set, it became evident that the model predominantly predicted the majority \"yes\" class. In optuna runs where quite a large learning rate was chosen (for example: 4e-3) the model would resort to solely predicting the majority class. To address this, I reduced the learning rate, incorporated a dropout layer and applied weight decay, which improved the model's performance, raising validation accuracy to XXXXXXXXX%. I also tried keeping the bert model in .eval() mode and just training the classifier (going a bit off script from the project assignment but oh well) proved to improve accuracy tremendously to XXXXXXXX%.\n",
    "\n",
    "### Training Accuracy and Overfitting\n",
    "The model quickly achieved a training accuracy of 98.9%, even after short runs. This suggests potential overfitting, as the validation accuracy remained significantly lower. Regularization techniques like dropout and weight decay were applied to address this, but the rapid overfitting indicates room for further improvement in generalization.\n",
    "\n",
    "### Comparison of BERT Models\n",
    "I ran only a few runs with Optuna (5 to be exact) to find hyperparameters that worked. Surprisingly, the bert-base-cased model came very close to the \"Hail Mary\" `bert-large-cased` run I did before formally starting this project. For that run, I used the `bert-large-cased` model, chose \"okay-looking\" hyperparameters, and let it train for 11 epochs until it early stopped. This \"Hail Mary\" run achieved a training accuracy of 72.63% and a validation accuracy of 72.60%. Astonishingly, my best `bert-base-cased` model, tuned using Optuna, reached a validation accuracy of XXXXXXXX% and a test accuracy of XXXXXXXX%. Seeing the smaller model perform so close to the larger model was unexpected and highlights the efficiency of hyperparameter tuning.\n",
    "\n",
    "### Evaluation on the test set\n",
    "For the test set evaluation, I selected the model with the highest validation accuracy from the Optuna trials. This model achieved a test accuracy of XXXXXXXX%, surpassing the baseline of 62.17%. While this represents a meaningful improvement, the model's reliance on the majority \"yes\" class limits its performance on minority class examples.\n",
    "\n",
    "### Recommendations for Future Projects\n",
    "To further address the class imbalance, future iterations could explore:\n",
    "\n",
    "- **Weighted Loss Functions**: Assigning higher weights to the minority \"no\" class to encourage the model to learn its patterns more effectively.\n",
    "- **Focal Loss**: Adjusting the loss function to focus more on harder-to-classify examples.\n",
    "Additionally, incorporating adversarial training or experimenting with ensemble methods could help the model better generalize to unseen data. Despite its limitations, the fine-tuned bert-base-cased model demonstrates promising performance and efficiency for the BoolQ dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6718fc66386e3c6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_path = Path(\"bert_checkpoints\")\n",
    "run_name = \"\"\n",
    "file_path = base_path / (run_name + \".ckpt\")\n",
    "\n",
    "model = BoolQClassifier.load_from_checkpoint(file_path)\n",
    "logger = WandbLogger(\n",
    "    project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=\"optuna_test\",\n",
    "        reinit=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(logger=logger)\n",
    "\n",
    "trainer.test(model, datamodule=data_module)\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844b07f90d871c4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "My last project went decently well, beating the majority class accuracy of 62.17% on the test set. Before writing this interpretation i toyed around with the `bert-large-cased` model, implementing and running it as quickly as possible just to see what it could do. With 333 Million parameters in the transformer model I had to use a `batch_size` of 16 to not run out of memory. Giving it a single run over the weekend, with \"looks about right\" choice for hyperparameters, it managed to reach a test accuracy of 72.63% after over 23 hours of runtime. Impressed by this result I am setting my expectations for the properly implemented and fine-tuned `bert-base-cased` model to reach a test accuracy of 69%. Nice."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "663d884170047d86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">Interpretation</span>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c9d0aa074af865a"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f53fb18422d02e2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
