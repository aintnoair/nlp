{
 "cells": [
  {
   "cell_type": "code",
   "id": "c66b5807-0571-4afe-89f6-84ada86885d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:24.216425Z",
     "start_time": "2024-11-25T14:43:21.704569Z"
    }
   },
   "source": [
    "%pip install -q transformers datasets pytorch-lightning optuna optuna-integration wandb"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:32.482776Z",
     "start_time": "2024-11-25T14:43:24.225430Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Dict\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:32.579985Z",
     "start_time": "2024-11-25T14:43:32.566204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(42, workers=True)\n"
   ],
   "id": "ca4df8620c95f13a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:32.611814Z",
     "start_time": "2024-11-25T14:43:32.607934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE: int = 32\n",
    "MAX_LENGTH: int = 512\n",
    "MODEL_NAME: str = \"bert-base-cased\"\n",
    "\n",
    "print(f\"Batch_size: {BATCH_SIZE}\"\n",
    "      f\"\\nMax_length: {MAX_LENGTH}\")"
   ],
   "id": "3ed88f62f13140bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size: 32\n",
      "Max_length: 512\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "1fb40973347bf146",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:32.659685Z",
     "start_time": "2024-11-25T14:43:32.653752Z"
    }
   },
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, Any], tokenizer: AutoTokenizer, max_length: int = MAX_LENGTH):\n",
    "        \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data[\"question\"])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Get question and passage\n",
    "        question = self.data[\"question\"][idx]\n",
    "        passage = self.data[\"passage\"][idx]\n",
    "        label = self.data[\"answer\"][idx]\n",
    "\n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            question,\n",
    "            passage,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Correctness tests for tokenization\n",
    "        assert encoded[\"input_ids\"].shape[-1] <= self.max_length, \"Token length exceeds max_length!\"\n",
    "        assert encoded[\"input_ids\"].shape == encoded[\"attention_mask\"].shape, \"Mismatch in token shapes!\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),  # Remove batch dimension\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),  # Float for binary classification\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "63ef54f0d51ffd70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:40.921417Z",
     "start_time": "2024-11-25T14:43:32.688294Z"
    }
   },
   "source": [
    "class BoolQDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, tokenizer_name: str, batch_size: int = BATCH_SIZE, max_length: int = MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Loading the dataset based on lecture slides\n",
    "        self.train_data = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "        self.validation_data = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "        self.test_data = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "\n",
    "        # Create datasets\n",
    "        self.train_dataset = BoolQDataset(self.train_data, self.tokenizer, self.max_length)\n",
    "        self.val_dataset = BoolQDataset(self.validation_data, self.tokenizer, self.max_length)\n",
    "        self.test_dataset = BoolQDataset(self.test_data, self.tokenizer, self.max_length)\n",
    "\n",
    "        # Test dataset length\n",
    "        assert len(self.train_dataset) == 8427, \"Train dataset length is incorrect!\"\n",
    "        assert len(self.val_dataset) == 1000, \"Validation dataset length is incorrect!\"\n",
    "        assert len(self.test_dataset) == 3270, \"Test dataset length is incorrect!\"\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "# Initialize DataModule\n",
    "data_module = BoolQDataModule(tokenizer_name=MODEL_NAME, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Prepare and test data loading\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Correctness test for DataLoader\n",
    "for batch in data_module.train_dataloader():\n",
    "    assert batch[\"input_ids\"].shape[0] == BATCH_SIZE, \"Batch size mismatch!\"\n",
    "    print(f\"Batch loaded successfully with shape: {batch['input_ids'].shape}\")\n",
    "    break\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loaded successfully with shape: torch.Size([32, 512])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "f3616761b60282e2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:40.948087Z",
     "start_time": "2024-11-25T14:43:40.932274Z"
    }
   },
   "source": [
    "class BoolQClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            learning_rate: float = 1e-5,\n",
    "            hidden_dim: int = 256,\n",
    "            dropout_rate: float = 0.\n",
    "    ):\n",
    "        super(BoolQClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "        # Storage for test metrics\n",
    "        self.val_preds = []\n",
    "        self.val_labels = []\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "    \n",
    "        # Store predictions and labels for confusion matrix\n",
    "        self.val_preds.extend(preds.cpu().numpy())\n",
    "        self.val_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "    \n",
    "        # Log validation metrics\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n",
    "    \n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(self.val_labels, self.val_preds)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        plt.title(\"Validation Confusion Matrix\")\n",
    "\n",
    "        # Log confusion matrix to WandB\n",
    "        wandb.log({\"val_confusion_matrix\": wandb.Image(fig)})\n",
    "\n",
    "        # Clear storage\n",
    "        self.val_preds.clear()\n",
    "        self.val_labels.clear()\n",
    "\n",
    "    def test_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, Any]:\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['label'])\n",
    "        preds = (logits > 0.5).float()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        self.test_preds.extend(preds.cpu().numpy())\n",
    "        self.test_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "        acc = (preds == batch['label']).float().mean()\n",
    "\n",
    "        # Log test metrics\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(self.test_labels, self.test_preds)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        plt.title(\"Test Confusion Matrix\")\n",
    "\n",
    "        # Log confusion matrix to WandB\n",
    "        wandb.log({\"test_confusion_matrix\": wandb.Image(fig)})\n",
    "\n",
    "        # Clear storage\n",
    "        self.test_preds.clear()\n",
    "        self.test_labels.clear()\n",
    "\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        # Separate parameter groups\n",
    "        transformer_params = list(self.bert.parameters())\n",
    "        classifier_params = list(self.classifier.parameters())\n",
    "    \n",
    "        # Define learning rates\n",
    "        transformer_lr = self.hparams.learning_rate  # Base learning rate\n",
    "        classifier_lr = self.hparams.learning_rate * 10  # Higher learning rate for classifier\n",
    "    \n",
    "        # Create parameter groups\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': transformer_params, 'lr': transformer_lr},\n",
    "            {'params': classifier_params, 'lr': classifier_lr}\n",
    "        ])\n",
    "    \n",
    "        return optimizer\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3a83f28347954c8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-25T14:43:43.349870Z",
     "start_time": "2024-11-25T14:43:40.956830Z"
    }
   },
   "source": [
    "# WandB Logger initialization\n",
    "def get_wandb_logger(run_name: str, group_name: str, hyperparameters: dict):\n",
    "    wandb.finish()\n",
    "    return WandbLogger(\n",
    "        project=\"nlp-p4-pretrained_transformers\",\n",
    "        name=run_name,\n",
    "        group=group_name,\n",
    "        log_model=True,\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "# Custom WandB Callback for Optuna Integration\n",
    "class CustomWandbLoggingCallback(pl.Callback):\n",
    "    def __init__(self, log_interval: int = 10):\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if (batch_idx + 1) % self.log_interval == 0:\n",
    "            metrics = trainer.callback_metrics\n",
    "            wandb.log({\n",
    "                \"train_loss\": metrics.get(\"train_loss\", None),\n",
    "                \"train_acc\": metrics.get(\"train_acc\", None),\n",
    "            })\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        wandb.log({\n",
    "            \"val_loss\": metrics.get(\"val_loss_epoch\", None),\n",
    "            \"val_acc\": metrics.get(\"val_acc_epoch\", None),\n",
    "        })\n",
    "\n",
    "# Helper Function to Format Run Name\n",
    "def format_run_name(hyperparams: dict) -> str:\n",
    "    return \"_\".join([f\"{key[:2]}_{val}\" for key, val in hyperparams.items()])\n",
    "\n",
    "# Manual Training\n",
    "def train_manual():\n",
    "    # Hyperparameters\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout_rate\": 0.3,\n",
    "        \"batch_size\": BATCH_SIZE\n",
    "    }\n",
    "\n",
    "    # Run Name\n",
    "    run_name = format_run_name(hyperparameters)\n",
    "\n",
    "    # WandB Logger\n",
    "    wandb_logger = get_wandb_logger(run_name, \"manual_testing\", hyperparameters)\n",
    "\n",
    "    # Initialize DataModule\n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME, \n",
    "        batch_size=hyperparameters[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Initialize Model\n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename=run_name)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=100,\n",
    "        callbacks=[early_stopping, checkpoint, CustomWandbLoggingCallback()],\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "# Optuna Objective with WandB Logging\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # Suggest hyperparameters\n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": trial.suggest_loguniform('learning_rate', 1e-6, 1e-4),\n",
    "        \"hidden_dim\": trial.suggest_int('hidden_dim', 128, 512, step=64),\n",
    "        \"dropout_rate\": trial.suggest_uniform('dropout_rate', 0.1, 0.5),\n",
    "        \"batch_size\": BATCH_SIZE\n",
    "    }\n",
    "\n",
    "    # Run Name\n",
    "    run_name = format_run_name(hyperparameters)\n",
    "\n",
    "    # WandB Logger\n",
    "    wandb_logger = get_wandb_logger(run_name, \"optuna_testing\", hyperparameters)\n",
    "\n",
    "    # Initialize DataModule\n",
    "    data_module = BoolQDataModule(\n",
    "        tokenizer_name=MODEL_NAME, \n",
    "        batch_size=hyperparameters[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Initialize Model\n",
    "    model = BoolQClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        hidden_dim=hyperparameters[\"hidden_dim\"],\n",
    "        dropout_rate=hyperparameters[\"dropout_rate\"]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1, filename=run_name)\n",
    "    pruning_callback = PyTorchLightningPruningCallback(trial, monitor='val_loss')\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[early_stopping, checkpoint, pruning_callback, CustomWandbLoggingCallback()],\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        logger=wandb_logger\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    # Validate to fetch the latest metrics\n",
    "    val_metrics = trainer.validate(model, datamodule=data_module, verbose=False)\n",
    "\n",
    "    # Finish WandB run\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Retrieve best score\n",
    "    return val_metrics[0]['val_acc']\n",
    "\n",
    "# Optuna Study\n",
    "def run_optuna():\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    # Best Hyperparameters\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Example Execution\n",
    "# Uncomment one of the following to run\n",
    "# train_manual()\n",
    "run_optuna()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-25 15:43:40,969] A new study created in memory with name: no-name-350ce0ae-4542-4e0c-9d09-52c78c10c198\n",
      "C:\\Users\\Pascal\\AppData\\Local\\Temp\\ipykernel_18252\\3816517564.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform('learning_rate', 1e-6, 1e-4),\n",
      "C:\\Users\\Pascal\\AppData\\Local\\Temp\\ipykernel_18252\\3816517564.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"dropout_rate\": trial.suggest_uniform('dropout_rate', 0.1, 0.5),\n",
      "[W 2024-11-25 15:43:42,519] Trial 0 failed with parameters: {'learning_rate': 1.2177914779583088e-06, 'hidden_dim': 512, 'dropout_rate': 0.1928871365969061} because of the following error: ValueError('Expected a parent').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Pascal\\AppData\\Local\\Temp\\ipykernel_18252\\3816517564.py\", line 121, in objective\n",
      "    trainer = Trainer(\n",
      "              ^^^^^^^^\n",
      "  File \"C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py\", line 70, in insert_env_defaults\n",
      "    return fn(self, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 425, in __init__\n",
      "    self._callback_connector.on_trainer_init(\n",
      "  File \"C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py\", line 79, in on_trainer_init\n",
      "    _validate_callbacks_list(self.trainer.callbacks)\n",
      "  File \"C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py\", line 227, in _validate_callbacks_list\n",
      "    stateful_callbacks = [cb for cb in callbacks if is_overridden(\"state_dict\", instance=cb)]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py\", line 227, in <listcomp>\n",
      "    stateful_callbacks = [cb for cb in callbacks if is_overridden(\"state_dict\", instance=cb)]\n",
      "                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Pascal\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py\", line 42, in is_overridden\n",
      "    raise ValueError(\"Expected a parent\")\n",
      "ValueError: Expected a parent\n",
      "[W 2024-11-25 15:43:42,523] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a parent",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 153\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest hyperparameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_params\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    150\u001B[0m \u001B[38;5;66;03m# Example Execution\u001B[39;00m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;66;03m# Uncomment one of the following to run\u001B[39;00m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m# train_manual()\u001B[39;00m\n\u001B[1;32m--> 153\u001B[0m run_optuna()\n",
      "Cell \u001B[1;32mIn[8], line 144\u001B[0m, in \u001B[0;36mrun_optuna\u001B[1;34m()\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_optuna\u001B[39m():\n\u001B[0;32m    143\u001B[0m     study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 144\u001B[0m     study\u001B[38;5;241m.\u001B[39moptimize(objective, n_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;66;03m# Best Hyperparameters\u001B[39;00m\n\u001B[0;32m    147\u001B[0m     best_params \u001B[38;5;241m=\u001B[39m study\u001B[38;5;241m.\u001B[39mbest_params\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\optuna\\study\\study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    385\u001B[0m \n\u001B[0;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 475\u001B[0m     _optimize(\n\u001B[0;32m    476\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    477\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m    478\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[0;32m    479\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    480\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[0;32m    481\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[0;32m    482\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    483\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[0;32m    484\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[0;32m    485\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 63\u001B[0m         _optimize_sequential(\n\u001B[0;32m     64\u001B[0m             study,\n\u001B[0;32m     65\u001B[0m             func,\n\u001B[0;32m     66\u001B[0m             n_trials,\n\u001B[0;32m     67\u001B[0m             timeout,\n\u001B[0;32m     68\u001B[0m             catch,\n\u001B[0;32m     69\u001B[0m             callbacks,\n\u001B[0;32m     70\u001B[0m             gc_after_trial,\n\u001B[0;32m     71\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     72\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     73\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[0;32m     74\u001B[0m         )\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    247\u001B[0m ):\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[8], line 121\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m    118\u001B[0m pruning_callback \u001B[38;5;241m=\u001B[39m PyTorchLightningPruningCallback(trial, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    120\u001B[0m \u001B[38;5;66;03m# Trainer\u001B[39;00m\n\u001B[1;32m--> 121\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m    122\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m    123\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[early_stopping, checkpoint, pruning_callback, CustomWandbLoggingCallback()],\n\u001B[0;32m    124\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    125\u001B[0m     devices\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    126\u001B[0m     logger\u001B[38;5;241m=\u001B[39mwandb_logger\n\u001B[0;32m    127\u001B[0m )\n\u001B[0;32m    129\u001B[0m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[0;32m    130\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model, datamodule\u001B[38;5;241m=\u001B[39mdata_module)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:70\u001B[0m, in \u001B[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mlist\u001B[39m(env_variables\u001B[38;5;241m.\u001B[39mitems()) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(kwargs\u001B[38;5;241m.\u001B[39mitems()))\n\u001B[0;32m     69\u001B[0m \u001B[38;5;66;03m# all args were already moved to kwargs\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:425\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccumulate_grad_batches \u001B[38;5;241m=\u001B[39m accumulate_grad_batches\n\u001B[0;32m    423\u001B[0m \u001B[38;5;66;03m# init callbacks\u001B[39;00m\n\u001B[0;32m    424\u001B[0m \u001B[38;5;66;03m# Declare attributes to be set in _callback_connector on_trainer_init\u001B[39;00m\n\u001B[1;32m--> 425\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback_connector\u001B[38;5;241m.\u001B[39mon_trainer_init(\n\u001B[0;32m    426\u001B[0m     callbacks,\n\u001B[0;32m    427\u001B[0m     enable_checkpointing,\n\u001B[0;32m    428\u001B[0m     enable_progress_bar,\n\u001B[0;32m    429\u001B[0m     default_root_dir,\n\u001B[0;32m    430\u001B[0m     enable_model_summary,\n\u001B[0;32m    431\u001B[0m     max_time,\n\u001B[0;32m    432\u001B[0m )\n\u001B[0;32m    434\u001B[0m \u001B[38;5;66;03m# init data flags\u001B[39;00m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_val_every_n_epoch: Optional[\u001B[38;5;28mint\u001B[39m]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:79\u001B[0m, in \u001B[0;36m_CallbackConnector.on_trainer_init\u001B[1;34m(self, callbacks, enable_checkpointing, enable_progress_bar, default_root_dir, enable_model_summary, max_time)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configure_model_summary_callback(enable_model_summary)\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mextend(_load_external_callbacks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpytorch_lightning.callbacks_factory\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 79\u001B[0m _validate_callbacks_list(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mcallbacks)\n\u001B[0;32m     81\u001B[0m \u001B[38;5;66;03m# push all model checkpoint callbacks to the end\u001B[39;00m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;66;03m# it is important that these are the last callbacks to run\u001B[39;00m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mcallbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reorder_callbacks(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mcallbacks)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:227\u001B[0m, in \u001B[0;36m_validate_callbacks_list\u001B[1;34m(callbacks)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_callbacks_list\u001B[39m(callbacks: List[Callback]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 227\u001B[0m     stateful_callbacks \u001B[38;5;241m=\u001B[39m [cb \u001B[38;5;28;01mfor\u001B[39;00m cb \u001B[38;5;129;01min\u001B[39;00m callbacks \u001B[38;5;28;01mif\u001B[39;00m is_overridden(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m, instance\u001B[38;5;241m=\u001B[39mcb)]\n\u001B[0;32m    228\u001B[0m     seen_callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m stateful_callbacks:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:227\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_callbacks_list\u001B[39m(callbacks: List[Callback]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 227\u001B[0m     stateful_callbacks \u001B[38;5;241m=\u001B[39m [cb \u001B[38;5;28;01mfor\u001B[39;00m cb \u001B[38;5;129;01min\u001B[39;00m callbacks \u001B[38;5;28;01mif\u001B[39;00m is_overridden(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m, instance\u001B[38;5;241m=\u001B[39mcb)]\n\u001B[0;32m    228\u001B[0m     seen_callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m stateful_callbacks:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:42\u001B[0m, in \u001B[0;36mis_overridden\u001B[1;34m(method_name, instance, parent)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m parent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     41\u001B[0m         _check_mixed_imports(instance)\n\u001B[1;32m---> 42\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a parent\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlightning_utilities\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moverrides\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_overridden \u001B[38;5;28;01mas\u001B[39;00m _is_overridden\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _is_overridden(method_name, instance, parent)\n",
      "\u001B[1;31mValueError\u001B[0m: Expected a parent"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "455f188879f0fb06",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\"\"\"\n",
    "# Define paths and load model from checkpoint\n",
    "base_path = Path(\"nlp-p4-pretrained_transformers/nlfg3sfr/checkpoints\")\n",
    "run_name = \"best_model\"\n",
    "file_path = base_path / (run_name + \".ckpt\")\n",
    "\n",
    "# Initialize WandB logger for evaluation\n",
    "wandb_logger = WandbLogger(project=\"nlp-p4-pretrained_transformers\", name=run_name, group=\"evaluation\")\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model = BoolQClassifier.load_from_checkpoint(file_path)\n",
    "\n",
    "# Initialize DataModule for testing\n",
    "data_module = BoolQDataModule(tokenizer_name=\"bert-base-cased\", batch_size=BATCH_SIZE)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Initialize the trainer for testing\n",
    "trainer = pl.Trainer(logger=wandb_logger)\n",
    "\n",
    "# Run testing on the test set\n",
    "trainer.test(model, dataloaders=data_module.test_dataloader())\n",
    "\n",
    "# Finish WandB session\n",
    "wandb.finish()\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d0431a2-cf45-4d06-b93d-3b773ac0a073",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
