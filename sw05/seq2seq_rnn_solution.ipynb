{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cf81d1",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN\n",
    "In this exercise, we implement a sequence-to-sequence RNN (without attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827d5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6923b",
   "metadata": {},
   "source": [
    "We first define our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b02ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "sequence_length = 5\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfc188",
   "metadata": {},
   "source": [
    "Create a bidirectional [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f1c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc0342",
   "metadata": {},
   "source": [
    "We create an example input `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89463769",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a3a2c",
   "metadata": {},
   "source": [
    "What should the initial hidden and cell state be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf1dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_directions = 2 if bidirectional else 1\n",
    "h0 = torch.zeros(num_layers * num_directions, batch_size, hidden_dim)\n",
    "c0 = torch.zeros(num_layers * num_directions, batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3373c",
   "metadata": {},
   "source": [
    "Now we run our LSTM. Look at the output. Explain each dimension of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18b7612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 3, 40]), sequence_length x batch_size x concatenated final-layer hidden states from both directions\n",
      "Last hidden state shape: torch.Size([4, 3, 20]), num_layers * num_directions x batch_size x hidden_dim\n",
      "Last cell state shape: torch.Size([4, 3, 20]), num_layers * num_directions x batch_size x hidden_dim\n"
     ]
    }
   ],
   "source": [
    "output, (hn, cn) = lstm(x, (h0, c0))\n",
    "print(f'Output shape: {output.shape}, sequence_length x batch_size x concatenated final-layer hidden states from both directions')\n",
    "print(f'Last hidden state shape: {hn.shape}, num_layers * num_directions x batch_size x hidden_dim')\n",
    "print(f'Last cell state shape: {cn.shape}, num_layers * num_directions x batch_size x hidden_dim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d6d2d",
   "metadata": {},
   "source": [
    "All outputs are from the last (2nd) layer of the LSTM. If we want to have access to the hidden states of layer 1 as well, we have to run the `LSTMCell`s ourselves.\n",
    "\n",
    "When we take the above LSTM as the encoder, what is its output that serves as the input to the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5386b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40])\n"
     ]
    }
   ],
   "source": [
    "encoder = lstm\n",
    "\n",
    "# concat the final hidden state of the last layer in both directions\n",
    "# hn[2] is the n-th hidden state of the second layer in the left-to-right direction\n",
    "# hn[3] is the 1st hidden state of the second layer in the right-to-left direction (i.e. the last that was processed when going from right to left)\n",
    "encoder_output = torch.concat((hn[2, :, :], hn[3, :, :]), dim=-1)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7afab4",
   "metadata": {},
   "source": [
    "Create a decoder LSTM with 2 layers. Why can't it be bidirectional as well? What is the hidden dimension of the decoder LSTM when you want to initialize it with the encoder output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373c7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder can't be bidirectional since it generates the words:\n",
    "# When generating word i, the word at position i+1 is not known yet.\n",
    "decoder_hidden_dim = num_directions * hidden_dim  # has to be the same as encoder output\n",
    "# if you want a smaller decoder hidden dim, insert a projection (multiplication with a matrix)\n",
    "decoder = nn.LSTM(embedding_dim, decoder_hidden_dim, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab709dc",
   "metadata": {},
   "source": [
    "Run your decoder LSTM on an example sequence. Condition it with the encoder representation of the sequence. How do we get the correct shape for the initial hidden state?\n",
    "\n",
    "**Hint:** Take a look at [Torch's tensor operations](https://pytorch.org/docs/stable/tensors.html) and compare `Torch.repeat`, `Torch.repeat_interleave` and `Tensor.expand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56965f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_dec_0 shape: torch.Size([2, 3, 40])\n",
      "decoder output shape: torch.Size([8, 3, 40])\n"
     ]
    }
   ],
   "source": [
    "output_seq_len = 8\n",
    "y = torch.randn(output_seq_len, batch_size, embedding_dim)\n",
    "h_dec_0 = encoder_output.unsqueeze(0).expand(num_layers, -1, -1)\n",
    "print('h_dec_0 shape:', h_dec_0.shape)\n",
    "c_dec_0 = torch.zeros(num_layers, batch_size, decoder_hidden_dim)\n",
    "decoder_output, (h_dec_n, c_dec_n) = decoder(y, (h_dec_0, c_dec_0))\n",
    "print('decoder output shape:', decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ac2ab",
   "metadata": {},
   "source": [
    "In most RNNs, the final encoder hidden state is used as the first hidden state of the decoder RNN. In some variants, it has also been concatenated with the hidden state of the previous time step at each decoder time step. In PyTorch's `nn.LSTM` implementation, we cannot easily do that, so we would have to resort to the lower-level `nn.LSTMCell` class again.\n",
    "\n",
    "Put it all together in a seq2seq LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af981a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqLSTM(nn.Module):\n",
    "    \"\"\" Sequence-to-sequence LSTM. \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, num_encoder_layers, num_decoder_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_encoder_layers, bidirectional=bidirectional)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.decoder = nn.LSTM(embedding_dim, self.num_directions * hidden_dim, num_layers=num_decoder_layers)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        assert x.dim() == 3, \"Expected input of shape [sequence length, batch size, embedding dim]\"\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        # encoder forward\n",
    "        h0 = torch.zeros(self.encoder.num_layers * self.num_directions, batch_size, self.encoder.hidden_size)\n",
    "        c0 = torch.zeros(self.encoder.num_layers * self.num_directions, batch_size, self.encoder.hidden_size)\n",
    "        encoder_outputs, (hn, cn) = self.encoder(x, (h0, c0))\n",
    "\n",
    "        # decoder forward\n",
    "        encoder_output = torch.concat((hn[-2], hn[-1]), -1) if self.num_directions == 2 else hn[-1]\n",
    "        h_dec_0 = encoder_output.expand(self.decoder.num_layers, -1, -1)\n",
    "        c_dec_0 = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size)\n",
    "        decoder_outputs, _ = self.decoder(y, (h_dec_0, c_dec_0))\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241dd1ad",
   "metadata": {},
   "source": [
    "Test your seq2seq LSTM with an input sequence `x` and a ground truth output sequence `y` that the decoder tries to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ef14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "seq2seq_lstm = Seq2seqLSTM(embedding_dim, hidden_dim, num_layers, num_layers, bidirectional)\n",
    "x = torch.randn(10, 2, embedding_dim)\n",
    "y = torch.randn(9, 2, embedding_dim)\n",
    "outputs = seq2seq_lstm(x, y)\n",
    "assert outputs.dim() == 3 and list(outputs.size()) == [9, 2, decoder_hidden_dim], \"Wrong output shape\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
