{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701d093c",
   "metadata": {},
   "source": [
    "# Position Encoding\n",
    "In this exercise, we will look at position encoding for the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d33bf",
   "metadata": {},
   "source": [
    "## Creating an Embedding\n",
    "Consider the following vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1456a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['dog', 'cat', 'fox', 'walks', 'jumps', 'sleeps', 'and', 'the', '.', ',']\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3db4da",
   "metadata": {},
   "source": [
    "Take a look at the [nn.Embedding documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). Create an embedding with a vocabulary size of 10 and an embedding dimension of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f670891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31df965e",
   "metadata": {},
   "source": [
    "Embed the sentence: \"the dog sleeps , the cat walks and the fox jumps .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00533ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5279810",
   "metadata": {},
   "source": [
    "## Absolute Position Encoding\n",
    "The functions for absolute position encoding, as defined in [the Transformer paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "pos_{i, 2j} &= \\sin(i / 10000^{2j/d}) \\\\\n",
    "pos_{i, 2j+1} &= \\cos(i / 10000^{2j/d})\n",
    "\\end{align}\n",
    "$$\n",
    "where $i$ is the absolute position in the sequence, and $j$ is the dimension of the embedding vector.\n",
    "\n",
    "Create a function `absolute_position_encoding` that takes the position in the sequence $i$ and the dimension $d$ as an input and returns the position vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae18bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_position_encoding(position, dim):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83501bf",
   "metadata": {},
   "source": [
    "Run the command below to see the values for the position vectors of the first 100 positions and the first 5 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8941fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "x = list(range(100))\n",
    "pos_vectors = [absolute_position_encoding(i, emb_dim) for i in x]\n",
    "for dim in range(5):\n",
    "    y = [pv[dim].item() for pv in pos_vectors]\n",
    "    plt.plot(x, y, label=f'dim {dim}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247803a",
   "metadata": {},
   "source": [
    "Apply the position encoding to the embeddings from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44457e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3f7ae75",
   "metadata": {},
   "source": [
    "## Absolute Position Embedding\n",
    "Write a class `AbsolutePositionEmbedding` that is initialized with a maximum length and an embedding dimension. In its `forward` method, it should take an input tensor (of shape `[batch_size, sequence_length, embedding_dim]`) and add the position embeddings to the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, max_length=512):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9c11e",
   "metadata": {},
   "source": [
    "Try your class with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_position_embedding = AbsolutePositionEmbedding(20, 512)\n",
    "x1 = torch.randn(5, 12, 20)\n",
    "x = absolute_position_embedding(x1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a5ec7",
   "metadata": {},
   "source": [
    "## Relative Position Embedding\n",
    "Create a class `RelativePositionEmbedding` that is initialized with a maximum relative distance and an embedding dimension. Its `forward` method should take an input tensor of size `[batch_size, hidden_dim]` and apply the relative position embeddings given the positions $i$ of the query and $j$ of the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a58a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, max_dist=16):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, i, j):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb111a",
   "metadata": {},
   "source": [
    "Try it with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_pos_emb = RelativePositionEmbedding(20, 16)\n",
    "x = torch.randn(5, 20)\n",
    "result = rel_pos_emb(x, 0, 3)\n",
    "result = rel_pos_emb(x, 49, 15)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d94df0",
   "metadata": {},
   "source": [
    "**Question:** Where would we use this module?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f992c1",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f2fa7",
   "metadata": {},
   "source": [
    "**Question:** Look at the HuggingFace implementation of relative position embeddings in the BERT model:\n",
    "- [initialization from line 244](https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/models/bert/modeling_bert.py#L244)\n",
    "- [forward method from line 320](https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/models/bert/modeling_bert.py#L320)\n",
    "\n",
    "Describe what their `relative_key` method does differently from the [Shaw et al. (2018)](https://aclanthology.org/N18-2074/) paper we saw in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdaa68f",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
