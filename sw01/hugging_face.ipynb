{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e265922",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "In this notebook, we'll get to know the Hugging Face ecosystem by loading a dataset, encoding the input data, running a model, and evaluating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261d2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbbaad",
   "metadata": {},
   "source": [
    "Take a look at the [Hugging Face datasets hub](https://huggingface.co/datasets). Find the MRPC (Microsoft Research Paraphrase Corpus) dataset that is part of the GLUE (General Language Understanding Evaluation) benchmark. Download the validation split of the dataset with dataset's `load_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673e7824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 408\n",
      "})\n",
      "{'sentence1': \"He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\", 'sentence2': '\" The foodservice pie business does not fit our long-term growth strategy .', 'label': 1, 'idx': 9}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"validation\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4f191",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "With Transformers (we will get to know them in more detail later in the course), tokenization has become part of the model itself. We first install Hugging Face's transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772fb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.9.11 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4933fc",
   "metadata": {},
   "source": [
    "Use the [model page of the base-uncased version of BERT](https://huggingface.co/bert-base-uncased) to initialize a `BertTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd20df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ccab3",
   "metadata": {},
   "source": [
    "Encode the first sentence of the first example in the dataset. Look at the outputs of the following functions:\n",
    "- `tokenizer(sentence)`\n",
    "- `tokenizer.encode(sentence)`\n",
    "- `tokenizer.tokenize(sentence)`\n",
    "- `tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928c8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: \n",
      " dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Sub-words: \n",
      " {'input_ids': [101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Input Ids: \n",
      " [101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102]\n",
      "Splitting Text: \n",
      " ['he', 'said', 'the', 'foods', '##er', '##vic', '##e', 'pie', 'business', 'doesn', \"'\", 't', 'fit', 'the', 'company', \"'\", 's', 'long', '-', 'term', 'growth', 'strategy', '.']\n",
      "Tokenize then map to Ids: \n",
      " [2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012]\n"
     ]
    }
   ],
   "source": [
    "sentence = dataset[0]['sentence1']\n",
    "\n",
    "print(f\"Keys: \\n {tokenizer(sentence).keys()}\")\n",
    "print(f\"Sub-words: \\n {tokenizer(sentence)}\")\n",
    "print(f\"Input Ids: \\n {tokenizer.encode(sentence)}\")\n",
    "print(f\"Splitting Text: \\n {tokenizer.tokenize(sentence)}\")\n",
    "print(f\"Tokenize then map to Ids: \\n {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44f21a",
   "metadata": {},
   "source": [
    "**Decoding.** Check out the various ways of decoding: `.decode`, `.convert_ids_to_tokens`, `.convert_tokens_to_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee2d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] he said the foodservice pie business doesn't fit the company's long - term growth strategy. [SEP]\n",
      "he said the foodservice pie business doesn't fit the company's long - term growth strategy.\n",
      "he said the foodservice pie business doesn't fit the company's long - term growth strategy.\n",
      "['[CLS]', 'he', 'said', 'the', 'foods', '##er', '##vic', '##e', 'pie', 'business', 'doesn', \"'\", 't', 'fit', 'the', 'company', \"'\", 's', 'long', '-', 'term', 'growth', 'strategy', '.', '[SEP]']\n",
      "[CLS] he said the foodservice pie business doesn ' t fit the company ' s long - term growth strategy . [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(sentence)['input_ids']\n",
    "print(tokenizer.decode(input_ids))\n",
    "print(tokenizer.decode(input_ids, skip_special_tokens=True))\n",
    "print(tokenizer.decode(input_ids, skip_special_tokens=True, remove_tokenization_spaces=True))\n",
    "print(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748080cf",
   "metadata": {},
   "source": [
    "Use the NLP section of the [quickstart guide](https://huggingface.co/docs/datasets/quickstart) to apply encoding to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c07753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2002, 2056, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2987, 1005, 1056, 4906, 1996, 2194, 1005, 1055, 2146, 1011, 2744, 3930, 5656, 1012, 102, 1000, 1996, 9440, 2121, 7903, 2063, 11345, 2449, 2515, 2025, 4906, 2256, 2146, 1011, 2744, 3930, 5656, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 20201, 22948, 2056, 10958, 19053, 4140, 6283, 1996, 8956, 6939, 1998, 2246, 2830, 2000, 2478, 2010, 2146, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 2010, 2564, 2056, 2002, 2001, 1000, 2531, 3867, 2369, 2577, 5747, 1000, 1998, 2246, 2830, 2000, 2478, 2010, 2086, 1997, 2731, 1999, 1996, 2162, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "print(dataset[0]['input_ids'])\n",
    "print(dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc640318",
   "metadata": {},
   "source": [
    "We have to rename the \"label\" column to \"labels\" to match the expected name in BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "751be0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bd126",
   "metadata": {},
   "source": [
    "- Use the guide again to set the data format to \"torch\". Make sure the columns `input_ids`, `token_type_ids`, `attention_mask` and `labels` are present.\n",
    "- Create a data loader with a `batch_size` of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38eda65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6e2bc",
   "metadata": {},
   "source": [
    "## Model\n",
    "We now load a pretrained BERT model and perform sequence classification on the MRPC dataset. Load the `BertForSequenceClassification` model. Set the model to evaluation mode by calling `.eval()` on the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955542a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.eval() # If you just want to use, but not train the model -> put it into eval mode\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be37355-37ce-4d6d-bff0-25cb8c55c018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ead6fcf",
   "metadata": {},
   "source": [
    "In the evaluation state, no gradient information will be saved in the forward pass, and no dropout will be applied (and the values rescaled to match the training's output distribution). We can always set it back to train mode with `.train()`.\n",
    "\n",
    "Additionally, we should call the model in a `torch.no_grad()` context, which sets all the tensors' `.requires_grad` fields to False.\n",
    "\n",
    "## Forward pass\n",
    "Now we run the model on a single batch. Get a batch from the dataloader, pass it to the model's forward function. It is preferred to use `model(.)` to do this instead of `model.forward(.)`. Some hooks may not be run if you use the latter version, as mentioned in this [PyTorch forum question](https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690).\n",
    "\n",
    "- Run a single batch through the model.\n",
    "- Get the output logits\n",
    "- Run a softmax function on it (use `torch.nn.functional.softmax`) to get output probabilities\n",
    "- Display the result (i.e. is sentence2 a paraphrase of sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31c562f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: he said the foodservice pie business doesn't fit the company's long - term growth strategy. \" the foodservice pie business does not fit our long - term growth strategy.\n",
      "Sentence 2: he said the foodservice pie business doesn't fit the company's long - term growth strategy. \" the foodservice pie business does not fit our long - term growth strategy.\n",
      "Is sentence2 a paraphrase of sentence1? Yes\n",
      "Probabilities: tensor([0.2025, 0.7975], device='cuda:0')\n",
      "\n",
      "Sentence 1: magnarelli said racicot hated the iraqi regime and looked forward to using his long years of training in the war. his wife said he was \" 100 percent behind george bush \" and looked forward to using his years of training in the war.\n",
      "Sentence 2: magnarelli said racicot hated the iraqi regime and looked forward to using his long years of training in the war. his wife said he was \" 100 percent behind george bush \" and looked forward to using his years of training in the war.\n",
      "Is sentence2 a paraphrase of sentence1? Yes\n",
      "Probabilities: tensor([0.2030, 0.7970], device='cuda:0')\n",
      "\n",
      "Sentence 1: the dollar was at 116. 92 yen against the yen, flat on the session, and at 1. 2891 against the swiss franc, also flat. the dollar was at 116. 78 yen jpy =, virtually flat on the session, and at 1. 2871 against the swiss franc chf =, down 0. 1 percent.\n",
      "Sentence 2: the dollar was at 116. 92 yen against the yen, flat on the session, and at 1. 2891 against the swiss franc, also flat. the dollar was at 116. 78 yen jpy =, virtually flat on the session, and at 1. 2871 against the swiss franc chf =, down 0. 1 percent.\n",
      "Is sentence2 a paraphrase of sentence1? Yes\n",
      "Probabilities: tensor([0.2043, 0.7957], device='cuda:0')\n",
      "\n",
      "Sentence 1: the afl - cio is waiting until october to decide if it will endorse a candidate. the afl - cio announced wednesday that it will decide in october whether to endorse a candidate before the primaries.\n",
      "Sentence 2: the afl - cio is waiting until october to decide if it will endorse a candidate. the afl - cio announced wednesday that it will decide in october whether to endorse a candidate before the primaries.\n",
      "Is sentence2 a paraphrase of sentence1? Yes\n",
      "Probabilities: tensor([0.1967, 0.8033], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Get a single batch from the dataloader\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                   token_type_ids=batch[\"token_type_ids\"],\n",
    "                   attention_mask=batch[\"attention_mask\"])\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "predictions = torch.argmax(probs, dim=-1)\n",
    "\n",
    "for i, (prob, pred) in enumerate(zip(probs, predictions)):\n",
    "    paraphrase = \"Yes\" if pred == 1 else \"No\"\n",
    "    print(f\"Sentence 1: {tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)}\")\n",
    "    print(f\"Sentence 2: {tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True)}\")\n",
    "    print(f\"Is sentence2 a paraphrase of sentence1? {paraphrase}\")\n",
    "    print(f\"Probabilities: {prob}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6b233-9cbf-427c-a267-cd1d99aeac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course solution:\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        print(len(batch))\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            token_type_ids=batch[\"token_type_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch['labels'],\n",
    "        )\n",
    "        # if your batch object has all the right fields: ouputs = model(**batch)\n",
    "        print(outputs.keys())\n",
    "        print(outputs.loss)\n",
    "        print(outputs.logits)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901d575-f7f7-4024-9edf-976ea0b60e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course solution:\n",
    "from torch.nn import functional as F\n",
    "\n",
    "probs = F.softmax(outputs.logits, dim=-1)\n",
    "print(probs)\n",
    "for sent1, sent2, label, probs in zip(dataset['sentence1'], dataset['sentence2'], dataset['labels'], probs):\n",
    "    #incomplete atm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8dcea",
   "metadata": {},
   "source": [
    "**Question:** Load the model again (execute the cell just below the [Model](#Model) section), run the forward pass and your evaluation. Why do you get different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227748e1",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "This is probably due to the initialization of the Model. Loading the model again reinitiates it with random weights as we are not loading a model with fine-tuned weights. Or because of the batch loader generating a different batch each time. I'm not 100 percent certain.\n",
    "- it will randomly initialize the classifier, so the output will be random. set torch.manual_seed(...) to avoid this random init, with it it will initialize the same every time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f1ff9",
   "metadata": {},
   "source": [
    "## Evaluate a trained model\n",
    "We now download a different model instead: `textattack/bert-base-uncased-QQP`. This is a model trained to detect duplicate questions on Quora, so basically our paraphrase detection task, but trained on a different dataset. Let's see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51a6bf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"textattack/bert-base-uncased-QQP\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-QQP\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa6ed301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: he said the foodservice pie business doesn't fit the company's long - term growth strategy. \" the foodservice pie business does not fit our long - term growth strategy.\n",
      "Sentence 2: he said the foodservice pie business doesn't fit the company's long - term growth strategy. \" the foodservice pie business does not fit our long - term growth strategy.\n",
      "Is sentence2 a paraphrase of sentence1? Yes\n",
      "Probabilities: tensor([0.0240, 0.9760], device='cuda:0')\n",
      "\n",
      "Sentence 1: magnarelli said racicot hated the iraqi regime and looked forward to using his long years of training in the war. his wife said he was \" 100 percent behind george bush \" and looked forward to using his years of training in the war.\n",
      "Sentence 2: magnarelli said racicot hated the iraqi regime and looked forward to using his long years of training in the war. his wife said he was \" 100 percent behind george bush \" and looked forward to using his years of training in the war.\n",
      "Is sentence2 a paraphrase of sentence1? No\n",
      "Probabilities: tensor([0.9547, 0.0453], device='cuda:0')\n",
      "\n",
      "Sentence 1: the dollar was at 116. 92 yen against the yen, flat on the session, and at 1. 2891 against the swiss franc, also flat. the dollar was at 116. 78 yen jpy =, virtually flat on the session, and at 1. 2871 against the swiss franc chf =, down 0. 1 percent.\n",
      "Sentence 2: the dollar was at 116. 92 yen against the yen, flat on the session, and at 1. 2891 against the swiss franc, also flat. the dollar was at 116. 78 yen jpy =, virtually flat on the session, and at 1. 2871 against the swiss franc chf =, down 0. 1 percent.\n",
      "Is sentence2 a paraphrase of sentence1? No\n",
      "Probabilities: tensor([0.9861, 0.0139], device='cuda:0')\n",
      "\n",
      "Sentence 1: the afl - cio is waiting until october to decide if it will endorse a candidate. the afl - cio announced wednesday that it will decide in october whether to endorse a candidate before the primaries.\n",
      "Sentence 2: the afl - cio is waiting until october to decide if it will endorse a candidate. the afl - cio announced wednesday that it will decide in october whether to endorse a candidate before the primaries.\n",
      "Is sentence2 a paraphrase of sentence1? Yes\n",
      "Probabilities: tensor([0.1669, 0.8331], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                    token_type_ids=batch[\"token_type_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"])\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "predictions = torch.argmax(probs, dim=-1)\n",
    "\n",
    "for i, (prob, pred) in enumerate(zip(probs, predictions)):\n",
    "    paraphrase = \"Yes\" if pred == 1 else \"No\"\n",
    "    sentence1 = tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True)\n",
    "    sentence2 = tokenizer.decode(batch[\"input_ids\"][i], skip_special_tokens=True)\n",
    "    print(f\"Sentence 1: {sentence1}\")\n",
    "    print(f\"Sentence 2: {sentence2}\")\n",
    "    print(f\"Is sentence2 a paraphrase of sentence1? {paraphrase}\")\n",
    "    print(f\"Probabilities: {prob}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b95a0-8d19-4018-8ff0-6c66b59efe72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
