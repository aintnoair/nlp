{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e265922",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "In this notebook, we'll get to know the Hugging Face ecosystem by loading a dataset, encoding the input data, running a model, and evaluating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q datasets ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbbaad",
   "metadata": {},
   "source": [
    "Take a look at the [Hugging Face datasets hub](https://huggingface.co/datasets). Find the MRPC (Microsoft Research Paraphrase Corpus) dataset that is part of the GLUE (General Language Understanding Evaluation) benchmark. Download the validation split of the dataset with dataset's `load_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e7824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24c4f191",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "With Transformers (we will get to know them in more detail later in the course), tokenization has become part of the model itself. We first install Hugging Face's transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4933fc",
   "metadata": {},
   "source": [
    "Use the [model page of the base-uncased version of BERT](https://huggingface.co/bert-base-uncased) to initialize a `BertTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd20df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de2ccab3",
   "metadata": {},
   "source": [
    "Encode the first sentence of the first example in the dataset. Look at the outputs of the following functions:\n",
    "- `tokenizer(sentence)`\n",
    "- `tokenizer.encode(sentence)`\n",
    "- `tokenizer.tokenize(sentence)`\n",
    "- `tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c8a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c44f21a",
   "metadata": {},
   "source": [
    "**Decoding.** Check out the various ways of decoding: `.decode`, `.convert_ids_to_tokens`, `.convert_tokens_to_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2d203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "748080cf",
   "metadata": {},
   "source": [
    "Use the NLP section of the [quickstart guide](https://huggingface.co/docs/datasets/quickstart) to apply encoding to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c07753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc640318",
   "metadata": {},
   "source": [
    "We have to rename the \"label\" column to \"labels\" to match the expected name in BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751be0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c9bd126",
   "metadata": {},
   "source": [
    "- Use the guide again to set the data format to \"torch\". Make sure the columns `input_ids`, `token_type_ids`, `attention_mask` and `labels` are present.\n",
    "- Create a data loader with a `batch_size` of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eda65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1c6e2bc",
   "metadata": {},
   "source": [
    "## Model\n",
    "We now load a pretrained BERT model and perform sequence classification on the MRPC dataset. Load the `BertForSequenceClassification` model. Set the model to evaluation mode by calling `.eval()` on the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955542a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ead6fcf",
   "metadata": {},
   "source": [
    "In the evaluation state, no gradient information will be saved in the forward pass, and no dropout will be applied (and the values rescaled to match the training's output distribution). We can always set it back to train mode with `.train()`.\n",
    "\n",
    "Additionally, we should call the model in a `torch.no_grad()` context, which sets all the tensors' `.requires_grad` fields to False.\n",
    "\n",
    "## Forward pass\n",
    "Now we run the model on a single batch. Get a batch from the dataloader, pass it to the model's forward function. It is preferred to use `model(.)` to do this instead of `model.forward(.)`. Some hooks may not be run if you use the latter version, as mentioned in this [PyTorch forum question](https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690).\n",
    "\n",
    "- Run a single batch through the model.\n",
    "- Get the output logits\n",
    "- Run a softmax function on it (use `torch.nn.functional.softmax`) to get output probabilities\n",
    "- Display the result (i.e. is sentence2 a paraphrase of sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c562f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40a8dcea",
   "metadata": {},
   "source": [
    "**Question:** Load the model again (execute the cell just below the [Model](#Model) section), run the forward pass and your evaluation. Why do you get different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227748e1",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f1ff9",
   "metadata": {},
   "source": [
    "## Evaluate a trained model\n",
    "We now download a different model instead: `textattack/bert-base-uncased-QQP`. This is a model trained to detect duplicate questions on Quora, so basically our paraphrase detection task, but trained on a different dataset. Let's see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"textattack/bert-base-uncased-QQP\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-QQP\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ed301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
