{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks for BoolQ Reading Comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e72b2ad5a819003"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "- **Objective**: Develop a reading comprehension model using a 2-layer LSTM and a 2-layer classifier. The model will be trained end-to-end on the BoolQ dataset.\n",
    "- **Task**: The BoolQ dataset involves answering yes/no questions given a passage. The goal is to predict the correct label for each question.\n",
    "- **Approach**: Utilize PyTorch for building the model, and Hugging Face's datasets library to manage data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca4c37d307d33a05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Foreword...\n",
    "I unfortunately ran into many stupid problems during this project. For example not being able to get fastText installed and working on my Desktop or GPUHub and having to run this model on my laptop... The model only predicting one class constantly... and many more. More details with the respective code below:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b70dacf984131815"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 2. Setup\n",
    "- **Libraries**: \n",
    "  - `torch`: For building the neural network.\n",
    "  - `datasets`: For loading the BoolQ dataset.\n",
    "  - `nltk`: For word tokenization.\n",
    "  - `fasttext`: To load and use FastText embeddings.\n",
    "  - `numpy`, `matplotlib`, `seaborn`: For data manipulation and visualization.\n",
    "  - `optuna`: For hyperparameter tuning.\n",
    "  - `sklearn`: For metrics.\n",
    "  - `wandb`: For experiment tracking\n",
    "\n",
    "- **Planned Correctness Tests**:\n",
    "  - Use `assert` statements to check tensor dimensions, and confirm the expected shapes of inputs and outputs throughout the data pipeline.\n",
    "  - Print sample outputs at different stages to validate transformations.\n",
    "\n",
    "- **Experiment Tracking**:\n",
    "  - Use `wandb` for logging experiments, including hyperparameters, metrics, and visualizations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50258e8b99db9e90"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.18.2)\r\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.7)\r\n",
      "Requirement already satisfied: fasttext in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.9.2)\r\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.3)\r\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.11.2)\r\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.5.3)\r\n",
      "Requirement already satisfied: optuna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.0.0)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (4.2.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (5.26.1)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (5.9.5)\r\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (6.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (2.14.0)\r\n",
      "Requirement already satisfied: setproctitle in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (69.2.0)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.13.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (1.4.4)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.66.5)\r\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.10.7)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.25.1)\r\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (23.1)\r\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: pybind11>=2.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fasttext) (2.12.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2.8.8)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.40.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (1.13.3)\r\n",
      "Requirement already satisfied: colorlog in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (6.8.2)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from optuna) (2.0.35)\r\n",
      "Requirement already satisfied: Mako in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.13.1)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# TODO: make the pip install for used libraries and packages !!!\n",
    "%pip install wandb datasets nltk fasttext numpy torch scikit-learn seaborn matplotlib optuna"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:32.898523Z",
     "start_time": "2024-10-21T21:27:32.174478Z"
    }
   },
   "id": "a977d9e2d10e0d78"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import nltk\n",
    "import fasttext\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:36.585017Z",
     "start_time": "2024-10-21T21:27:34.775516Z"
    }
   },
   "id": "f8484a2a506d95aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the hyperparameters (keeping it on top contrary to what I said in my last project)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5de3ffd1cb28f735"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "max_seq_len = 512               # You already have this defined\n",
    "embedding_dim: int = 300        # fastText model embedding dimension\n",
    "padding_type: str = 'zeros'     # Choose between padding w/ 'zeros' or the 'avg' of the embedding\n",
    "\n",
    "optimizer_choice: str = 'Adam'  # Optimizer ['Adam', 'AdamW', 'SGD']\n",
    "\n",
    "hidden_dim: int  = 128          # Hidden size for LSTM\n",
    "output_dim: int = 1             # Binary classification (yes/no)\n",
    "n_layers: int  = 2              # Two LSTM layers\n",
    "dropout_rate: float = 0.0       # Dropout rate for regularization\n",
    "learning_rate: float = 1e-3     # Optimizer learning rate\n",
    "weight_decay: float = 0.00      # For AdamW or L2 in SGD\n",
    "train_batch_size: int = 32      # Training Batch size\n",
    "val_batch_size: int = 32        # Validation and Testing Batch size\n",
    "n_epochs: int  = 10             # Number of epochs\n",
    "patience: int = 3               # Early stopping patience"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:37.263993Z",
     "start_time": "2024-10-21T21:27:37.259483Z"
    }
   },
   "id": "db9bc34f53a5c58d"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:awsmhtg2) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.614 MB of 0.614 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc353439dd9d4ec98bd413bc84c1c3ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">run_2-optuna-trial-0</strong> at: <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/awsmhtg2' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/awsmhtg2</a><br/> View project at: <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20241021_223751-awsmhtg2/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:awsmhtg2). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/blackbook/Desktop/dev-5-sem/nlp/project-2/wandb/run-20241021_230151-bw8fxnkw</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/bw8fxnkw' target=\"_blank\">run_1-default-run</a></strong> to <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/bw8fxnkw' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/bw8fxnkw</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/bw8fxnkw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x522078150>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_number: int = 1 # TODO: Don't forget to change this!!!\n",
    "\n",
    "# defining the WandB project and run name\n",
    "project_name: str = 'nlp-rnn_lstm_pt'\n",
    "run_name: str = f\"run_{run_number}-default-run\"\n",
    "\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"max_seq_len\": max_seq_len,\n",
    "        \"padding_type\": padding_type,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"val_batch_size\": val_batch_size,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"patience\": patience,\n",
    "        \"optimizer_choice\": optimizer_choice\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:01:54.782085Z",
     "start_time": "2024-10-21T21:01:51.493938Z"
    }
   },
   "id": "b5285055537a313d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading the required BoolQ dataset and splitting it like required from the project presentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3890b48a2aee852"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:49.069510Z",
     "start_time": "2024-10-21T21:27:43.450116Z"
    }
   },
   "id": "b2bdeff042b31f82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the data, labels and distributions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90eef930ca6a750"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you use oyster card at epsom station', 'answer': False, 'passage': \"Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\"}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n",
      "Train set - Yes: 5279, No: 3148, Ratio (y/n): 1.68, Percent Yes: 62.64%\n",
      "Validation set - Yes: 595, No: 405, Ratio (y/n): 1.47, Percent Yes: 59.5%\n",
      "Test set - Yes: 2033, No: 1237, Ratio (y/n): 1.64, Percent Yes: 62.17%\n"
     ]
    }
   ],
   "source": [
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")\n",
    "\n",
    "train_yes_count = sum(1 for label in train_data['answer'] if label == 1)\n",
    "train_no_count = sum(1 for label in train_data['answer'] if label == 0)\n",
    "train_total = train_yes_count + train_no_count\n",
    "\n",
    "validation_yes_count = sum(1 for label in validation_data['answer'] if label == 1)\n",
    "validation_no_count = sum(1 for label in validation_data['answer'] if label == 0)\n",
    "validation_total = validation_yes_count + validation_no_count\n",
    "\n",
    "test_yes_count = sum(1 for label in test_data['answer'] if label == 1)\n",
    "test_no_count = sum(1 for label in test_data['answer'] if label == 0)\n",
    "test_total = test_yes_count + test_no_count\n",
    "\n",
    "print(f\"Train set - Yes: {train_yes_count}, No: {train_no_count}, Ratio (y/n): {round(train_yes_count / train_no_count, 2)}, Percent Yes: {round(train_yes_count / train_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Validation set - Yes: {validation_yes_count}, No: {validation_no_count}, Ratio (y/n): {round(validation_yes_count / validation_no_count, 2)}, Percent Yes: {round(validation_yes_count / validation_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Test set - Yes: {test_yes_count}, No: {test_no_count}, Ratio (y/n): {round(test_yes_count / test_no_count, 2)}, Percent Yes: {round(test_yes_count / test_total * 100, 2)}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:49.080142Z",
     "start_time": "2024-10-21T21:27:49.078579Z"
    }
   },
   "id": "619cd85df4224366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the fastText model if not already in directory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a913a3df2c404597"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_bin = Path('cc.en.300.bin')\n",
    "\n",
    "if not model_bin.exists():\n",
    "    fasttext.util.download_model('en', if_exists='ignore') # download if not already in dir\n",
    "\n",
    "ft = fasttext.load_model(str(model_bin))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.644438Z",
     "start_time": "2024-10-21T21:27:49.080993Z"
    }
   },
   "id": "248ed6e5746879c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure the nltk tokenizer is downloaded"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61c2334d73b996db"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/blackbook/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.771787Z",
     "start_time": "2024-10-21T21:27:54.626224Z"
    }
   },
   "id": "2b184b9bbc120d0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "- **Text Cleaning**:\n",
    "  - **Operations**:\n",
    "    - Convert text to lowercase for consistency.\n",
    "    - Remove special characters and URLs while keeping necessary hyphens.\n",
    "    - Remove extra whitespace between words as well as before or after a sequence.\n",
    "  - **Reasoning**: These basic cleaning steps standardize the input without over-complicating the preprocessing and removing as little sentiment as possible from the sentences. I chose to not remove stopwords and not do stemming or lemmatizing for the same reason.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8f7871108c5ac1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.773981Z",
     "start_time": "2024-10-21T21:27:54.770806Z"
    }
   },
   "id": "37e5ff2bfa672e78"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "epsom railway station serves the town of epsom in surrey. it is located off waterloo road and is less than two minutes' walk from the high street. it is not in the london oyster card zone unlike epsom downs or tattenham corner stations. the station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(text: str) -> str:\n",
    "    lowered_text = text.lower()\n",
    "    assert lowered_text.islower(), \"Text is not fully lowercase\"\n",
    "    return lowered_text\n",
    "\n",
    "print(to_lowercase(test_question))\n",
    "print(to_lowercase(test_passage))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.777562Z",
     "start_time": "2024-10-21T21:27:54.775217Z"
    }
   },
   "id": "f22d4b60adff0a22"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "Epsom railway station serves the town of Epsom in Surrey It is located off Waterloo Road and is less than two minutes' walk from the High Street It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations The station building was replaced in 2012 2013 with a new building with apartments above the station see end of article\n",
      "Visit us at  for more info\n",
      "Some RANDOM text With VARIETY Check this out  and the year is 2012 2013\n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters_and_urls(text: str) -> str:\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "    # Replace slashes with spaces first\n",
    "    text = text.replace('/', ' ')\n",
    "    # Remove special characters except for alphanumeric characters and spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s\\'\\-]', '', text)\n",
    "    assert not re.search(r'http[s]?://|www\\.', cleaned_text), \"URLs were not fully removed\"\n",
    "    return cleaned_text\n",
    "\n",
    "test_question_w_url = \"Visit us at https://example.com for more info!\"\n",
    "test_passage_w_url = \"Some RANDOM text With VARIETY. Check this out: www.example.org and the year is 2012/2013.\"\n",
    "\n",
    "print(remove_special_characters_and_urls(test_question))\n",
    "print(remove_special_characters_and_urls(test_passage))\n",
    "print(remove_special_characters_and_urls(test_question_w_url))\n",
    "print(remove_special_characters_and_urls(test_passage_w_url))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.780993Z",
     "start_time": "2024-10-21T21:27:54.778799Z"
    }
   },
   "id": "c870a2e4ebb98354"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\n"
     ]
    }
   ],
   "source": [
    "def remove_extra_whitespace(text: str) -> str:\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    assert '  ' not in cleaned_text, \"There are still multiple spaces\"\n",
    "    return cleaned_text\n",
    "\n",
    "print(remove_extra_whitespace(test_question))\n",
    "print(remove_extra_whitespace(test_passage))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.793520Z",
     "start_time": "2024-10-21T21:27:54.781334Z"
    }
   },
   "id": "a1ef7c329467e226"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Initial Plan:*\n",
    "- ***Tokenization**:*\n",
    "  - ***Decision**: Use a pre-trained Byte-Pair Encoding (BPE) tokenizer from the `transformers` library.*\n",
    "  - ***Reasoning**:*\n",
    "    - *Using a pre-trained tokenizer simplifies the preprocessing pipeline, as the tokenizer has already been trained on a large and diverse corpus, which increases its generalization capability.*\n",
    "    - *Pre-trained tokenizers from `transformers` are well-optimized and widely used in various NLP tasks.*\n",
    "    - *BPE helps handle out-of-vocabulary (OOV) words by breaking them into known subword units, allowing for more robust word representations.*\n",
    "\n",
    "Adjusted plan after feedback on project 1 as well as stage 1 of project 2:\n",
    "- **Tokenization**\n",
    "  - **Decision**: Use NLTK's word-level tokenizer.\n",
    "  - **Reasoning**:\n",
    "    - Ensures compatibility with FastText, which expects whole words.\n",
    "    - Reduces complexity by relying on FastText's internal OOV handling.\n",
    "    - Aligns with the word-based tokenization used in FastText’s training.\n",
    "- *Note*: I finally got what the TA meant with compatability between the tokenizer and fastText... I needed word-level tokens for the fastText to generate embeddings from, and it will automatically generate embeddings for OOV words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad0822e95495d979"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    assert isinstance(tokens, list) and len(tokens) > 0, \"Tokenization failed or empty token list\"\n",
    "    return tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.793693Z",
     "start_time": "2024-10-21T21:27:54.783822Z"
    }
   },
   "id": "7ba8ca92c7221d6c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epsom', 'railway', 'station', 'serves', 'the', 'town', 'of', 'epsom', 'in', 'surrey', 'it', 'is', 'located', 'off', 'waterloo', 'road', 'and', 'is', 'less', 'than', 'two', 'minutes', \"'\", 'walk', 'from', 'the', 'high', 'street', 'it', 'is', 'not', 'in', 'the', 'london', 'oyster', 'card', 'zone', 'unlike', 'epsom', 'downs', 'or', 'tattenham', 'corner', 'stations', 'the', 'station', 'building', 'was', 'replaced', 'in', '2012', '2013', 'with', 'a', 'new', 'building', 'with', 'apartments', 'above', 'the', 'station', 'see', 'end', 'of', 'article']\n",
      "['can', 'you', 'use', 'oyster', 'card', 'at', 'epsom', 'station']\n"
     ]
    }
   ],
   "source": [
    "lower_passage = to_lowercase(test_passage)\n",
    "lower_question = to_lowercase(test_question)\n",
    "\n",
    "cleaned_passage = remove_extra_whitespace(remove_special_characters_and_urls(lower_passage))\n",
    "cleaned_question = remove_extra_whitespace(remove_special_characters_and_urls(lower_question))\n",
    "\n",
    "print(tokenize(cleaned_passage))\n",
    "print(tokenize(cleaned_question))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.839430Z",
     "start_time": "2024-10-21T21:27:54.787685Z"
    }
   },
   "id": "732c45f0593dfcc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Word Embedding Lookups**:\n",
    "  - **Decision**: Use the FastText API directly to obtain embeddings for tokenized words.\n",
    "  - **Reasoning**:\n",
    "    - The FastText API considers subword information when generating word embeddings, providing robust handling of OOV words.\n",
    "    - This approach prevents the issue of having to map subword tokens directly to embeddings, which is not feasible with traditional embedding lookup methods.\n",
    "  - **OOV Word Handling**:\n",
    "    - Rely on FastText's built-in subword handling to generate embeddings for unknown words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db08990dadde1090"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_fasttext_embeddings(tokens: list) -> np.ndarray:\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embedding = ft.get_word_vector(token)\n",
    "        assert embedding.shape == (embedding_dim,), f\"Embedding shape mismatch for token: {token}\"\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.840683Z",
     "start_time": "2024-10-21T21:27:54.795295Z"
    }
   },
   "id": "b3d5913344feda42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Sequence Truncation and Padding**:\n",
    "  - **Truncating**: Truncate sequences to a fixed length of 512 tokens.\n",
    "  - **Padding**: Apply padding to make all sequences in a batch have the same length.\n",
    "  - **Reasoning**:\n",
    "    - Limiting the sequence length to 512 tokens balances computational efficiency and context retention. This choice ensures that the input size remains manageable while still covering most of the content in the passages. It is also a popular sequence length for nlp applications, that's why I chose it. I will play with the maximum sequence length for tuning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7a5b021979f9e6b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def pad_or_truncate(sequence: np.ndarray, max_length: int = max_seq_len, padding_type: str = 'zeros') -> np.ndarray:\n",
    "    current_length = len(sequence)\n",
    "    \n",
    "    if current_length > max_length:\n",
    "        return sequence[:max_length]\n",
    "    \n",
    "    elif current_length < max_length:\n",
    "        if padding_type == 'zeros':\n",
    "            padding = np.zeros((max_length - current_length, sequence.shape[1]))\n",
    "        elif padding_type == 'avg':\n",
    "            avg_embedding = np.mean(sequence, axis=0)\n",
    "            padding = np.tile(avg_embedding, (max_length - current_length, 1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid padding_type. Use 'zeros' or 'avg'.\")\n",
    "        \n",
    "        padded_sequence = np.vstack((sequence, padding))\n",
    "        assert padded_sequence.shape == (max_length, sequence.shape[1]), \"Padding failed\"\n",
    "        return padded_sequence\n",
    "    \n",
    "    else:\n",
    "        return sequence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.840849Z",
     "start_time": "2024-10-21T21:27:54.798991Z"
    }
   },
   "id": "5fdacfb5795ee089"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine all preprocessing steps into a single preprocessing pipeline for easy data preparation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5afde4001fe58359"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text: str) -> np.ndarray:\n",
    "    # Text cleaning:\n",
    "    lowercase_text = to_lowercase(text)\n",
    "    cleaned_text = remove_special_characters_and_urls(lowercase_text)\n",
    "    prepared_text = remove_extra_whitespace(cleaned_text)\n",
    "    \n",
    "    # Tokenization:\n",
    "    tokens = tokenize(prepared_text)\n",
    "    \n",
    "    # Embeddings:\n",
    "    embeddings = get_fasttext_embeddings(tokens)\n",
    "    \n",
    "    # Pad or truncate:\n",
    "    padded_embeddings = pad_or_truncate(embeddings)\n",
    "    \n",
    "    # Check final output shape\n",
    "    assert padded_embeddings.shape == (max_seq_len, embedding_dim), f\"Final embedding shape is {padded_embeddings.shape}, expected ({max_seq_len}, {embedding_dim})\"\n",
    "    \n",
    "    return padded_embeddings\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:54.840890Z",
     "start_time": "2024-10-21T21:27:54.801820Z"
    }
   },
   "id": "60b400b89c7c9b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print some test results to see if the preprocessing worked like expected."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebdb983ddd3382d5"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: can you use oyster card at epsom station\n",
      "Displaying first 5 tokens and their embeddings:\n",
      "Token: can - Embedding: [ 0.04433588  0.09070976  0.05368941  0.18836261 -0.1909022 ]...\n",
      "Token: you - Embedding: [ 0.10789153 -0.04412316  0.13406183  0.0902128  -0.15022287]...\n",
      "Token: use - Embedding: [-0.0271657   0.06062786 -0.06484954  0.04023458  0.0097399 ]...\n",
      "Token: oyster - Embedding: [ 0.02858743 -0.06040148  0.00882877  0.10480718  0.07261764]...\n",
      "Token: card - Embedding: [0.07530363 0.10068872 0.0854513  0.08289765 0.00129725]...\n"
     ]
    }
   ],
   "source": [
    "# Print some sample embeddings for inspection\n",
    "def print_sample_embeddings(tokens: list, embeddings: np.ndarray, num_samples: int = 5):\n",
    "    print(f\"Displaying first {num_samples} tokens and their embeddings:\")\n",
    "    for i in range(min(num_samples, len(tokens))):\n",
    "        print(f\"Token: {tokens[i]} - Embedding: {embeddings[i][:5]}...\")  # Show the first 5 dimensions of each embedding for brevity\n",
    "\n",
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "test_question = train_data[5]['question']\n",
    "\n",
    "print(f\"Original Question: {test_question}\")\n",
    "processed_embeddings = preprocessing_pipeline(test_question)\n",
    "tokens = tokenize(remove_extra_whitespace(remove_special_characters_and_urls(to_lowercase(test_question))))\n",
    "print_sample_embeddings(tokens, processed_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:56.808784Z",
     "start_time": "2024-10-21T21:27:54.805639Z"
    }
   },
   "id": "bdd334bfc49b3586"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Log confusion matrix\n",
    "def log_confusion_matrix(labels, preds):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix on Validation set')\n",
    "    \n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:56.809030Z",
     "start_time": "2024-10-21T21:27:56.803407Z"
    }
   },
   "id": "8848ec28078fd33c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Input Preparation**:\n",
    "  - Each input is a concatenation of the question and passage of total length 1024 (512 * 2).\n",
    "  - The resulting input will have the shape of `(batch_size, max_sequence_length * 2, embedding_dim)`— for example, `(32, 1024, 300)` for a batch size of 32.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1e2c73a5ba337b3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc970d736147822"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, max_seq_length: int =512) -> None:\n",
    "        self.data = data\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.tensor:\n",
    "        # Retrieve passage and question\n",
    "        passage = self.data[idx]['passage']\n",
    "        question = self.data[idx]['question']\n",
    "        \n",
    "        # Preprocess both passage and question using the preprocessing pipeline\n",
    "        passage_embeddings = preprocessing_pipeline(passage)\n",
    "        question_embeddings = preprocessing_pipeline(question)\n",
    "        \n",
    "        # Concatenate passage and question embeddings (passage first, then question)\n",
    "        combined_embeddings = np.concatenate((passage_embeddings, question_embeddings), axis=0)\n",
    "        \n",
    "        # Convert the label to tensor (1 for 'yes', 0 for 'no')\n",
    "        label = torch.tensor([1 if self.data[idx]['answer'] else 0], dtype=torch.long)  # Shape (1,)\n",
    "        \n",
    "        output = output = torch.tensor(combined_embeddings, dtype=torch.float32), label\n",
    "        \n",
    "        # Return the combined embeddings and the label as tensors\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:56.809077Z",
     "start_time": "2024-10-21T21:27:56.803690Z"
    }
   },
   "id": "41fc6be83e8eb578"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c3f9629aee75e3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_dataset = BoolQDataset(train_data)\n",
    "validation_dataset = BoolQDataset(validation_data)\n",
    "test_dataset = BoolQDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:57.652119Z",
     "start_time": "2024-10-21T21:27:57.649354Z"
    }
   },
   "id": "8d73e9f1e3c60f9e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Shape: torch.Size([32, 1024, 300])\n",
      "Labels Shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Labels Shape: {labels.shape}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:27:58.161005Z",
     "start_time": "2024-10-21T21:27:58.056036Z"
    }
   },
   "id": "d6c1103da2b9a46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Architecture\n",
    "- **RNN Type**:\n",
    "  - **Decision**: Use LSTM for the RNN layers.\n",
    "  - **Reasoning**: LSTM cells help maintain long-term dependencies through gating mechanisms, which is beneficial for reading comprehension tasks where context from the entire passage can be important for answering questions.\n",
    "\n",
    "- **Model Configuration**:\n",
    "  - **Embedding Layer**: Input dimension of 300 using FastText embeddings.\n",
    "  - **RNN Layers**: Two LSTM layers with a hidden size of 128.\n",
    "  - **Dropout**: Apply dropout with a rate of 0.3 between the LSTM layers for regularization.\n",
    "  - **Classifier**: A two-layer fully connected network (hidden layer of size 64) with ReLU activation.\n",
    "\n",
    "- **Loss and Optimizer**:\n",
    "  - **Loss Function**: Use Binary Cross-Entropy Loss for the binary classification task.\n",
    "  - **Optimizer**: Use the Adam optimizer with an initial learning rate of 0.001.\n",
    "  - **Reasoning**:\n",
    "    - Adam is chosen for its adaptive learning rate, which can improve training stability and convergence.\n",
    "\n",
    "- **Regularization**:\n",
    "  - **Dropout**: Applied to reduce overfitting.\n",
    "  - **Early Stopping**: Monitor validation loss and stop training if it does not improve for 3 consecutive epochs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1e7a81ace8a8a0"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Optimizer selection function\n",
    "def get_optimizer(optimizer_choice, model, learning_rate, weight_decay=0):\n",
    "    if optimizer_choice == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_choice == 'AdamW':\n",
    "        return optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_choice == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_choice}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:28:01.575640Z",
     "start_time": "2024-10-21T21:28:01.562424Z"
    }
   },
   "id": "e36558b41efce54d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        last_hidden_state = hn[-1]\n",
    "        x = self.relu(self.fc1(last_hidden_state))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:28:02.213576Z",
     "start_time": "2024-10-21T21:28:02.196859Z"
    }
   },
   "id": "3302d00301393ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = LSTMModel(embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate)\n",
    "\n",
    "# Get optimizer based on choice\n",
    "optimizer = get_optimizer(optimizer_choice, model, learning_rate, weight_decay)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:28:03.509598Z",
     "start_time": "2024-10-21T21:28:03.293406Z"
    }
   },
   "id": "376c3d1cefd935b1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: cpu, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Move model and criterion to the correct device (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Type: {device.type}, Device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:28:03.582748Z",
     "start_time": "2024-10-21T21:28:03.574407Z"
    }
   },
   "id": "b4c453214917920e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training\n",
    "- **Number of Epochs**: Train for up to 20 epochs with early stopping.\n",
    "- **Checkpointing**: Save the model with the best validation accuracy to avoid overfitting.\n",
    "\n",
    "- **Hyperparameter Experimentation**:\n",
    "  - **Learning Rate**: Test various learning rates (e.g., 0.001, 0.0005, 0.0001) to find an optimal balance between convergence speed and training stability.\n",
    "  - **Batch Size**: Experiment with different batch sizes (e.g., 16, 32, 64) to optimize memory usage and training time.\n",
    "  - **Dropout Rate**: Adjust dropout rates (e.g., 0.2, 0.3, 0.5) to find the optimal level of regularization.\n",
    "  - **Hidden Layer Size**: Try varying the number of hidden units in the RNN and classifier layers (e.g., 64, 128, 256) to assess their impact on model capacity.\n",
    "\n",
    "- *Note*:\n",
    "    - I did not manage to test these hyperparameters manually like I originally planed for. I used optuna to do hyperparameter tuning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affaf388d1f82eb4"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, n_epochs, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            labels = labels.squeeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend((outputs.cpu().detach().numpy() > 0.5).astype(int))\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_labels = []\n",
    "        val_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                labels = labels.squeeze(1)\n",
    "                \n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend((outputs.cpu().detach().numpy() > 0.5).astype(int))\n",
    "\n",
    "        avg_val_loss = val_loss / len(validation_loader)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_precision = precision_score(val_labels, val_preds)\n",
    "        val_recall = recall_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds)\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"val_f1\": val_f1\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Log confusion matrix without step parameter\n",
    "        log_confusion_matrix(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Log final metrics at the end of training\n",
    "    wandb.log({\"best_val_loss\": best_val_loss, \"final_val_accuracy\": val_acc, \"final_val_f1\": val_f1})\n",
    "    \n",
    "    # Finish WandB run\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:28:06.101349Z",
     "start_time": "2024-10-21T21:28:06.089821Z"
    }
   },
   "id": "ac7c15be85ddf7a1"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[26], line 20\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, validation_loader, n_epochs, patience)\u001B[0m\n\u001B[1;32m     18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     19\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m---> 20\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     23\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, validation_loader, n_epochs, patience)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T21:28:12.697717Z",
     "start_time": "2024-10-21T21:28:07.138988Z"
    }
   },
   "id": "4ab3d778c324c1a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluation\n",
    "- **Primary Metric**:\n",
    "  - **Accuracy**: Chosen as the main evaluation metric since it reflects the overall model performance in binary classification.\n",
    "- **Baseline Comparison**:\n",
    "  - Compare the model's accuracy against a majority class baseline (e.g., always predicting \"yes\") to understand the model's relative performance.\n",
    "- **Error Analysis**:\n",
    "  - Analyze the confusion matrix to identify patterns in misclassifications and judge the types of errors the model makes.\n",
    "\n",
    "- From the confusion matrices we can clearly see the model only ever predicts the majority class. This is not good, but unfortunately I was not able to implement a model in time that did not have this bias. I chose to revert back to this older version of my code to have something that runs consistently for the submission.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f3eba8bc7297f7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Interpretation\n",
    "- **Performance Expectations**:\n",
    "  - Learning from the results of Project 1 I am setting my expectations a bit lower (more realistic) this time. I'm expecting the LSTM to achieve an accuracy of 65 - 70%. Hopefully beating the baseline of always predicting \"yes\" (accuracy of 61-63%)\n",
    "\n",
    "- The results of the model are disappointing to say the least. Unfortunately the model only ever predicts the majority class. I have tried many different things to stop this, but I could not get something working in time. This project broke me, it took me all weekend of trying things out just to end up with no working model other than the one in this notebook that predicts only one class.\n",
    "- The results are just a couple of runs predicting either the minority class in some cases with optuna runs but mainly the majority class. My best performance as such was a validation accuracy of 59.5% or the percent of \"yes\" labels in the validation set.\n",
    "- A couple of hours before deadline of this project I found a mistake in my input data format that I thought caused these one-class predictions, but even after correcting this mistake the model continues to predict one class.\n",
    "- [Link to WandB workspace](https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/workspace?nw=nwuseraintnoair)\n",
    "- [Link to WandB report](https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/reports/LSTM-Results--Vmlldzo5ODE2NDgy)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "936219fee6686c70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "- I struggled with this project alot, having many little issues with libraries and in the end the sum of those little problems cost me valuable time I needed to experiment with the model. I had issues in getting the model working in the first place and even after I struggled with trying different things to troubleshoot the single-class classification. I have spent all weekend trying to figure out why the model will only predict one class and tried many ways to fix it but none of them were successful. This cost me a lot of time I should have spent tuning hyperparameters and experimenting. With all the things that went wrong in this project I can still call it at least a mild success, I was able to fix the mistakes in my preprocessing compared to the first project, I found out why the tokenizer didn't work with fastText previously and I got optuna working to automatically find hyperparameters. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b88d3598126676f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "68944b8c0c0be3f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
