{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks for BoolQ Reading Comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e72b2ad5a819003"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "- **Objective**: Develop a reading comprehension model using a 2-layer LSTM and a 2-layer classifier. The model will be trained end-to-end on the BoolQ dataset.\n",
    "- **Task**: The BoolQ dataset involves answering yes/no questions given a passage. The goal is to predict the correct label for each question.\n",
    "- **Approach**: Utilize PyTorch for building the model, and Hugging Face's datasets library to manage data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca4c37d307d33a05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setup\n",
    "- **Libraries**: \n",
    "  - `torch`: For building the neural network.\n",
    "  - `datasets`: For loading the BoolQ dataset.\n",
    "  - `transformers`: For using a pre-trained BPE tokenizer.\n",
    "  - `fasttext`: To load and use FastText embeddings.\n",
    "  - `numpy`, `pandas`, `matplotlib`, `seaborn`: For data manipulation and visualization.\n",
    "  - `gensim`: For loading the pre-trained word embedding model.\n",
    "  - `sklearn`: For metrics.\n",
    "  - `wandb`: For experiment tracking\n",
    "\n",
    "- **Planned Correctness Tests**:\n",
    "  - Use `assert` statements to check tensor dimensions, and confirm the expected shapes of inputs and outputs throughout the data pipeline.\n",
    "  - Print sample outputs at different stages to validate transformations.\n",
    "\n",
    "- **Experiment Tracking**:\n",
    "  - Use `wandb` for logging experiments, including hyperparameters, metrics, and visualizations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50258e8b99db9e90"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\r\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.35.2)\r\n",
      "Requirement already satisfied: fasttext in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.9.2)\r\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.3)\r\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.4.4)\r\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.5.3)\r\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.3.3)\r\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: wandb in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.18.2)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2.8.8)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (4.66.5)\r\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (3.10.7)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (0.25.1)\r\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2023.10.3)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.15.0)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.1)\r\n",
      "Requirement already satisfied: pybind11>=2.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fasttext) (2.12.0)\r\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fasttext) (69.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.40.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (3.1.0)\r\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (1.11.4)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gensim) (7.0.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (4.2.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (5.26.1)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (5.9.5)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (2.14.0)\r\n",
      "Requirement already satisfied: setproctitle in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.13.1)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\r\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch datasets transformers fasttext numpy pandas matplotlib gensim scikit-learn wandb "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:51:41.545618Z",
     "start_time": "2024-10-15T09:51:40.643542Z"
    }
   },
   "id": "a977d9e2d10e0d78"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "import fasttext.util\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:51:42.944501Z",
     "start_time": "2024-10-15T09:51:42.941770Z"
    }
   },
   "id": "f8484a2a506d95aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading the required BoolQ dataset and splitting it like required from the project presentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3890b48a2aee852"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:51:49.384770Z",
     "start_time": "2024-10-15T09:51:44.177036Z"
    }
   },
   "id": "b2bdeff042b31f82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the data and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90eef930ca6a750"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you use oyster card at epsom station', 'answer': False, 'passage': \"Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\"}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n",
      "Train set - Yes: 5279, No: 3148, Ratio (y/n): 1.68\n",
      "Validation set - Yes: 595, No: 405, Ratio (y/n): 1.47\n",
      "Test set - Yes: 2033, No: 1237, Ratio (y/n): 1.64\n"
     ]
    }
   ],
   "source": [
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")\n",
    "\n",
    "train_yes_count = sum(1 for label in train_data['answer'] if label == 1)\n",
    "train_no_count = sum(1 for label in train_data['answer'] if label == 0)\n",
    "\n",
    "validation_yes_count = sum(1 for label in validation_data['answer'] if label == 1)\n",
    "validation_no_count = sum(1 for label in validation_data['answer'] if label == 0)\n",
    "\n",
    "test_yes_count = sum(1 for label in test_data['answer'] if label == 1)\n",
    "test_no_count = sum(1 for label in test_data['answer'] if label == 0)\n",
    "\n",
    "# Print the counts and ratios\n",
    "print(f\"Train set - Yes: {train_yes_count}, No: {train_no_count}, Ratio (y/n): {round(train_yes_count / train_no_count, 2)}\")\n",
    "print(f\"Validation set - Yes: {validation_yes_count}, No: {validation_no_count}, Ratio (y/n): {round(validation_yes_count / validation_no_count, 2)}\")\n",
    "print(f\"Test set - Yes: {test_yes_count}, No: {test_no_count}, Ratio (y/n): {round(test_yes_count / test_no_count, 2)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:51:49.400546Z",
     "start_time": "2024-10-15T09:51:49.394578Z"
    }
   },
   "id": "619cd85df4224366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the model if not already in directory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a913a3df2c404597"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Define the file names\n",
    "model_bin = Path('cc.en.300.bin')\n",
    "model_gz = Path('cc.en.300.bin.gz')\n",
    "\n",
    "# Check if the model files already exist\n",
    "if not model_bin.exists() and not model_gz.exists():\n",
    "    fasttext.util.download_model('en', if_exists='ignore') # download it\n",
    "\n",
    "# Load the model\n",
    "ft = fasttext.load_model(str(model_bin))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:53:19.509476Z",
     "start_time": "2024-10-15T09:53:13.027033Z"
    }
   },
   "id": "248ed6e5746879c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "- **Handling Text Cleaning**:\n",
    "  - **Operations**:\n",
    "    - Convert text to lowercase for consistency.\n",
    "    - Remove special characters and extra whitespace.\n",
    "  - **Reasoning**: These basic cleaning steps standardize the input without over-complicating the preprocessing and removing as little sentiment as possible from the sentences. I chose to not remove stopwords and not do stemming or lemmatizing for the same reason.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8f7871108c5ac1"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "epsom railway station serves the town of epsom in surrey. it is located off waterloo road and is less than two minutes' walk from the high street. it is not in the london oyster card zone unlike epsom downs or tattenham corner stations. the station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "print(to_lowercase(test_question))\n",
    "print(to_lowercase(test_passage))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:30:53.745809Z",
     "start_time": "2024-10-15T09:30:53.736971Z"
    }
   },
   "id": "f22d4b60adff0a22"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "Epsom railway station serves the town of Epsom in Surrey It is located off Waterloo Road and is less than two minutes walk from the High Street It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations The station building was replaced in 2012 2013 with a new building with apartments above the station see end of article\n",
      "Visit us at  for more info\n",
      "Some RANDOM text With VARIETY Check this out  and the year is 2012 2013\n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters_and_urls(text: str) -> str:\n",
    "   # Remove URLs using regex\n",
    "    text: str = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "   \n",
    "   # Replace slashes with spaces first\n",
    "    text: str = text.replace('/', ' ')\n",
    "\n",
    "    # Remove special characters except for alphanumeric characters and spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "   \n",
    "    return cleaned_text\n",
    "\n",
    "test_question_w_url = \"Visit us at https://example.com for more info!\"\n",
    "test_passage_w_url = \"Some RANDOM text With VARIETY. Check this out: www.example.org and the year is 2012/2013.\"\n",
    "\n",
    "print(remove_special_characters_and_urls(test_question))\n",
    "print(remove_special_characters_and_urls(test_passage))\n",
    "print(remove_special_characters_and_urls(test_question_w_url))\n",
    "print(remove_special_characters_and_urls(test_passage_w_url))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:30:53.763465Z",
     "start_time": "2024-10-15T09:30:53.742090Z"
    }
   },
   "id": "c870a2e4ebb98354"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\n"
     ]
    }
   ],
   "source": [
    "def remove_extra_whitespace(text: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "print(remove_extra_whitespace(test_question))\n",
    "print(remove_extra_whitespace(test_passage))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:30:53.763763Z",
     "start_time": "2024-10-15T09:30:53.744001Z"
    }
   },
   "id": "a1ef7c329467e226"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing function to combine all preprocessing steps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5afde4001fe58359"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "epsom railway station serves the town of epsom in surrey it is located off waterloo road and is less than two minutes walk from the high street it is not in the london oyster card zone unlike epsom downs or tattenham corner stations the station building was replaced in 2012 2013 with a new building with apartments above the station see end of article\n",
      "visit us at for more info\n",
      "some random text with variety check this out and the year is 2012 2013\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text: str) -> str:\n",
    "    lowercase_text = to_lowercase(text)\n",
    "    cleaned_text = remove_special_characters_and_urls(lowercase_text)\n",
    "    prepared_text = remove_extra_whitespace(cleaned_text)\n",
    "    \n",
    "    return prepared_text\n",
    "\n",
    "test_question_w_url = \"Visit us at https://example.com for more info!\"\n",
    "test_passage_w_url = \"Some RANDOM text With VARIETY. Check this out: www.example.org and the year is 2012/2013.\"\n",
    "\n",
    "\n",
    "print(preprocessing(test_question))\n",
    "print(preprocessing(test_passage))\n",
    "print(preprocessing(test_question_w_url))\n",
    "print(preprocessing(test_passage_w_url))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T09:30:53.764383Z",
     "start_time": "2024-10-15T09:30:53.745775Z"
    }
   },
   "id": "60b400b89c7c9b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Tokenization**:\n",
    "  - **Decision**: Use a pre-trained Byte-Pair Encoding (BPE) tokenizer from the `transformers` library.\n",
    "  - **Reasoning**:\n",
    "    - Using a pre-trained tokenizer simplifies the preprocessing pipeline, as the tokenizer has already been trained on a large and diverse corpus, which increases its generalization capability.\n",
    "    - Pre-trained tokenizers from `transformers` are well-optimized and widely used in various NLP tasks.\n",
    "    - BPE helps handle out-of-vocabulary (OOV) words by breaking them into known subword units, allowing for more robust word representations.\n",
    "\n",
    "- **Sequence Truncation and Padding**:\n",
    "  - **Truncating**: Truncate sequences to a fixed length of 512 tokens.\n",
    "  - **Padding**: Apply padding to make all sequences in a batch have the same length.\n",
    "  - **Reasoning**:\n",
    "    - Limiting the sequence length to 512 tokens balances computational efficiency and context retention. This choice ensures that the input size remains manageable while still covering most of the content in the passages. It is also a popular sequence length for nlp applications, that's why I chose it.\n",
    "\n",
    "- **Word Embedding Lookups**:\n",
    "  - **Decision**: Use the FastText API directly to obtain embeddings for tokenized words.\n",
    "  - **Reasoning**:\n",
    "    - The FastText API considers subword information when generating word embeddings, providing robust handling of OOV words.\n",
    "    - This approach prevents the issue of having to map subword tokens directly to embeddings, which is not feasible with traditional embedding lookup methods.\n",
    "  - **OOV Word Handling**:\n",
    "    - Rely on FastText's built-in subword handling to generate embeddings for unknown words.\n",
    "\n",
    "- **Input Preparation**:\n",
    "  - Each input is a concatenation of the question and passage of total length 1024 (512 * 2). This sequence will be tokenized and converted into a sequence of FastText word embeddings (each of dimension 300).\n",
    "  - The resulting input will have the required shape of `(max_sequence_length * 2, batch_size, embedding_dim)`â€” for example, `(1024, 32, 300)` for a batch size of 32.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1e2c73a5ba337b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Architecture\n",
    "- **RNN Type**:\n",
    "  - **Decision**: Use LSTM for the RNN layers.\n",
    "  - **Rationale**: LSTM cells help maintain long-term dependencies through gating mechanisms, which is beneficial for reading comprehension tasks where context from the entire passage can be important for answering questions.\n",
    "\n",
    "- **Model Configuration**:\n",
    "  - **Embedding Layer**: Input dimension of 300 using FastText embeddings.\n",
    "  - **RNN Layers**: Two LSTM layers with a hidden size of 128.\n",
    "  - **Dropout**: Apply dropout with a rate of 0.3 between the LSTM layers for regularization.\n",
    "  - **Classifier**: A two-layer fully connected network (hidden layer of size 64) with ReLU activation.\n",
    "\n",
    "- **Loss and Optimizer**:\n",
    "  - **Loss Function**: Use Binary Cross-Entropy Loss for the binary classification task.\n",
    "  - **Optimizer**: Use the Adam optimizer with an initial learning rate of 0.001.\n",
    "  - **Rationale**:\n",
    "    - Adam is chosen for its adaptive learning rate, which can improve training stability and convergence.\n",
    "\n",
    "- **Regularization**:\n",
    "  - **Dropout**: Applied to reduce overfitting.\n",
    "  - **Early Stopping**: Monitor validation loss and stop training if it does not improve for 3 consecutive epochs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1e7a81ace8a8a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training\n",
    "- **Number of Epochs**: Train for up to 20 epochs with early stopping.\n",
    "- **Checkpointing**: Save the model with the best validation accuracy to avoid overfitting.\n",
    "\n",
    "- **Hyperparameter Experimentation**:\n",
    "  - **Learning Rate**: Test various learning rates (e.g., 0.001, 0.0005, 0.0001) to find an optimal balance between convergence speed and training stability.\n",
    "  - **Batch Size**: Experiment with different batch sizes (e.g., 16, 32, 64) to optimize memory usage and training time.\n",
    "  - **Dropout Rate**: Adjust dropout rates (e.g., 0.2, 0.3, 0.5) to find the optimal level of regularization.\n",
    "  - **Hidden Layer Size**: Try varying the number of hidden units in the RNN and classifier layers (e.g., 64, 128, 256) to assess their impact on model capacity.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affaf388d1f82eb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluation\n",
    "- **Primary Metric**:\n",
    "  - **Accuracy**: Chosen as the main evaluation metric since it reflects the overall model performance in binary classification.\n",
    "- **Baseline Comparison**:\n",
    "  - Compare the model's accuracy against a majority class baseline (e.g., always predicting \"yes\") to understand the model's relative performance.\n",
    "- **Error Analysis**:\n",
    "  - Analyze the confusion matrix to identify patterns in misclassifications and judge the types of errors the model makes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f3eba8bc7297f7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Interpretation\n",
    "- **Performance Expectations**:\n",
    "  - Learning from the results of Project 1 I am setting my expectations a bit lower (more realistic) this time. I'm expecting the LSTM to achieve an accuracy of 65 - 70%. Hopefully beating the baseline of always predicting \"yes\" (accuracy of 61-63%)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "936219fee6686c70"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9b88d3598126676f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
