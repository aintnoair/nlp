{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks for BoolQ Reading Comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e72b2ad5a819003"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "- **Objective**: Develop a reading comprehension model using a 2-layer LSTM and a 2-layer classifier. The model will be trained end-to-end on the BoolQ dataset.\n",
    "- **Task**: The BoolQ dataset involves answering yes/no questions given a passage. The goal is to predict the correct label for each question.\n",
    "- **Approach**: Utilize PyTorch for building the model, and Hugging Face's datasets library to manage data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca4c37d307d33a05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setup\n",
    "- **Libraries**: \n",
    "  - `torch`: For building the neural network.\n",
    "  - `datasets`: For loading the BoolQ dataset.\n",
    "  - `transformers`: For using a pre-trained BPE tokenizer.\n",
    "  - `fasttext`: To load and use FastText embeddings.\n",
    "  - `numpy`, `pandas`, `matplotlib`, `seaborn`: For data manipulation and visualization.\n",
    "  - `gensim`: For loading the pre-trained word embedding model.\n",
    "  - `sklearn`: For metrics.\n",
    "  - `wandb`: For experiment tracking\n",
    "\n",
    "- **Planned Correctness Tests**:\n",
    "  - Use `assert` statements to check tensor dimensions, and confirm the expected shapes of inputs and outputs throughout the data pipeline.\n",
    "  - Print sample outputs at different stages to validate transformations.\n",
    "\n",
    "- **Experiment Tracking**:\n",
    "  - Use `wandb` for logging experiments, including hyperparameters, metrics, and visualizations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50258e8b99db9e90"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# TODO: make the pip install for used libraries and packages !!!\n",
    "\n",
    "# %pip install torch datasets transformers fasttext numpy pandas matplotlib gensim scikit-learn wandb re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:04.987031Z",
     "start_time": "2024-10-21T11:16:04.972006Z"
    }
   },
   "id": "a977d9e2d10e0d78"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# TODO: remove all chatGPT comments and add my own documentation in MDF"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:05.218878Z",
     "start_time": "2024-10-21T11:16:05.213515Z"
    }
   },
   "id": "da88b6d880b92bc3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import nltk\n",
    "import fasttext\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:10.550311Z",
     "start_time": "2024-10-21T11:16:05.433726Z"
    }
   },
   "id": "f8484a2a506d95aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the hyperparameters (keeping it on top)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5de3ffd1cb28f735"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "max_seq_len = 512               # You already have this defined\n",
    "embedding_dim: int = 300        # fastText model embedding dimension\n",
    "padding_type: str = 'zeros'     # Choose between padding w/ 'zeros' or the 'avg' of the embedding\n",
    "\n",
    "optimizer_choice: str = 'Adam'  # Optimizer ['Adam', 'AdamW', 'SGD']\n",
    "\n",
    "hidden_dim: int  = 128          # Hidden size for LSTM\n",
    "output_dim: int = 1             # Binary classification (yes/no)\n",
    "n_layers: int  = 2              # Two LSTM layers\n",
    "dropout_rate: float = 0.3       # Dropout rate for regularization\n",
    "learning_rate: float = 1e-3     # Optimizer learning rate\n",
    "weight_decay: float = 0.01      # For AdamW or L2 in SGD\n",
    "train_batch_size: int = 32      # Training Batch size\n",
    "val_batch_size: int = 32        # Validation and Testing Batch size\n",
    "n_epochs: int  = 10             # Number of epochs\n",
    "patience: int = 3               # Early stopping patience\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:10.558599Z",
     "start_time": "2024-10-21T11:16:10.551778Z"
    }
   },
   "id": "db9bc34f53a5c58d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33maintnoair\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/blackbook/Desktop/dev-5-sem/nlp/project-2/wandb/run-20241021_131611-0sw4ka8v</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/0sw4ka8v' target=\"_blank\">run_1-default-run</a></strong> to <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/0sw4ka8v' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/0sw4ka8v</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/0sw4ka8v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x3272d79d0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_number: int = 1 # TODO: Don't forget to change this!!!\n",
    "\n",
    "# defining the WandB project and run name\n",
    "project_name: str = 'nlp-rnn_lstm_pt'\n",
    "run_name: str = f\"run_{run_number}-default-run\"\n",
    "\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"max_seq_len\": max_seq_len,\n",
    "        \"padding_type\": padding_type,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"val_batch_size\": val_batch_size,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"patience\": patience,\n",
    "        \"optimizer_choice\": optimizer_choice\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:11.913496Z",
     "start_time": "2024-10-21T11:16:10.554431Z"
    }
   },
   "id": "b5285055537a313d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading the required BoolQ dataset and splitting it like required from the project presentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3890b48a2aee852"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:33.485742Z",
     "start_time": "2024-10-21T11:16:27.334195Z"
    }
   },
   "id": "b2bdeff042b31f82"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fe046ba02f3668e4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bc75346b2a04366b50f2edbbc245b25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">run_1-default-run</strong> at: <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/0sw4ka8v' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/0sw4ka8v</a><br/> View project at: <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20241021_131611-0sw4ka8v/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:24.273494Z",
     "start_time": "2024-10-21T11:16:20.631435Z"
    }
   },
   "id": "91ed1684f3b064aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the data and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90eef930ca6a750"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you use oyster card at epsom station', 'answer': False, 'passage': \"Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\"}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n",
      "Train set - Yes: 5279, No: 3148, Ratio (y/n): 1.68, Percent Yes: 62.64%\n",
      "Validation set - Yes: 595, No: 405, Ratio (y/n): 1.47, Percent Yes: 59.5%\n",
      "Test set - Yes: 2033, No: 1237, Ratio (y/n): 1.64, Percent Yes: 62.17%\n"
     ]
    }
   ],
   "source": [
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")\n",
    "\n",
    "train_yes_count = sum(1 for label in train_data['answer'] if label == 1)\n",
    "train_no_count = sum(1 for label in train_data['answer'] if label == 0)\n",
    "train_total = train_yes_count + train_no_count\n",
    "\n",
    "validation_yes_count = sum(1 for label in validation_data['answer'] if label == 1)\n",
    "validation_no_count = sum(1 for label in validation_data['answer'] if label == 0)\n",
    "validation_total = validation_yes_count + validation_no_count\n",
    "\n",
    "test_yes_count = sum(1 for label in test_data['answer'] if label == 1)\n",
    "test_no_count = sum(1 for label in test_data['answer'] if label == 0)\n",
    "test_total = test_yes_count + test_no_count\n",
    "\n",
    "# Print the counts and ratios\n",
    "print(f\"Train set - Yes: {train_yes_count}, No: {train_no_count}, Ratio (y/n): {round(train_yes_count / train_no_count, 2)}, Percent Yes: {round(train_yes_count / train_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Validation set - Yes: {validation_yes_count}, No: {validation_no_count}, Ratio (y/n): {round(validation_yes_count / validation_no_count, 2)}, Percent Yes: {round(validation_yes_count / validation_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Test set - Yes: {test_yes_count}, No: {test_no_count}, Ratio (y/n): {round(test_yes_count / test_no_count, 2)}, Percent Yes: {round(test_yes_count / test_total * 100, 2)}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:49.459126Z",
     "start_time": "2024-10-21T11:16:49.434179Z"
    }
   },
   "id": "619cd85df4224366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the fastText model if not already in directory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a913a3df2c404597"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Load the fastText model\n",
    "model_bin = Path('cc.en.300.bin')\n",
    "\n",
    "if not model_bin.exists():\n",
    "    fasttext.util.download_model('en', if_exists='ignore') # download if not already in dir\n",
    "\n",
    "ft = fasttext.load_model(str(model_bin))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:57.944857Z",
     "start_time": "2024-10-21T11:16:57.943265Z"
    }
   },
   "id": "248ed6e5746879c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure the nltk tokenizer is downloaded"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61c2334d73b996db"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/blackbook/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:58.072405Z",
     "start_time": "2024-10-21T11:16:57.945949Z"
    }
   },
   "id": "2b184b9bbc120d0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "- **Text Cleaning**:\n",
    "  - **Operations**:\n",
    "    - Convert text to lowercase for consistency.\n",
    "    - Remove special characters and URLs while keeping necessary hyphens.\n",
    "    - Remove extra whitespace between words as well as before or after a sequence.\n",
    "  - **Reasoning**: These basic cleaning steps standardize the input without over-complicating the preprocessing and removing as little sentiment as possible from the sentences. I chose to not remove stopwords and not do stemming or lemmatizing for the same reason.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8f7871108c5ac1"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:58.073913Z",
     "start_time": "2024-10-21T11:16:58.072841Z"
    }
   },
   "id": "37e5ff2bfa672e78"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "epsom railway station serves the town of epsom in surrey. it is located off waterloo road and is less than two minutes' walk from the high street. it is not in the london oyster card zone unlike epsom downs or tattenham corner stations. the station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(text: str) -> str:\n",
    "    lowered_text = text.lower()\n",
    "    assert lowered_text.islower(), \"Text is not fully lowercase\"\n",
    "    return lowered_text\n",
    "\n",
    "print(to_lowercase(test_question))\n",
    "print(to_lowercase(test_passage))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:58.083791Z",
     "start_time": "2024-10-21T11:16:58.075702Z"
    }
   },
   "id": "f22d4b60adff0a22"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "Epsom railway station serves the town of Epsom in Surrey It is located off Waterloo Road and is less than two minutes' walk from the High Street It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations The station building was replaced in 2012 2013 with a new building with apartments above the station see end of article\n",
      "Visit us at  for more info\n",
      "Some RANDOM text With VARIETY Check this out  and the year is 2012 2013\n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters_and_urls(text: str) -> str:\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "    # Replace slashes with spaces first\n",
    "    text = text.replace('/', ' ')\n",
    "    # Remove special characters except for alphanumeric characters and spaces\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s\\'\\-]', '', text)\n",
    "    assert not re.search(r'http[s]?://|www\\.', cleaned_text), \"URLs were not fully removed\"\n",
    "    return cleaned_text\n",
    "\n",
    "test_question_w_url = \"Visit us at https://example.com for more info!\"\n",
    "test_passage_w_url = \"Some RANDOM text With VARIETY. Check this out: www.example.org and the year is 2012/2013.\"\n",
    "\n",
    "print(remove_special_characters_and_urls(test_question))\n",
    "print(remove_special_characters_and_urls(test_passage))\n",
    "print(remove_special_characters_and_urls(test_question_w_url))\n",
    "print(remove_special_characters_and_urls(test_passage_w_url))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:58.084Z",
     "start_time": "2024-10-21T11:16:58.079118Z"
    }
   },
   "id": "c870a2e4ebb98354"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you use oyster card at epsom station\n",
      "Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\n"
     ]
    }
   ],
   "source": [
    "def remove_extra_whitespace(text: str) -> str:\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    assert '  ' not in cleaned_text, \"There are still multiple spaces\"\n",
    "    return cleaned_text\n",
    "\n",
    "print(remove_extra_whitespace(test_question))\n",
    "print(remove_extra_whitespace(test_passage))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:16:59.178043Z",
     "start_time": "2024-10-21T11:16:59.166071Z"
    }
   },
   "id": "a1ef7c329467e226"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Initial Plan:*\n",
    "- ***Tokenization**:*\n",
    "  - ***Decision**: Use a pre-trained Byte-Pair Encoding (BPE) tokenizer from the `transformers` library.*\n",
    "  - ***Reasoning**:*\n",
    "    - *Using a pre-trained tokenizer simplifies the preprocessing pipeline, as the tokenizer has already been trained on a large and diverse corpus, which increases its generalization capability.*\n",
    "    - *Pre-trained tokenizers from `transformers` are well-optimized and widely used in various NLP tasks.*\n",
    "    - *BPE helps handle out-of-vocabulary (OOV) words by breaking them into known subword units, allowing for more robust word representations.*\n",
    "\n",
    "Adjusted plan after feedback on project 1 as well as stage 1 of project 2:\n",
    "- **Tokenization**\n",
    "  - **Decision**: Use NLTK's word-level tokenizer.\n",
    "  - **Reasoning**:\n",
    "    - Ensures compatibility with FastText, which expects whole words.\n",
    "    - Reduces complexity by relying on FastText's internal OOV handling.\n",
    "    - Aligns with the word-based tokenization used in FastText’s training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad0822e95495d979"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Tokenization using NLTK\n",
    "def tokenize(text: str) -> list:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    assert isinstance(tokens, list) and len(tokens) > 0, \"Tokenization failed or empty token list\"\n",
    "    return tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:17:02.518882Z",
     "start_time": "2024-10-21T11:17:02.504840Z"
    }
   },
   "id": "7ba8ca92c7221d6c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epsom', 'railway', 'station', 'serves', 'the', 'town', 'of', 'epsom', 'in', 'surrey', 'it', 'is', 'located', 'off', 'waterloo', 'road', 'and', 'is', 'less', 'than', 'two', 'minutes', \"'\", 'walk', 'from', 'the', 'high', 'street', 'it', 'is', 'not', 'in', 'the', 'london', 'oyster', 'card', 'zone', 'unlike', 'epsom', 'downs', 'or', 'tattenham', 'corner', 'stations', 'the', 'station', 'building', 'was', 'replaced', 'in', '2012', '2013', 'with', 'a', 'new', 'building', 'with', 'apartments', 'above', 'the', 'station', 'see', 'end', 'of', 'article']\n",
      "['can', 'you', 'use', 'oyster', 'card', 'at', 'epsom', 'station']\n"
     ]
    }
   ],
   "source": [
    "lower_passage = to_lowercase(test_passage)\n",
    "lower_question = to_lowercase(test_question)\n",
    "\n",
    "cleaned_passage = remove_extra_whitespace(remove_special_characters_and_urls(lower_passage))\n",
    "cleaned_question = remove_extra_whitespace(remove_special_characters_and_urls(lower_question))\n",
    "\n",
    "print(tokenize(cleaned_passage))\n",
    "print(tokenize(cleaned_question))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:17:04.309119Z",
     "start_time": "2024-10-21T11:17:04.295494Z"
    }
   },
   "id": "732c45f0593dfcc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Word Embedding Lookups**:\n",
    "  - **Decision**: Use the FastText API directly to obtain embeddings for tokenized words.\n",
    "  - **Reasoning**:\n",
    "    - The FastText API considers subword information when generating word embeddings, providing robust handling of OOV words.\n",
    "    - This approach prevents the issue of having to map subword tokens directly to embeddings, which is not feasible with traditional embedding lookup methods.\n",
    "  - **OOV Word Handling**:\n",
    "    - Rely on FastText's built-in subword handling to generate embeddings for unknown words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db08990dadde1090"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Function to get FastText embeddings\n",
    "def get_fasttext_embeddings(tokens: list) -> np.ndarray:\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embedding = ft.get_word_vector(token)\n",
    "        assert embedding.shape == (embedding_dim,), f\"Embedding shape mismatch for token: {token}\"\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:17:17.242434Z",
     "start_time": "2024-10-21T11:17:17.218567Z"
    }
   },
   "id": "b3d5913344feda42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Sequence Truncation and Padding**:\n",
    "  - **Truncating**: Truncate sequences to a fixed length of 512 tokens.\n",
    "  - **Padding**: Apply padding to make all sequences in a batch have the same length.\n",
    "  - **Reasoning**:\n",
    "    - Limiting the sequence length to 512 tokens balances computational efficiency and context retention. This choice ensures that the input size remains manageable while still covering most of the content in the passages. It is also a popular sequence length for nlp applications, that's why I chose it. I will play with the maximum sequence length for tuning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7a5b021979f9e6b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Padding/truncation function\n",
    "def pad_or_truncate(sequence: np.ndarray, max_length: int = max_seq_len, padding_type: str = 'zeros') -> np.ndarray:\n",
    "    current_length = len(sequence)\n",
    "    \n",
    "    # Truncate if the sequence is longer than max_length\n",
    "    if current_length > max_length:\n",
    "        return sequence[:max_length]\n",
    "    \n",
    "    # If the sequence is shorter than max_length, pad\n",
    "    elif current_length < max_length:\n",
    "        if padding_type == 'zeros':\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((max_length - current_length, sequence.shape[1]))\n",
    "        elif padding_type == 'avg':\n",
    "            # Pad with the average embedding\n",
    "            avg_embedding = np.mean(sequence, axis=0)\n",
    "            padding = np.tile(avg_embedding, (max_length - current_length, 1))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid padding_type. Use 'zeros' or 'avg'.\")\n",
    "        \n",
    "        padded_sequence = np.vstack((sequence, padding))\n",
    "        assert padded_sequence.shape == (max_length, sequence.shape[1]), \"Padding failed\"\n",
    "        return padded_sequence\n",
    "    \n",
    "    else:\n",
    "        return sequence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:18:01.233740Z",
     "start_time": "2024-10-21T11:18:01.212224Z"
    }
   },
   "id": "5fdacfb5795ee089"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine all preprocessing steps into a single preprocessing pipeline for easy data preparation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5afde4001fe58359"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Full preprocessing pipeline\n",
    "def preprocessing_pipeline(text: str) -> np.ndarray:\n",
    "    # Step 1: Clean the text\n",
    "    lowercase_text = to_lowercase(text)\n",
    "    cleaned_text = remove_special_characters_and_urls(lowercase_text)\n",
    "    prepared_text = remove_extra_whitespace(cleaned_text)\n",
    "    \n",
    "    # Step 2: Tokenize the text\n",
    "    tokens = tokenize(prepared_text)\n",
    "    \n",
    "    # Step 3: Get FastText embeddings\n",
    "    embeddings = get_fasttext_embeddings(tokens)\n",
    "    \n",
    "    # Step 4: Pad or truncate the embeddings to a fixed length\n",
    "    padded_embeddings = pad_or_truncate(embeddings)\n",
    "    \n",
    "    # Ensure the final output shape is correct\n",
    "    assert padded_embeddings.shape == (max_seq_len, embedding_dim), f\"Final embedding shape is {padded_embeddings.shape}, expected (512, 300)\"\n",
    "    \n",
    "    return padded_embeddings\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:18:03.980015Z",
     "start_time": "2024-10-21T11:18:03.957802Z"
    }
   },
   "id": "60b400b89c7c9b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print some test results to see if the preprocessing worked like expected."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebdb983ddd3382d5"
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: can you use oyster card at epsom station\n",
      "Displaying first 5 tokens and their embeddings:\n",
      "Token: can - Embedding: [ 0.04433588  0.09070976  0.05368941  0.18836261 -0.1909022 ]...\n",
      "Token: you - Embedding: [ 0.10789153 -0.04412316  0.13406183  0.0902128  -0.15022287]...\n",
      "Token: use - Embedding: [-0.0271657   0.06062786 -0.06484954  0.04023458  0.0097399 ]...\n",
      "Token: oyster - Embedding: [ 0.02858743 -0.06040148  0.00882877  0.10480718  0.07261764]...\n",
      "Token: card - Embedding: [0.07530363 0.10068872 0.0854513  0.08289765 0.00129725]...\n"
     ]
    }
   ],
   "source": [
    "# Function to print some sample embeddings for inspection\n",
    "def print_sample_embeddings(tokens: list, embeddings: np.ndarray, num_samples: int = 5):\n",
    "    print(f\"Displaying first {num_samples} tokens and their embeddings:\")\n",
    "    for i in range(min(num_samples, len(tokens))):\n",
    "        print(f\"Token: {tokens[i]} - Embedding: {embeddings[i][:5]}...\")  # Show the first 5 dimensions of each embedding for brevity\n",
    "\n",
    "# Test the pipeline\n",
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "test_question = train_data[5]['question']\n",
    "\n",
    "print(f\"Original Question: {test_question}\")\n",
    "processed_embeddings = preprocessing_pipeline(test_question)\n",
    "\n",
    "# Run a separate tokenization to print tokens (as they are not returned from the pipeline in the current design)\n",
    "tokens = tokenize(remove_extra_whitespace(remove_special_characters_and_urls(to_lowercase(test_question))))\n",
    "print_sample_embeddings(tokens, processed_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.238168Z",
     "start_time": "2024-10-19T22:09:06.848300Z"
    }
   },
   "id": "bdd334bfc49b3586"
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "# Function to log confusion matrix\n",
    "def log_confusion_matrix(labels, preds):\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    # Plot confusion matrix using Seaborn heatmap\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix on Validation set')\n",
    "\n",
    "    # Log confusion matrix plot as an image to WandB\n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.238360Z",
     "start_time": "2024-10-19T22:09:08.233291Z"
    }
   },
   "id": "8848ec28078fd33c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Input Preparation**:\n",
    "  - Each input is a concatenation of the question and passage of total length 1024 (512 * 2). This sequence will be tokenized and converted into a sequence of FastText word embeddings (each of dimension 300).\n",
    "  - The resulting input will have the required shape of `(max_sequence_length * 2, batch_size, embedding_dim)`— for example, `(1024, 32, 300)` for a batch size of 32.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1e2c73a5ba337b3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc970d736147822"
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "class BoolQDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, max_seq_length: int =512) -> None:\n",
    "        self.data = data\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.tensor:\n",
    "        # Retrieve passage and question\n",
    "        passage = self.data[idx]['passage']\n",
    "        question = self.data[idx]['question']\n",
    "        \n",
    "        # Preprocess both passage and question using the preprocessing pipeline\n",
    "        passage_embeddings = preprocessing_pipeline(passage)\n",
    "        question_embeddings = preprocessing_pipeline(question)\n",
    "        \n",
    "        # Concatenate passage and question embeddings (passage first, then question)\n",
    "        combined_embeddings = np.concatenate((passage_embeddings, question_embeddings), axis=0)\n",
    "        \n",
    "        # If the combined length exceeds the max_seq_length, truncate it\n",
    "        combined_embeddings = pad_or_truncate(combined_embeddings, max_length=self.max_seq_length)\n",
    "        \n",
    "        # Convert the label to tensor (1 for 'yes', 0 for 'no')\n",
    "        label = 1 if self.data[idx]['answer'] else 0\n",
    "        \n",
    "        # Return the combined embeddings and the label as tensors\n",
    "        return torch.tensor(combined_embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.240066Z",
     "start_time": "2024-10-19T22:09:08.238561Z"
    }
   },
   "id": "41fc6be83e8eb578"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c3f9629aee75e3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec4277ed1f8b476"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "# Define the dataset and dataloader objects with dynamically calculated max lengths\n",
    "train_dataset = BoolQDataset(train_data)\n",
    "validation_dataset = BoolQDataset(validation_data)\n",
    "test_dataset = BoolQDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.246705Z",
     "start_time": "2024-10-19T22:09:08.243273Z"
    }
   },
   "id": "8d73e9f1e3c60f9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Architecture\n",
    "- **RNN Type**:\n",
    "  - **Decision**: Use LSTM for the RNN layers.\n",
    "  - **Rationale**: LSTM cells help maintain long-term dependencies through gating mechanisms, which is beneficial for reading comprehension tasks where context from the entire passage can be important for answering questions.\n",
    "\n",
    "- **Model Configuration**:\n",
    "  - **Embedding Layer**: Input dimension of 300 using FastText embeddings.\n",
    "  - **RNN Layers**: Two LSTM layers with a hidden size of 128.\n",
    "  - **Dropout**: Apply dropout with a rate of 0.3 between the LSTM layers for regularization.\n",
    "  - **Classifier**: A two-layer fully connected network (hidden layer of size 64) with ReLU activation.\n",
    "\n",
    "- **Loss and Optimizer**:\n",
    "  - **Loss Function**: Use Binary Cross-Entropy Loss for the binary classification task.\n",
    "  - **Optimizer**: Use the Adam optimizer with an initial learning rate of 0.001.\n",
    "  - **Rationale**:\n",
    "    - Adam is chosen for its adaptive learning rate, which can improve training stability and convergence.\n",
    "\n",
    "- **Regularization**:\n",
    "  - **Dropout**: Applied to reduce overfitting.\n",
    "  - **Early Stopping**: Monitor validation loss and stop training if it does not improve for 3 consecutive epochs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1e7a81ace8a8a0"
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "# Optimizer selection function\n",
    "def get_optimizer(optimizer_choice, model, learning_rate, weight_decay=0):\n",
    "    if optimizer_choice == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_choice == 'AdamW':\n",
    "        return optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_choice == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_choice}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.248574Z",
     "start_time": "2024-10-19T22:09:08.246409Z"
    }
   },
   "id": "e36558b41efce54d"
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "# Model definition (same as before)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        last_hidden_state = hn[-1]\n",
    "        x = self.relu(self.fc1(last_hidden_state))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.279429Z",
     "start_time": "2024-10-19T22:09:08.250080Z"
    }
   },
   "id": "3302d00301393ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = LSTMModel(embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate)\n",
    "\n",
    "# Get optimizer based on choice\n",
    "optimizer = get_optimizer(optimizer_choice, model, learning_rate, weight_decay)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.289387Z",
     "start_time": "2024-10-19T22:09:08.252738Z"
    }
   },
   "id": "376c3d1cefd935b1"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: cpu, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Move model and criterion to the correct device (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Type: {device.type}, Device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:22:19.204809Z",
     "start_time": "2024-10-21T11:22:19.186699Z"
    }
   },
   "id": "b4c453214917920e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9231b301c278962c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training\n",
    "- **Number of Epochs**: Train for up to 20 epochs with early stopping.\n",
    "- **Checkpointing**: Save the model with the best validation accuracy to avoid overfitting.\n",
    "\n",
    "- **Hyperparameter Experimentation**:\n",
    "  - **Learning Rate**: Test various learning rates (e.g., 0.001, 0.0005, 0.0001) to find an optimal balance between convergence speed and training stability.\n",
    "  - **Batch Size**: Experiment with different batch sizes (e.g., 16, 32, 64) to optimize memory usage and training time.\n",
    "  - **Dropout Rate**: Adjust dropout rates (e.g., 0.2, 0.3, 0.5) to find the optimal level of regularization.\n",
    "  - **Hidden Layer Size**: Try varying the number of hidden units in the RNN and classifier layers (e.g., 64, 128, 256) to assess their impact on model capacity.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affaf388d1f82eb4"
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, n_epochs, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend((outputs.cpu().detach().numpy() > 0.5).astype(int))\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_labels = []\n",
    "        val_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend((outputs.cpu().detach().numpy() > 0.5).astype(int))\n",
    "\n",
    "        avg_val_loss = val_loss / len(validation_loader)\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_precision = precision_score(val_labels, val_preds)\n",
    "        val_recall = recall_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds)\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"val_f1\": val_f1\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Log confusion matrix without step parameter\n",
    "        log_confusion_matrix(val_labels, val_preds)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Log final metrics at the end of training\n",
    "    wandb.log({\"best_val_loss\": best_val_loss, \"final_val_accuracy\": val_acc, \"final_val_f1\": val_f1})\n",
    "    \n",
    "    # Finish WandB run\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:09:08.289703Z",
     "start_time": "2024-10-19T22:09:08.266512Z"
    }
   },
   "id": "ac7c15be85ddf7a1"
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.6638, Val Loss: 0.6749, Val Acc: 0.5950\n",
      "Epoch 2/10 - Train Loss: 0.6613, Val Loss: 0.6755, Val Acc: 0.5950\n",
      "Epoch 3/10 - Train Loss: 0.6620, Val Loss: 0.6748, Val Acc: 0.5950\n",
      "Epoch 4/10 - Train Loss: 0.6623, Val Loss: 0.6747, Val Acc: 0.5950\n",
      "Epoch 5/10 - Train Loss: 0.6623, Val Loss: 0.6763, Val Acc: 0.5950\n",
      "Epoch 6/10 - Train Loss: 0.6614, Val Loss: 0.6752, Val Acc: 0.5950\n",
      "Epoch 7/10 - Train Loss: 0.6611, Val Loss: 0.6787, Val Acc: 0.5950\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.131 MB of 0.131 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b21b1448d59744fb8d22800c4669eb3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>final_val_f1</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▆▆▆▇█▇</td></tr><tr><td>train_loss</td><td>█▁▃▄▄▂▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▂▁▁▄▂█</td></tr><tr><td>val_precision</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>val_recall</td><td>▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.67472</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_val_accuracy</td><td>0.595</td></tr><tr><td>final_val_f1</td><td>0.74608</td></tr><tr><td>train_accuracy</td><td>0.62656</td></tr><tr><td>train_loss</td><td>0.66108</td></tr><tr><td>val_accuracy</td><td>0.595</td></tr><tr><td>val_f1</td><td>0.74608</td></tr><tr><td>val_loss</td><td>0.67868</td></tr><tr><td>val_precision</td><td>0.595</td></tr><tr><td>val_recall</td><td>1</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">run_1-default-run</strong> at: <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/2mx8i5r3' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt/runs/2mx8i5r3</a><br/> View project at: <a href='https://wandb.ai/aintnoair/nlp-rnn_lstm_pt' target=\"_blank\">https://wandb.ai/aintnoair/nlp-rnn_lstm_pt</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 7 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20241020_000855-2mx8i5r3/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming train_loader and validation_loader are already defined\n",
    "train_model(model, train_loader, validation_loader, n_epochs, patience)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:20:16.671496Z",
     "start_time": "2024-10-19T22:09:08.268763Z"
    }
   },
   "id": "4ab3d778c324c1a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluation\n",
    "- **Primary Metric**:\n",
    "  - **Accuracy**: Chosen as the main evaluation metric since it reflects the overall model performance in binary classification.\n",
    "- **Baseline Comparison**:\n",
    "  - Compare the model's accuracy against a majority class baseline (e.g., always predicting \"yes\") to understand the model's relative performance.\n",
    "- **Error Analysis**:\n",
    "  - Analyze the confusion matrix to identify patterns in misclassifications and judge the types of errors the model makes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f3eba8bc7297f7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Interpretation\n",
    "- **Performance Expectations**:\n",
    "  - Learning from the results of Project 1 I am setting my expectations a bit lower (more realistic) this time. I'm expecting the LSTM to achieve an accuracy of 65 - 70%. Hopefully beating the baseline of always predicting \"yes\" (accuracy of 61-63%)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "936219fee6686c70"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9b88d3598126676f"
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:20:16.674Z",
     "start_time": "2024-10-19T22:20:16.672283Z"
    }
   },
   "id": "2c9d092a1447e02a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
