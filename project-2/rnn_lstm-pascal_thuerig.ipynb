{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks for BoolQ Reading Comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e72b2ad5a819003"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "- **Objective**: Develop a reading comprehension model using a 2-layer LSTM and a 2-layer classifier. The model will be trained end-to-end on the BoolQ dataset.\n",
    "- **Task**: The BoolQ dataset involves answering yes/no questions given a passage. The goal is to predict the correct label for each question.\n",
    "- **Approach**: Utilize PyTorch for building the model, and Hugging Face's datasets library to manage data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca4c37d307d33a05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setup\n",
    "- **Libraries**: \n",
    "  - `torch`: For building the neural network.\n",
    "  - `datasets`: For loading the BoolQ dataset.\n",
    "  - `transformers`: For using a pre-trained BPE tokenizer.\n",
    "  - `fasttext`: To load and use FastText embeddings.\n",
    "  - `numpy`, `pandas`, `matplotlib`, `seaborn`: For data manipulation and visualization.\n",
    "  - `gensim`: For loading the pre-trained word embedding model.\n",
    "  - `sklearn`: For metrics.\n",
    "  - `wandb`: For experiment tracking\n",
    "\n",
    "- **Environment Configuration**:\n",
    "  - Ensure reproducibility by setting random seeds where possible.\n",
    "\n",
    "- **Planned Correctness Tests**:\n",
    "  - Use `assert` statements to check tensor dimensions, and confirm the expected shapes of inputs and outputs throughout the data pipeline.\n",
    "  - Print sample outputs at different stages to validate transformations.\n",
    "\n",
    "- **Experiment Tracking**:\n",
    "  - Use `wandb` for logging experiments, including hyperparameters, metrics, and visualizations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50258e8b99db9e90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Preprocessing\n",
    "- **Tokenization**:\n",
    "  - **Decision**: Use a pre-trained Byte-Pair Encoding (BPE) tokenizer from the `transformers` library.\n",
    "  - **Rationale**:\n",
    "    - Using a pre-trained tokenizer simplifies the preprocessing pipeline, as the tokenizer has already been trained on a large and diverse corpus, which increases its generalization capability.\n",
    "    - Pre-trained tokenizers from `transformers` are well-optimized and widely used in various NLP tasks.\n",
    "    - BPE helps handle out-of-vocabulary (OOV) words by breaking them into known subword units, allowing for more robust word representations.\n",
    "\n",
    "- **Word Embedding**:\n",
    "  - **Decision**: Use the FastText API for embeddings, as it can leverage subword information to generate embeddings for words not in the vocabulary.\n",
    "  - **Rationale**:\n",
    "    - The FastText API is designed to handle OOV words by computing embeddings using subword units.\n",
    "\n",
    "- **Handling Text Cleaning**:\n",
    "  - **Operations**:\n",
    "    - Convert text to lowercase for consistency.\n",
    "    - Remove special characters and extra whitespace.\n",
    "  - **Justification**: These basic cleaning steps standardize the input without over-complicating the preprocessing and removing as little sentiment as possible from the sentences. I chose to not remove stopwords and not stemm- / lemmatize for the same reason.\n",
    "\n",
    "- **Sequence Truncation and Padding**:\n",
    "  - **Truncating**: Truncate sequences to a fixed length of 512 tokens.\n",
    "  - **Padding**: Apply padding to make all sequences in a batch have the same length.\n",
    "  - **Rationale**:\n",
    "    - Limiting the sequence length to 512 tokens balances computational efficiency and context retention. This choice ensures that the input size remains manageable while still covering most of the content in the passages. It is also a popular sequence length for nlp applications, that's why I chose it.\n",
    "\n",
    "- **Input**:\n",
    "  - **Decision**: Pass the sequence of word embeddings directly to the LSTM without averaging, ensuring that each embedding retains its position in the sequence.\n",
    "  - **Rationale**:\n",
    "    - LSTM networks are designed to process sequential data, where each step in the sequence corresponds to a time step in the model. By feeding the LSTM with a sequence of embeddings, it can learn the dependencies between the tokens, capturing contextual information across the entire sequence.\n",
    "    - Retaining the full sequence allows the LSTM's gating mechanisms to selectively remember or forget information based on the input at each step, which is crucial for understanding long-term dependencies in reading comprehension tasks.\n",
    "\n",
    "  - **Input Preparation**:\n",
    "    - Each input sequence will be tokenized and converted into a sequence of FastText word embeddings (each of dimension 300).\n",
    "    - The resulting input will have the required shape of `(max_sequence_length, batch_size, embedding_dim)`â€” for example, `(512, 32, 300)` for a batch size of 32.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8f7871108c5ac1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Model Architecture\n",
    "- **RNN Type**:\n",
    "  - **Decision**: Use LSTM for the RNN layers.\n",
    "  - **Rationale**: LSTM cells help maintain long-term dependencies through gating mechanisms, which is beneficial for reading comprehension tasks where context from the entire passage can be important for answering questions.\n",
    "\n",
    "- **Model Configuration**:\n",
    "  - **Embedding Layer**: Input dimension of 300 using FastText embeddings.\n",
    "  - **RNN Layers**: Two LSTM layers with a hidden size of 128.\n",
    "  - **Dropout**: Apply dropout with a rate of 0.3 between the LSTM layers for regularization.\n",
    "  - **Classifier**: A two-layer fully connected network (hidden layer of size 64) with ReLU activation.\n",
    "\n",
    "- **Loss and Optimizer**:\n",
    "  - **Loss Function**: Use Binary Cross-Entropy Loss for the binary classification task.\n",
    "  - **Optimizer**: Use the Adam optimizer with an initial learning rate of 0.001.\n",
    "  - **Rationale**:\n",
    "    - Adam is chosen for its adaptive learning rate, which can improve training stability and convergence.\n",
    "\n",
    "- **Regularization**:\n",
    "  - **Dropout**: Applied to reduce overfitting.\n",
    "  - **Early Stopping**: Monitor validation loss and stop training if it does not improve for 3 consecutive epochs.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1e7a81ace8a8a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Training\n",
    "- **Number of Epochs**: Train for up to 20 epochs with early stopping.\n",
    "- **Checkpointing**: Save the model with the best validation accuracy to avoid overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "affaf388d1f82eb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluation\n",
    "- **Primary Metric**:\n",
    "  - **Accuracy**: Chosen as the main evaluation metric since it reflects the overall model performance in binary classification.\n",
    "- **Baseline Comparison**:\n",
    "  - Compare the model's accuracy against a majority class baseline (e.g., always predicting \"yes\") to understand the model's relative performance.\n",
    "- **Error Analysis**:\n",
    "  - Analyze the confusion matrix to identify patterns in misclassifications and judge the types of errors the model makes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f3eba8bc7297f7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Interpretation\n",
    "- **Performance Expectations**:\n",
    "  - Learning from the results of Project 1 I am setting my expectations a bit lower (more realistic) this time. I'm expecting the LSTM to achieve an accuracy of 65 - 70%. Hopefully beating the baseline of always predicting \"yes\" (accuracy of 61-63%)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "936219fee6686c70"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9b88d3598126676f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
