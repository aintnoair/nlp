{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for BoolQ reading comprehension\n",
    "*All changes and additions compared to Stage 1 are marked in <span style=\"color: orange;\">orange</span>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "My sources for this project are linked in the respecting sections of the notebook. I used AI tools such as ChatGPT to correct my writing and grammar in stage 1 of this project and plan on using it for debugging during stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">TLDR - Executive summary</span>\n",
    "### **Topic:**\n",
    "Develop and train a Transformer-based model end to end for binary question answering on the BoolQ dataset from Huggin Face.\n",
    "\n",
    "### **Data:**\n",
    "The dataset is for reading comprehension and question answering tasks. It consists of questions, passages and the corresponding yes/no answers, with seperate training and test splits. Splitting the last 1000 entries of the training split to use as a validation split in this project.\n",
    "\n",
    "### **Methods:**\n",
    "The project involves preprocessing the data using the AutoTokenizer with the pre-trained \"bert-base-cased\" model. Then transforming the token IDs to word embeddings using the nn.Embedding layer. Using projection layer in between we reach the transformerEncoder with six layers followed by the mean pooling layer for dimensionality reduction into the 2-layer classifier from which we finally receive the binary output.\n",
    "\n",
    "### **Model:**\n",
    "The model is based on a PyTorch implementation of a Transformer Encoder, structured as follows:\n",
    "- Embedding Layer: nn.Embedding layer to map tokens to embeddings.\n",
    "- Positional Embedding Layer: Adds positional information to embeddings.\n",
    "- Linear Projection Layer: Projects from embedding dimension to transformer hidden dimension.\n",
    "- Transformer Encoder: A 6-layer transformer encoder block.\n",
    "- Pooling Layer: Mean pooling for dimensionality reduction.\n",
    "- Classifier: A 2-layer classifier with ReLU non-linearity.\n",
    "- Loss Function: Cross-Entropy Loss.\n",
    "\n",
    "### **Experiments:**\n",
    "Link to WandB: [Weights and Biases](https://wandb.ai/aintnoair/nlp_p3_transformer/overview) <br>\n",
    "17 manual experiments were conducted, exploring combinations of transformer (from 128 to 1024) and classifier hidden dimensions (from 64 to 512) as well as embedding dimensions (from 128 to 512) as well as further adjustments mentioned in more detail in the model section of the notebook.\n",
    "\n",
    "### **Results:**\n",
    "Key findings:\n",
    "- Adding 100 warmup steps at a batch size of 64 improved performance compared to no warmup.\n",
    "- A dropout rate of 0.2 was introduced to reduce overfitting on the training set.\n",
    "- After facing some issues with optuna for automatic hyperparameter tuning, manual tuning resulted in a model barely outperforming the majority baseline test accuracy of 62.17% with a test accuracy of **63.70%**.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Importing Python Packages**\n",
    "Making sure the notebook is reproducible and runs without error, I will install the necessary libraries in a pip cell below.\n",
    "\n",
    "**Data Loading and Split**\n",
    "The data consists of the questions, a passage and the answer. In total there are 12'697 entries in the dataset. Splitting them according to the lecture slides into train (8427), validation (1000) and test (3270).\n",
    "\n",
    "**Seeding for Reproducibility**\n",
    "Setting the random Seed to 42 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T13:18:46.381221Z",
     "start_time": "2024-11-16T13:18:15.173250Z"
    }
   },
   "source": [
    "# TODO: make the pip install for used libraries and packages !!!\n",
    "%pip install -q wandb datasets torch transformers pytorch_lightning torchmetrics matplotlib seaborn scikit-learn "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:19:19.520479Z",
     "start_time": "2024-11-16T13:18:46.382822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:26.631826Z",
     "start_time": "2024-11-16T13:20:26.628835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "pl.seed_everything(42, workers=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:27.004414Z",
     "start_time": "2024-11-16T13:20:26.996644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:27.283626Z",
     "start_time": "2024-11-16T13:20:27.281396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 64"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading the dataset based on lecture slides\n",
    "train_data = load_dataset('google/boolq', split='train[:-1000]')\n",
    "validation_data = load_dataset('google/boolq', split='train[-1000:]')\n",
    "test_data = load_dataset('google/boolq', split='validation')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:32.783132Z",
     "start_time": "2024-11-16T13:20:27.588222Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "test_question = train_data[5]['question']\n",
    "test_passage = train_data[5]['passage']\n",
    "print(train_data[5])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(validation_data)}\")\n",
    "print(f\"Number of validation samples: {len(test_data)}\")\n",
    "\n",
    "train_yes_count = sum(1 for label in train_data['answer'] if label == 1)\n",
    "train_no_count = sum(1 for label in train_data['answer'] if label == 0)\n",
    "train_total = train_yes_count + train_no_count\n",
    "\n",
    "validation_yes_count = sum(1 for label in validation_data['answer'] if label == 1)\n",
    "validation_no_count = sum(1 for label in validation_data['answer'] if label == 0)\n",
    "validation_total = validation_yes_count + validation_no_count\n",
    "\n",
    "test_yes_count = sum(1 for label in test_data['answer'] if label == 1)\n",
    "test_no_count = sum(1 for label in test_data['answer'] if label == 0)\n",
    "test_total = test_yes_count + test_no_count\n",
    "\n",
    "print(f\"Train set (yes/no) Ratio: {round(train_yes_count / train_no_count, 2)}, Percent Yes: {round(train_yes_count / train_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Validation set (yes/no) Ratio: {round(validation_yes_count / validation_no_count, 2)}, Percent Yes: {round(validation_yes_count / validation_total * 100, 2)}%\")\n",
    "\n",
    "print(f\"Test set (yes/no) Ratio: {round(test_yes_count / test_no_count, 2)}, Percent Yes: {round(test_yes_count / test_total * 100, 2)}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:32.797640Z",
     "start_time": "2024-11-16T13:20:32.784990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you use oyster card at epsom station', 'answer': False, 'passage': \"Epsom railway station serves the town of Epsom in Surrey. It is located off Waterloo Road and is less than two minutes' walk from the High Street. It is not in the London Oyster card zone unlike Epsom Downs or Tattenham Corner stations. The station building was replaced in 2012/2013 with a new building with apartments above the station (see end of article).\"}\n",
      "Number of training samples: 8427\n",
      "Number of validation samples: 1000\n",
      "Number of validation samples: 3270\n",
      "Train set (yes/no) Ratio: 1.68, Percent Yes: 62.64%\n",
      "Validation set (yes/no) Ratio: 1.47, Percent Yes: 59.5%\n",
      "Test set (yes/no) Ratio: 1.64, Percent Yes: 62.17%\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Tokenizer\n",
    "In past projects I always did some sort of manual preprocessing of the data. In this project I deliberately refrain from any manual preprocessing and will let the built-in features of the AutoTokenizer with the from_pretrained(\"bert-base-cased\") model handle the following steps for me:\n",
    "- Whitespace and Special Character removal (e.g. emojis or phonetic pronunciations)\n",
    "- Case Sensitivity\n",
    "- Padding and Truncation (pad automatically, truncate to max: 512 tokens - amount of pretrained position embeddings)\n",
    "\n",
    "I only now found out about this from the Hugging Face Transformer [Preprocessing Data Documentation](https://huggingface.co/transformers/v3.0.2/preprocessing.html).\n",
    "\n",
    "### Lowercase / Case Sensitivity\n",
    "From my feedback I will now keep case sensitivity instead of lower-casing all text. Example of case sensitivity: the word \"US\" would become \"us\" and could thus change the meaning of a sentence drastically. <br>\n",
    "*Source*: Feedback from Project 2 (LSTM)\n",
    "\n",
    "### Padding / Truncation\n",
    "I rely on the built-in padding and truncation functions of the AutoTokenizer from Hugging Face to manage sequence lengths efficiently:\n",
    "- Questions are limited to a maximum of 21 tokens, based on the length of the longest question in the dataset.\n",
    "- Passages are padded to a maximum of 488 tokens, ensuring that when the question (21 tokens), start token, end token, and separator token are included, the total length remains within the 512-token limit supported by the Transformer’s positional embeddings.\n",
    "\n",
    "### Stemming / Lemmatization / Stopword removal\n",
    "From a past lecture I took away that stemming or lemmatization is not the right choice for a reading comprehension task. It removes valuable meaning\n",
    "No stemming or lemmatization will be done in my preprocessing as to keep the most amount of information possible in my sequences. Stopwords will also not be removed for the same reason.\n",
    "\n",
    "### Embedding Layer\n",
    "In this project, the embedding layer is implemented using PyTorch's nn.Embedding class. The embeddings are trained end-to-end alongside the rest of the model, allowing them to adapt to the specific nuances of the BoolQ dataset.\n",
    "- **Vocabulary Size**: Determined by the tokenizer\n",
    "- **Embedding Dimension**: Set to 300 as this is widely used by large pretrained embedding models like fastText or word2vec.\n",
    "- **Training**: Initialized randomly and updated during training through backpropagation.\n",
    "\n",
    "### Absolute Position Embeddings\n",
    "Since the nn.TransformerEncoder does not by default have positional embeddings I will be implementing them through absolute position embeddings. Choosing the embeddings over the encoding because it is more widely used in practice.\n",
    "Adding the learned absolute positional embeddings to the word embeddings before feeding the input into the transformer model. The position embeddings are initialized randomly and are trained with the model through backpropagation.\n",
    "*Source*: Lecutre on positional encodings\n",
    "\n",
    "### Input / Output / Label format\n",
    "Each data point in the dataset is made up of a questions, passage and the respective binary label. The preprocessing steps transform these into the following formats for my model inputs:\n",
    "- Embedding Layer:\n",
    "    - *input*: Tensor of (batch_size, sequence_length) containing token IDs.\n",
    "    - *output*: Tensor for (batch_size, sequence_length, embedding_dim) with each token ID mapped to a dense vector of size embedding_dim.\n",
    "\n",
    "- 6-Layer Transformer Encoder:\n",
    "    - *input*: The embeddings with shape (batch_size, sequence_length, embedding_dim).\n",
    "    - *output*: A Tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "\n",
    "- Pooling Layer:\n",
    "    - *input*: The output of the last transformer layer, with shape (batch_size, sequence_length, embedding_dim)\n",
    "    - *output*: A Tensor of shape (batch_size, embedding_dim), representing the aggregated sequence information.\n",
    "\n",
    "- 2-Layer Classifier:\n",
    "    - *input*: The pooled output, with shape (batch_size, embedding_dim)\n",
    "    - *output*: A tensor of shape (batch_size, hidden_dim) for the first layer and shape (batch_size, num_classes) for the final layer.\n",
    "\n",
    "- Label format:\n",
    "    - The labels will be encoded as boolean values, enabling the model to predict either 0 or 1 (False/True)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.299480Z",
     "start_time": "2024-11-16T13:20:32.794836Z"
    }
   },
   "source": [
    "# initialize tokenizer with bert-base-cased model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "\n",
    "def get_max_question_len(dataset):\n",
    "    max_len = 0\n",
    "    for item in dataset:\n",
    "        question = item['question']\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        max_len = max(max_len, len(tokenized_question))\n",
    "    return max_len\n",
    "\n",
    "max_question_len = get_max_question_len(train_data)\n",
    "print(max_question_len) # max len of BPE tokenized question (including the CLS and SEP tokens) \n",
    "\n",
    "\n",
    "print(VOCAB_SIZE)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "28996\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_batch(batch):\n",
    "    questions = batch['question']\n",
    "    passages = batch['passage']\n",
    "    \n",
    "    encodings = tokenizer(\n",
    "        questions,\n",
    "        passages,\n",
    "        max_length=512,  # Combined max length within Transformer limit\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {'input_ids': encodings['input_ids'], 'labels': torch.tensor(batch['answer'])}  # output of input_ids and labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.299657Z",
     "start_time": "2024-11-16T13:20:33.296330Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Output of tokenize_batch ===\n",
      "\n",
      "Input IDs:\n",
      "tensor([[  101,  1327,  1110,  1103,  2364,  1104,  1699,   136,   102,  1699,\n",
      "          1110,   170,  1583,  1107,  1980,   119,  2098,  2364,  1110,  2123,\n",
      "           119,   102],\n",
      "        [  101,  2627,  1724, 20332,   136,   102, 20332,  1110,   170, 12343,\n",
      "          1637,  1118,  1613,  7647,   119,   102,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1327,  1110,   123,   116,   123,   136,   102,   123,   116,\n",
      "           123, 22455,   125,   119,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2181,  1103,  4033,  1668,   136,   102,  1109,  2746,  1110,\n",
      "         21279,  1107,  3571,   119,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "\n",
      "Shape of Input IDs: torch.Size([4, 22])\n",
      "\n",
      "Labels:\n",
      "tensor([1, 1, 1, 1])\n",
      "\n",
      "Shape of Labels: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    questions = batch['question']\n",
    "    passages = batch['passage']\n",
    "    \n",
    "    encodings = tokenizer(\n",
    "        questions,\n",
    "        passages,\n",
    "        max_length=512,  # Combined max length within Transformer limit\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {'input_ids': encodings['input_ids'], 'labels': torch.tensor(batch['answer'])}  # output of input_ids and labels\n",
    "\n",
    "def test_tokenize_batch():\n",
    "    \"\"\"\n",
    "    Test the tokenize_batch function and inspect its output.\n",
    "    \"\"\"\n",
    "    # Create a sample input batch\n",
    "    example_batch = {\n",
    "        \"question\": [\"What is the capital of France?\", \"Who wrote Hamlet?\", \"What is 2+2?\", \"Is the earth round?\"],\n",
    "        \"passage\": [\n",
    "            \"France is a country in Europe. Its capital is Paris.\",\n",
    "            \"Hamlet is a tragedy written by William Shakespeare.\",\n",
    "            \"2+2 equals 4.\",\n",
    "            \"The Earth is spherical in shape.\"\n",
    "        ],\n",
    "        \"answer\": [1, 1, 1, 1]  # Example labels (1 = True, 0 = False)\n",
    "    }\n",
    "    \n",
    "    # Tokenize the batch\n",
    "    tokenized_output = tokenize_batch(example_batch)\n",
    "    \n",
    "    # Print the outputs\n",
    "    print(\"=== Output of tokenize_batch ===\")\n",
    "    print(\"\\nInput IDs:\")\n",
    "    print(tokenized_output['input_ids'])\n",
    "    print(\"\\nShape of Input IDs:\", tokenized_output['input_ids'].shape)\n",
    "    print(\"\\nLabels:\")\n",
    "    print(tokenized_output['labels'])\n",
    "    print(\"\\nShape of Labels:\", tokenized_output['labels'].shape)\n",
    "\n",
    "# Run the test\n",
    "test_tokenize_batch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.539577Z",
     "start_time": "2024-11-16T13:20:33.299967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenize the datasets\n",
    "train_data = train_data.map(tokenize_batch, batched=True).with_format(\"torch\", device=DEVICE)\n",
    "validation_data = validation_data.map(tokenize_batch, batched=True).with_format(\"torch\", device=DEVICE)\n",
    "test_data = test_data.map(tokenize_batch, batched=True).with_format(\"torch\", device=DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.550454Z",
     "start_time": "2024-11-16T13:20:33.528635Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    questions = batch['question']\n",
    "    passages = batch['passage']\n",
    "    \n",
    "    encodings = tokenizer(\n",
    "        questions,\n",
    "        passages,\n",
    "        max_length=512,  # Combined max length within Transformer limit\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {'input_ids': encodings['input_ids'], 'labels': torch.tensor(batch['answer'])}\n",
    "\n",
    "# Create the example batch\n",
    "example_batch = {\n",
    "    \"question\": [\"What is the capital of France?\", \"Who wrote Hamlet?\", \"What is 2+2?\", \"Is the earth round?\"],\n",
    "    \"passage\": [\n",
    "        \"France is a country in Europe. Its capital is Paris.\",\n",
    "        \"Hamlet is a tragedy written by William Shakespeare.\",\n",
    "        \"2+2 equals 4.\",\n",
    "        \"The Earth is spherical in shape.\"\n",
    "    ],\n",
    "    \"answer\": [1, 1, 1, 1]  # Example labels (1 = True, 0 = False)\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a Dataset object\n",
    "dataset = Dataset.from_dict(example_batch)\n",
    "\n",
    "# Apply the tokenize_batch function with .map\n",
    "test_data = dataset.map(tokenize_batch, batched=True).with_format(\"torch\", device=DEVICE)\n",
    "\n",
    "# Print the processed test_data\n",
    "print(test_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.550618Z",
     "start_time": "2024-11-16T13:20:33.544282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define collate function for dynamic padding in DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    \n",
    "    # Pad to the longest sequence in the batch\n",
    "    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "    return {'input_ids': input_ids, 'labels': labels}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.550676Z",
     "start_time": "2024-11-16T13:20:33.546568Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.550952Z",
     "start_time": "2024-11-16T13:20:33.548095Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Architecture\n",
    "- **Input Layer**:\n",
    "    - The input to my model is the nn.Embedding layer that will be trained on the dataset with the network.\n",
    "    - Each input sequence consists of a concatenated question and passage with a [SEP] token between them, marking the boundary. The separator token allows the model to distinguish between the two segments.\n",
    "    - The resulting shape of the input tensor after embedding is (batch_size, sequence_length, embedding_dim).\n",
    "    - <span style=\"color: orange;\">Adding a projection layer to reach the expected input dimension of the transformer model of: (batch_size, sequence_length, transformer_hidden_dim) This proved easier to implement than adjusting the first transformer layer.</span>\n",
    "- **6-Layer Transformer Encoder**:\n",
    "    - Using the PyTorch implementation of the Transformer Encoder. The input to this model will be the output of the embedding layer with shape (batch_size, sequence_length, <span style=\"color: orange;\">transformer_hidden_dim</span>). Using six layers to learn contextual representations of the concatenated questino-passage sequence.\n",
    "- **Pooling Layer**:\n",
    "    - Apply *mean pooling* across the sequence length to reducing the output from (batch_size, sequence_length, embedding_dim) to (batch_size, embedding_dim). This provides a fixed-size single vector that summarizes the entire sequence for the classifier which provides the advantage of efficient memory use in training with varying sequence lengths and a fixed-sized input for my classifier.\n",
    "- **2-Layer Classifier with ReLU**\n",
    "    - I will implement a two-layer classifier network as defined in the project assignment. The first layer will take the output from the pooling layer of size (batch_size, embedding_dim) as its input and provide an output shape of (batch_size, hidden_dim). Using a ReLU for non-linearity. The second layer has output dimensions of (batch_size, num_classes) with num_classes=2. The output layer will use a softmax as the activation function as it is preferable over a sigmoid function for binary classification.\n",
    "\n",
    "~~### Loss and Optimizer~~\n",
    "~~For this binary classification task I'm using Binary Cross-Entropy Loss. BCE is widely used in binary classification problems, as it provides a probabilistic interpretation of the model's outputs, making it convenient for distinguishing between two classes.~~ <br>\n",
    "~~*Source*: [Binary Cross-Entropy/Log Loss for Binary Classification](https://www.geeksforgeeks.org/binary-cross-entropy-log-loss-for-binary-classification/)~~ <br>\n",
    "<span style=\"color: orange;\">I used CrossEntropyLoss in this project, after stating to use BCELoss in stage 1. After trouble implementing BCELoss I decided to go with CrossEntropyLoss and 2 classes instead as I could not fix the issues with it in time for this project.</span>\n",
    "\n",
    "For my optimizer I choose the Adam Optimizer for its adaptive learning rates and efficient handling of sparse gradients. It is well suited for deep learning tasks, provides fast convergence and has worked well in prior projects. <br>\n",
    "*Source*: [Introduction to the Adam Optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "### Experiments\n",
    "*Batch Size*: I will start with a batch_size of 16 and increase it to the maximum my hardware can handle then leaving it fixed as it is not a hyperparameter. <span style=\"color: orange;\">Finally settling on a batch size of 64</span>\n",
    "\n",
    "To tune my models' hyperparameters I will be experimenting with the following ranges:\n",
    "- Learning Rate: [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "- Embedding Dimension: [128, 256, 300]\n",
    "- Hidden Dimension for Classifier: [64, 128, 256]\n",
    "- <span style=\"color: orange;\">Hidden Dimension for Transformer: [128, 1024]</span>\n",
    "- Number of Attention Heads: [4, 8, 12, 16]\n",
    "- Dropout Rate: [0.1, 0.2, 0.3]\n",
    "- Weight Decay: [1e-4, 1e-5, 1e-6]\n",
    "- <span style=\"color: orange;\">Warmup Steps: [0, 500]</span>\n",
    "\n",
    "### Training\n",
    "I do not expect any run to take longer than 25 epochs. Thus limiting the maximum number of epochs to 25 and implement the early stopping criteria like in past projects. <br>\n",
    "<span style=\"color: orange;\"> After some experimenting, I extended the maximum number of epochs to 60 because the model was struggling to learn anything before epoch 25.</span>\n",
    "\n",
    "### Checkpointing and Early Stopping\n",
    "**Checkpointing**: I will implement checkpointing to save the model with the best validation accuracy. Criteria for this will be the maximum validation accuracy.\n",
    "\n",
    "**Early Stopping**: Early stopping the run if the validation loss does not decrease within 15 epochs.\n",
    "\n",
    "### Planned Correctness Tests\n",
    "- Testing input shape to ensure the model receives a valid input format\n",
    "- Testing output shape to verify the model produces the expected output shape\n",
    "- Visually check the loss is decreasing while training\n",
    "- Visually check the output for overfitting\n",
    "- Visually check predictions using a confusion matrix\n",
    "- Ensure reproducibility by setting the random seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.560658Z",
     "start_time": "2024-11-16T13:20:33.552535Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:33.587103Z",
     "start_time": "2024-11-16T13:20:33.554900Z"
    }
   },
   "source": [
    "class TransformerClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            transformer_hidden_dim,\n",
    "            classifier_hidden_dim,\n",
    "            dropout_rate=0.0,\n",
    "            learning_rate=1e-4,\n",
    "            warmup_steps=0,\n",
    "            weight_decay=0.0,\n",
    "            num_layers=6,\n",
    "            num_classes=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional Embedding layer\n",
    "        self.position_embedding = nn.Embedding(512, embedding_dim)\n",
    "        \n",
    "        # Linear layer to project from embedding_dim to transformer_hidden_dim (input dim of Transformer)\n",
    "        self.input_projection = nn.Linear(embedding_dim, transformer_hidden_dim)\n",
    "        \n",
    "        # Transformer encoder with transformer_hidden_dim as input dimension\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=transformer_hidden_dim * 2,\n",
    "            dropout=dropout_rate,\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Pooling layer (mean pooling)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Classifier: 2-layer MLP with dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_hidden_dim, classifier_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(classifier_hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Accuracy metric\n",
    "        self.train_accuracy = BinaryAccuracy()\n",
    "        self.val_accuracy = BinaryAccuracy()\n",
    "        self.test_accuracy = BinaryAccuracy()\n",
    "        self.test_confusion_matrix = BinaryConfusionMatrix()\n",
    "        \n",
    "        # Storage for predictions and labels\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        positions = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
    "        pos_embeddings = self.position_embedding(positions)\n",
    "        embedded = embedded + pos_embeddings\n",
    "        \n",
    "        projected = self.input_projection(embedded)\n",
    "        projected = projected.permute(1, 0, 2)\n",
    "        encoded = self.transformer_encoder(projected)\n",
    "        encoded = encoded.permute(1, 0, 2)\n",
    "        \n",
    "        pooled = self.pooling(encoded.transpose(1, 2)).squeeze(-1)\n",
    "        \n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        labels = batch['labels'].long().to(self.device)\n",
    "        \n",
    "        outputs = self(input_ids)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Log accuracy and loss\n",
    "        acc = self.train_accuracy(preds, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_accuracy', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        labels = batch['labels'].long().to(self.device)\n",
    "        \n",
    "        outputs = self(input_ids)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Log accuracy and loss\n",
    "        acc = self.val_accuracy(preds, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_accuracy', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        labels = batch['labels'].long().to(self.device)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = self(input_ids)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Update accuracy and save predictions and labels for confusion matrix\n",
    "        accuracy = self.test_accuracy(preds, labels)\n",
    "        self.test_preds.extend(preds.cpu().numpy())\n",
    "        self.test_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Log test loss and accuracy\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_accuracy\", accuracy, prog_bar=True)\n",
    "        \n",
    "        return {\"test_loss\": loss, \"test_accuracy\": accuracy}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Calculate and plot the confusion matrix\n",
    "        cm = confusion_matrix(self.test_labels, self.test_preds)\n",
    "        self.plot_confusion_matrix(cm)\n",
    "        \n",
    "        # Clear stored predictions and labels after logging\n",
    "        self.test_preds.clear()\n",
    "        self.test_labels.clear()\n",
    "\n",
    "    def plot_confusion_matrix(self, cm):\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
    "        ax.set_xlabel(\"Predicted labels\")\n",
    "        ax.set_ylabel(\"True labels\")\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "        \n",
    "        # Log the plot to WandB\n",
    "        wandb.log({\"confusion_matrix\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < self.hparams.warmup_steps:\n",
    "                lr = float(current_step) / float(max(1, self.hparams.warmup_steps))\n",
    "                return lr\n",
    "            return 1.0\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': LambdaLR(optimizer, lr_lambda=lr_lambda),\n",
    "            'interval': 'step',\n",
    "            'name': 'learning_rate'\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "BATCH_S = 32\n",
    "SEQ_LEN = 512\n",
    "DIM = 300\n",
    "\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=DIM,\n",
    "    num_heads=8,\n",
    "    transformer_hidden_dim=256,\n",
    "    classifier_hidden_dim=64,\n",
    ").to(DEVICE)\n",
    "\n",
    "x = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN)).to(DEVICE)\n",
    "\n",
    "# Run the forward pass and check the output shape\n",
    "assert model.forward(x).shape == torch.Size([BATCH_SIZE, 2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:34.836876Z",
     "start_time": "2024-11-16T13:20:33.565233Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "def test_input_shapes(dataloader, model, batch_size):\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']  # Shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Check the batch size\n",
    "        assert input_ids.shape[0] == batch_size, f\"Expected batch size {batch_size}, got {input_ids.shape[0]}\"\n",
    "        \n",
    "        # Pass the input through the model\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Check that logits have the expected shape (batch_size, num_classes)\n",
    "        assert logits.shape[0] == batch_size, f\"Expected logits batch size {batch_size}, got {logits.shape[0]}\"\n",
    "        assert logits.shape[1] == model.classifier[-1].out_features, f\"Expected {model.classifier[-1].out_features} output classes, got {logits.shape[1]}\"\n",
    "        \n",
    "        print(f\"Batch passed with input_ids shape {input_ids.shape} and logits shape {logits.shape}\")\n",
    "        break  # Test the first batch only to verify shapes\n",
    "\n",
    "# Assuming `model` is an instance of TransformerClassifier\n",
    "test_input_shapes(train_loader, model, BATCH_SIZE)\n",
    "test_input_shapes(validation_loader, model, BATCH_SIZE)\n",
    "test_input_shapes(test_loader, model, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:20:37.550873Z",
     "start_time": "2024-11-16T13:20:34.837766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kn/1r08h7jx2hlcz7rndw5qc1d80000gn/T/ipykernel_95416/1889658370.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = [torch.tensor(item['input_ids']) for item in batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch passed with input_ids shape torch.Size([64, 512]) and logits shape torch.Size([64, 2])\n",
      "Batch passed with input_ids shape torch.Size([64, 512]) and logits shape torch.Size([64, 2])\n",
      "Batch passed with input_ids shape torch.Size([64, 512]) and logits shape torch.Size([64, 2])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration of my best performing model\n",
    "CONFIG = {\n",
    "    \"embedding_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"transformer_hidden_dim\": 512,\n",
    "    \"classifier_hidden_dim\": 128,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 1e-4,\n",
    "\n",
    "    \"patience\": 60,\n",
    "    \"epochs\": 60,\n",
    "    \n",
    "    \"num_classes\": 2,\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "    \"num_layers\": 6\n",
    "}\n",
    "\n",
    "# Initialize model with config\n",
    "TransformerModel = TransformerClassifier(\n",
    "    vocab_size=CONFIG[\"vocab_size\"],\n",
    "    embedding_dim=CONFIG[\"embedding_dim\"],\n",
    "    num_heads=CONFIG[\"num_heads\"],\n",
    "    transformer_hidden_dim=CONFIG[\"transformer_hidden_dim\"],\n",
    "    classifier_hidden_dim=CONFIG[\"classifier_hidden_dim\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    num_classes=CONFIG[\"num_classes\"],\n",
    "    dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "run_name = (\n",
    "    f\"emb_dim_{CONFIG['embedding_dim']}-\"\n",
    "    f\"n_heads_{CONFIG['num_heads']}-\"\n",
    "    f\"trans_h_dim_{CONFIG['transformer_hidden_dim']}-\"\n",
    "    f\"class_h_dim_{CONFIG['classifier_hidden_dim']}-\"\n",
    "    f\"dropout_{CONFIG['dropout_rate']}-\"\n",
    "    f\"lr_{CONFIG['learning_rate']}-\"\n",
    "    f\"warmup_{CONFIG['warmup_steps']}-\"\n",
    "    f\"w_decay_{CONFIG['weight_decay']}\"\n",
    ")\n",
    "\n",
    "print(\"Run Name:\", run_name)\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='nlp_p3_transformer',\n",
    "    name=run_name,\n",
    "    group=\"manual_runs\"\n",
    ")\n",
    "\n",
    "for key, value in CONFIG.items():\n",
    "    wandb_logger.experiment.config[key] = str(value)\n",
    "\n",
    "wandb_logger.log_hyperparams(TransformerModel.hparams)\n",
    "\n",
    "# Model Checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_accuracy\",            # Monitor validation accuracy\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=run_name,\n",
    "    save_top_k=1,\n",
    "    mode=\"max\",                        # Save model with the highest validation accuracy\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Early Stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",            # Monitor validation accuracy for early stopping\n",
    "    patience=CONFIG['patience'],\n",
    "    mode=\"max\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with these callbacks\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CONFIG['epochs'],\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger=wandb_logger,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(TransformerModel, train_loader, validation_loader)\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T13:21:41.981715Z",
     "start_time": "2024-11-16T13:20:37.550751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Name: emb_dim_256-n_heads_8-trans_h_dim_512-class_h_dim_128-dropout_0.2-lr_0.0001-warmup_100-w_decay_0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33maintnoair\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.7"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>./wandb/run-20241116_142038-jeetmdhl</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/aintnoair/nlp_p3_transformer/runs/jeetmdhl' target=\"_blank\">emb_dim_256-n_heads_8-trans_h_dim_512-class_h_dim_128-dropout_0.2-lr_0.0001-warmup_100-w_decay_0.0001</a></strong> to <a href='https://wandb.ai/aintnoair/nlp_p3_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/aintnoair/nlp_p3_transformer' target=\"_blank\">https://wandb.ai/aintnoair/nlp_p3_transformer</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/aintnoair/nlp_p3_transformer/runs/jeetmdhl' target=\"_blank\">https://wandb.ai/aintnoair/nlp_p3_transformer/runs/jeetmdhl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name                  | Type                  | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0  | embedding             | Embedding             | 7.4 M  | train\n",
      "1  | position_embedding    | Embedding             | 131 K  | train\n",
      "2  | input_projection      | Linear                | 131 K  | train\n",
      "3  | transformer_encoder   | TransformerEncoder    | 12.6 M | train\n",
      "4  | pooling               | AdaptiveAvgPool1d     | 0      | train\n",
      "5  | classifier            | Sequential            | 65.9 K | train\n",
      "6  | loss_fn               | CrossEntropyLoss      | 0      | train\n",
      "7  | train_accuracy        | BinaryAccuracy        | 0      | train\n",
      "8  | val_accuracy          | BinaryAccuracy        | 0      | train\n",
      "9  | test_accuracy         | BinaryAccuracy        | 0      | train\n",
      "10 | test_confusion_matrix | BinaryConfusionMatrix | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "20.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.4 M    Total params\n",
      "81.473    Total estimated model params size (MB)\n",
      "76        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "344afd4278fd489eb9ce5c138e6e2160"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blackbook/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/var/folders/kn/1r08h7jx2hlcz7rndw5qc1d80000gn/T/ipykernel_95416/1889658370.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
      "/Users/blackbook/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a2a227c9d90441d9ecf3847d6ba4df6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    570\u001B[0m     ckpt_path,\n\u001B[1;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m )\n\u001B[0;32m--> 574\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_stage()\n\u001B[1;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1025\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance()\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch_loop\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 140\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(data_fetcher)\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001B[39;00m\n\u001B[0;32m--> 250\u001B[0m     batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautomatic_optimization\u001B[38;5;241m.\u001B[39mrun(trainer\u001B[38;5;241m.\u001B[39moptimizers[\u001B[38;5;241m0\u001B[39m], batch_idx, kwargs)\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001B[0m, in \u001B[0;36m_AutomaticOptimization.run\u001B[0;34m(self, optimizer, batch_idx, kwargs)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step(batch_idx, closure)\n\u001B[1;32m    192\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001B[0m, in \u001B[0;36m_AutomaticOptimization._optimizer_step\u001B[0;34m(self, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 268\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\n\u001B[1;32m    269\u001B[0m     trainer,\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer_step\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    271\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mcurrent_epoch,\n\u001B[1;32m    272\u001B[0m     batch_idx,\n\u001B[1;32m    273\u001B[0m     optimizer,\n\u001B[1;32m    274\u001B[0m     train_step_and_backward_closure,\n\u001B[1;32m    275\u001B[0m )\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:167\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 167\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1306\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001B[0m\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1283\u001B[0m \u001B[38;5;124;03mthe optimizer.\u001B[39;00m\n\u001B[1;32m   1284\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1304\u001B[0m \n\u001B[1;32m   1305\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1306\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep(closure\u001B[38;5;241m=\u001B[39moptimizer_closure)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:153\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy\u001B[38;5;241m.\u001B[39moptimizer_step(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer, closure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:238\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule)\n\u001B[0;32m--> 238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39moptimizer_step(optimizer, model\u001B[38;5;241m=\u001B[39mmodel, closure\u001B[38;5;241m=\u001B[39mclosure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001B[0m, in \u001B[0;36mPrecision.optimizer_step\u001B[0;34m(self, optimizer, model, closure, **kwargs)\u001B[0m\n\u001B[1;32m    121\u001B[0m closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, closure)\n\u001B[0;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m optimizer\u001B[38;5;241m.\u001B[39mstep(closure\u001B[38;5;241m=\u001B[39mclosure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:137\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    136\u001B[0m opt\u001B[38;5;241m.\u001B[39m_opt_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(opt, opt\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/optim/adamw.py:197\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 197\u001B[0m         loss \u001B[38;5;241m=\u001B[39m closure()\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001B[0m, in \u001B[0;36mPrecision._wrap_closure\u001B[0;34m(self, model, optimizer, closure)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mhook is called.\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    106\u001B[0m \n\u001B[1;32m    107\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m closure_result \u001B[38;5;241m=\u001B[39m closure()\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclosure(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39menable_grad()\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 129\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step_fn()\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001B[0m, in \u001B[0;36m_AutomaticOptimization._training_step\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m    315\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\n\u001B[0;32m--> 317\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m call\u001B[38;5;241m.\u001B[39m_call_strategy_hook(trainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()  \u001B[38;5;66;03m# unused hook - call anyway for backward compatibility\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:390\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mtraining_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[0;32mIn[29], line 84\u001B[0m, in \u001B[0;36mTransformerClassifier.training_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     82\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mlong()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 84\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(input_ids)\n\u001B[1;32m     85\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(outputs, labels)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[29], line 72\u001B[0m, in \u001B[0;36mTransformerClassifier.forward\u001B[0;34m(self, input_ids)\u001B[0m\n\u001B[1;32m     71\u001B[0m projected \u001B[38;5;241m=\u001B[39m projected\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 72\u001B[0m encoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_encoder(projected)\n\u001B[1;32m     73\u001B[0m encoded \u001B[38;5;241m=\u001B[39m encoded\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/transformer.py:511\u001B[0m, in \u001B[0;36mTransformerEncoder.forward\u001B[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    510\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 511\u001B[0m     output \u001B[38;5;241m=\u001B[39m mod(\n\u001B[1;32m    512\u001B[0m         output,\n\u001B[1;32m    513\u001B[0m         src_mask\u001B[38;5;241m=\u001B[39mmask,\n\u001B[1;32m    514\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal,\n\u001B[1;32m    515\u001B[0m         src_key_padding_mask\u001B[38;5;241m=\u001B[39msrc_key_padding_mask_for_layers,\n\u001B[1;32m    516\u001B[0m     )\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_nested:\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/transformer.py:906\u001B[0m, in \u001B[0;36mTransformerEncoderLayer.forward\u001B[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    902\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(\n\u001B[1;32m    903\u001B[0m         x\n\u001B[1;32m    904\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[1;32m    905\u001B[0m     )\n\u001B[0;32m--> 906\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n\u001B[1;32m    908\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/transformer.py:931\u001B[0m, in \u001B[0;36mTransformerEncoderLayer._ff_block\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_ff_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 931\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear1(x))))\n\u001B[1;32m    932\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(x)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 85\u001B[0m\n\u001B[1;32m     78\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[1;32m     79\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39mCONFIG[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     80\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[checkpoint_callback, early_stopping_callback],\n\u001B[1;32m     81\u001B[0m     logger\u001B[38;5;241m=\u001B[39mwandb_logger,\n\u001B[1;32m     82\u001B[0m )\n\u001B[1;32m     84\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[0;32m---> 85\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(TransformerModel, train_loader, validation_loader)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# Finish the wandb run\u001B[39;00m\n\u001B[1;32m     88\u001B[0m wandb\u001B[38;5;241m.\u001B[39mfinish()\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[1;32m    539\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[1;32m    540\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/new_nlp_test/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[1;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[0;32m---> 64\u001B[0m     exit(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[1;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-16T13:21:41.975868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_label_distribution_from_boolq(train_data, val_data):\n",
    "    train_yes_count = sum(1 for label in train_data['answer'] if label == 1)\n",
    "    train_no_count = sum(1 for label in train_data['answer'] if label == 0)\n",
    "    val_yes_count = sum(1 for label in val_data['answer'] if label == 1)\n",
    "    val_no_count = sum(1 for label in val_data['answer'] if label == 0)\n",
    "    \n",
    "    train_total = train_yes_count + train_no_count\n",
    "    val_total = val_yes_count + val_no_count\n",
    "    train_yes_percent = (train_yes_count / train_total) * 100\n",
    "    train_no_percent = (train_no_count / train_total) * 100\n",
    "    val_yes_percent = (val_yes_count / val_total) * 100\n",
    "    val_no_percent = (val_no_count / val_total) * 100\n",
    "    \n",
    "    labels = [\"No\", \"Yes\"]\n",
    "    train_counts = [train_no_count, train_yes_count]\n",
    "    val_counts = [val_no_count, val_yes_count]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    ax[0].bar(labels, train_counts, color='blue', alpha=0.7)\n",
    "    ax[0].set_title(\"Training Label Distribution\")\n",
    "    ax[0].set_xlabel(\"Labels\")\n",
    "    ax[0].set_ylabel(\"Count\")\n",
    "    ax[0].set_ylim(0, max(train_counts + val_counts) * 1.1)  # Consistent y-axis scale\n",
    "    for i, v in enumerate(train_counts):\n",
    "        ax[0].text(i, v + 50, f\"{v} ({train_no_percent if i == 0 else train_yes_percent:.2f}%)\", ha='center', color='black')\n",
    "    \n",
    "    ax[1].bar(labels, val_counts, color='orange', alpha=0.7)\n",
    "    ax[1].set_title(\"Validation Label Distribution\")\n",
    "    ax[1].set_xlabel(\"Labels\")\n",
    "    ax[1].set_ylabel(\"Count\")\n",
    "    ax[1].set_ylim(0, max(train_counts + val_counts) * 1.1)  # Consistent y-axis scale\n",
    "    for i, v in enumerate(val_counts):\n",
    "        ax[1].text(i, v + 20, f\"{v} ({val_no_percent if i == 0 else val_yes_percent:.2f}%)\", ha='center', color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming train_data and validation_data are loaded datasets\n",
    "plot_label_distribution_from_boolq(train_data, validation_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The percentage of yes answers in each data split is: Train; 62.64%, Val; 59.50%, Test;62.17%\n",
    "Seeing how difficult it was in past projects to reach a much better accuracy than the baseline majority class I am setting my goal for the transformer model at 64% accuracy on the test set.\n",
    "\n",
    "### Metrics\n",
    "**Accuracy**: To evaluate model performance across different hyperparameter configurations, I will use validation accuracy as the primary metric.\n",
    "**Confusion Matrix**: This will give a comprehensive view of true positives, true negatives, false positives, and false negatives, allowing me deeper insight into the model’s performance.\n",
    "\n",
    "### Error Analysis\n",
    "To understand why the model may fail on certain predictions, I will conduct an error analysis investigating weather missclassifications are related to the confidence score the model has in it's predictions. Low confidence on correct answers or high confidence on wrong answers may indicate areas where the model is uncertain or overconfident."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color: orange;\">Evaluation</span>\n",
    "Choosing to do without any manual preprocessing and relying on the built-in functionality of the AutoTokenizer module to handle padding, truncation, extra whitespace removal, special character removal and case sensitivity management was a success. Making the implementation more straightforward and reducing the potential for implementation errors. In most of my runs the transformer model would reach a validation accuracy of ~59%, after plotting the distribution of labels in the validation set it was clear to me the model is only predicting the majority \"yes\" class. After this realization I added the dropout layer and weight decay to the model which showed improvement in reaching a better validation accuracy, now reaching 60%. <br>\n",
    "For my evaluation on the test set I chose the model reaching the highest accuracy on the validation set.\n",
    "### Results of test set validation\n",
    "Run name: emb_dim_256-n_heads_8-trans_h_dim_512-class_h_dim_128-dropout_0.2-lr_0.0001-warmup_100-w_decay_0.0001 <br>\n",
    "Test Accuracy: 63.70%\n",
    "True Positives: 1880\n",
    "True Negatives: 203\n",
    "\n",
    "The model reached an accuracy of 63.70% on the test set compared to the baseline of 62.17%, however it still predominantly predicts the majority class. In the end the best performing model used an embedding dimension of 256, 8 attention heads, a transformer hidden size of 512 and a classifier hidden size of 128. Coupled with the dropout rate of 0.2, learning rate and weight decay of 1e-4.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "start_time": "2024-11-16T13:19:30.276112Z"
    }
   },
   "source": [
    "# Define paths and load model from checkpoint\n",
    "base_path = Path(\"checkpoints\")\n",
    "run_name = \"emb_dim_256-n_heads_8-trans_h_dim_512-class_h_dim_128-dropout_0.2-lr_0.0001-warmup_100-w_decay_0.0001\"\n",
    "file_path = base_path / (run_name + \".ckpt\")\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model = TransformerClassifier.load_from_checkpoint(file_path)\n",
    "wandb_logger = WandbLogger(project=\"nlp_p3_transformer\", name=run_name, group=\"evaluation\")\n",
    "\n",
    "# Initialize the trainer for testing\n",
    "trainer = pl.Trainer(logger=wandb_logger)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "# Finish WandB session\n",
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "My expectation for this project are to beat the majority class baseline of 62.17% on the test set. My last project wasn't very successufl in that it only predicted the majority class every time. The feedback on that project was plenty and I hope I can improve on a lot of points for this project.\n",
    "\n",
    "Given the results form the LSTM implementation I am setting my expecation for the Transformer architecture to reach an accuracy of 63% to 65% on the test set.\n",
    "\n",
    "## <span style=\"color: orange;\">Interpretation</span>\n",
    "Looking at my pre-project expectation it is fair to say this model exceeded my initial hopes. Reaching an accuracy of 63.70% is exceeding the baseline of 62.17%. It was surprising to me how much of an effect the warmup steps had on the transformer model. If Andreas hadn't mentioned it in the Machine Learning Operations Course I don't think I would have tried it, I'm glad I did. When i 'maxed-out' the model (e.g. embedding_dim=512, both hidden_dims=1024) the model quickly overfit on the training data. This was insightful to see how less was more in this model.\n",
    "This transformer model also succeeded in the way of not solely predicting \"yes\" like my last LSTM based model did. Although, I was not able to get optuna functioning in time with this pytorch lightning implementation, I am happy with the brief hyperparameter tuning I did manually. Taking suggestions from my latest TA feedback I was able to implement the case sensitivity in this project rather than lower-casing all words and only logging accuracy as a metric for this BoolQ task. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "start_time": "2024-11-16T13:19:30.277015Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
