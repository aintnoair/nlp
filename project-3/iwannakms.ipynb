{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# If this doesn't work I'm ending it all tonight."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4349ff8a302cfa91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "**Importing Python Packages**\n",
    "Making sure the notebook is reproducible and runs without error, I will install the necessary libraries in a pip cell below.\n",
    "\n",
    "**Data Loading and Split**\n",
    "The data consists of the questions, a passage and the answer. In total there are 12'697 entries in the dataset. Splitting them according to the lecture slides into train (8427), validation (1000) and test (3270).\n",
    "\n",
    "**Seeding for Reproducibility**\n",
    "Setting the random Seed to 42 for reproducibility."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3bcf98a02dd84e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "### Tokenizer\n",
    "In past projects I always did some sort of manual preprocessing of the data. In this project I deliberately refrain from any manual preprocessing and will let the built-in features of the AutoTokenizer with the from_pretrained(\"bert-base-cased\") model handle the following steps for me:\n",
    "- Whitespace and Special Character removal (e.g. emojis or phonetic pronunciations)\n",
    "- Case Sensitivity\n",
    "- Padding and Truncation (pad automatically, truncate to max: 512 tokens - amount of pretrained position embeddings)\n",
    "\n",
    "I only now found out about this from the Hugging Face Transformer [Preprocessing Data Documentation](https://huggingface.co/transformers/v3.0.2/preprocessing.html).\n",
    "\n",
    "### Lowercase / Case Sensitivity\n",
    "From my feedback I will now keep case sensitivity instead of lower-casing all text, as well as using a more efficient maximum sequence length for the questions of 21 tokens (the length of the longest question), passages will be padded dynamically within the batch to the length of the longest passage in the batch without exceeding a total maximal length of 512 tokens (resulting in an absolute maximum length for passages of 491 tokens). Example of case sensitivity: the word \"US\" would become \"us\" and could thus change the meaning of a sentence drastically. <br>\n",
    "*Source*: Feedback from Project 2 (LSTM)\n",
    "\n",
    "### Stemming / Lemmatization\n",
    "From a past lecture I took away that stemming or lemmatization is not the right choice for a reading comprehension task. It removes valuable meaning\n",
    "No stemming or lemmatization will be done in my preprocessing as to keep the most amount of information possible in my sequences.\n",
    "\n",
    "### Embedding Layer\n",
    "In this project, the embedding layer is implemented using PyTorch's nn.Embedding class. The embeddings are trained end-to-end alongside the rest of the model, allowing them to adapt to the specific nuances of the BoolQ dataset.\n",
    "- **Vocabulary Size**: Determined by the tokenizer\n",
    "- **Embedding Dimension**: Set to 300 as this is widely used by large pretrained embedding models like fastText or word2vec.\n",
    "- **Training**: Initialized randomly and updated during training through backpropagation.\n",
    "\n",
    "### Input / Label format\n",
    "In the final input each data point will have the following format:\n",
    "- asdf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7793b906730c11ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "603cfdc9bde01765"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "### Architecture\n",
    "- **Input Layer**:\n",
    "    - The input to my model is the nn.Embedding layer that will be trained on the dataset with the network.\n",
    "    - Each input sequence consists of a concatenated question and passage with a [SEP] token between them, marking the boundary. The separator token allows the model to distinguish between the two segments.\n",
    "    - The resulting shape of the input tensor after embedding is (batch_size, sequence_length, embedding_dim).\n",
    "- **6-Layer Transformer Encoder**:\n",
    "    - Using the PyTorch implementation of the Transformer Encoder. The input to this model will be the output of the embedding layer with shape (batch_size, sequence_length, embedding_dim). Using six layers to learn contextual representations of the concatenated questino-passage sequence.\n",
    "- **Pooling Layer**:\n",
    "    - Apply *mean pooling* across the sequence length to reducing the output from (batch_size, sequence_length, embedding_dim) to (batch_size, embedding_dim). This provides a fixed-size single vector that summarizes the entire sequence for the classifier which provides the advantage of efficient memory use in training with varying sequence lengths and a fixed-sized input for my classifier.\n",
    "- **2-Layer Classifier with ReLU**\n",
    "    - I will implement a two-layer classifier network as defined in the project assignment. The first layer will take the output from the pooling layer of size (batch_size, embedding_dim) as its input and provide an output shape of (batch_size, hidden_dim). Using a ReLU for non-linearity. The second layer has output dimensions of (batch_size, num_classes) with num_classes=2. The output layer will use a softmax as the activation function as it is preferable over a sigmoid function for binary classification.\n",
    "\n",
    "### Loss and Optimizer\n",
    "For this binary classification task I'm using Binary Cross-Entropy Loss. BCE is widely used in binary classification problems, as it provides a probabilistic interpretation of the model's outputs, making it convenient for distinguishing between two classes. <br>\n",
    "*Source*: [Binary Cross-Entropy/Log Loss for Binary Classification](https://www.geeksforgeeks.org/binary-cross-entropy-log-loss-for-binary-classification/)\n",
    "\n",
    "For my optimizer I choose the Adam Optimizer for its adaptive learning rates and efficient handling of sparse gradients. It is well suited for deep learning tasks, provides fast convergence and has worked well in prior projects. <br>\n",
    "*Source*: [Introduction to the Adam Optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "### Experiments\n",
    "To tune my models' hyperparameters I will be experimenting with the following ranges:\n",
    "- Learning Rate: [1e-2, 1e-7]\n",
    "\n",
    "### Training\n",
    "\n",
    "### Checkpointing and Early Stopping\n",
    "\n",
    "### Planned Correctness Tests\n",
    "- Testing input shape to ensure the model receives a valid input format\n",
    "- Testing output shape to verify the model produces the expected output shape\n",
    "- Visually check the loss is decreasing while training\n",
    "- Visually check the output for overfitting\n",
    "- Visually check predictions using a confusion matrix\n",
    "- Ensure reproducibility by setting the random seed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4999c8de059e4ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3eb4081b7e105bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97df627ba28537b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3eb5a85d0e71d20c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcaca0b2c953a27b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e8ca6ae254d00b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
