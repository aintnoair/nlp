{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers for BoolQ Reading Comprehension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "762609a9410f543d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %pip install ..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "124fa7f9eaa1d3c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73e63b161928e5fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "### Architecture\n",
    "- **Input Layer**:\n",
    "    - The input to my model is the nn.Embedding layer that will be trained on the dataset with the network.\n",
    "    - Each input sequence consists of a concatenated question and passage with a [SEP] token between them, marking the boundary. The separator token allows the model to distinguish between the two segments.\n",
    "    - The resulting shape of the input tensor after embedding is (batch_size, sequence_length, embedding_dim).\n",
    "- **6-Layer Transformer Encoder**:\n",
    "    - Using the PyTorch implementation of the Transformer Encoder. The input to this model will be the output of the embedding layer with shape (batch_size, sequence_length, embedding_dim). Using six layers to learn contextual representations of the concatenated questino-passage sequence.\n",
    "- **Pooling Layer**:\n",
    "    - Apply *mean pooling* across the sequence length to reducing the output from (batch_size, sequence_length, embedding_dim) to (batch_size, embedding_dim). This provides a fixed-size single vector that summarizes the entire sequence for the classifier which provides the advantage of efficient memory use in training with varying sequence lengths and a fixed-sized input for my classifier.\n",
    "- **2-Layer Classifier with ReLU**\n",
    "    - I will implement a two-layer classifier network as defined in the project assignment. The first layer will take the output from the pooling layer of size (batch_size, embedding_dim) as its input and provide an output shape of (batch_size, hidden_dim). Using a ReLU for non-linearity. The second layer has output dimensions of (batch_size, num_classes) with num_classes=2. The output layer will use a softmax as the activation function as it is preferable over a sigmoid function for binary classification.\n",
    "\n",
    "### Loss and Optimizer\n",
    "For this binary classification task I'm using Binary Cross-Entropy Loss. BCE is widely used in binary classification problems, as it provides a probabilistic interpretation of the model's outputs, making it convenient for distinguishing between two classes. <br>\n",
    "*Source*: [Binary Cross-Entropy/Log Loss for Binary Classification](https://www.geeksforgeeks.org/binary-cross-entropy-log-loss-for-binary-classification/)\n",
    "\n",
    "For my optimizer I choose the Adam Optimizer for its adaptive learning rates and efficient handling of sparse gradients. It is well suited for deep learning tasks, provides fast convergence and has worked well in prior projects. <br>\n",
    "*Source*: [Introduction to the Adam Optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "### Experiments\n",
    "To tune my models' hyperparameters I will be experimenting with the following ranges:\n",
    "- Learning Rate: [1e-2, 1e-7]\n",
    "- "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe6ea3a6f6c20817"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "26950e5c322c0c44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "624a6785987ce808"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5428aa5698ef33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "425117557dca3479"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "54ecd46b6da48fca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
